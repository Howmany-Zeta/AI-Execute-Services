# æ–‡æ¡£è§£æå·¥å…· - å¿«é€Ÿå¼€å§‹æŒ‡å—

## ğŸš€ å¼€ç®±å³ç”¨çŠ¶æ€

ç°åœ¨æ–‡æ¡£è§£æç»„ä»¶å·²ç»å®Œå…¨å¯ä»¥å¼€ç®±å³ç”¨ï¼å¼€å‘è€…å¯ä»¥ç›´æ¥åœ¨é¡¹ç›®ä¸­ä½¿ç”¨è¿™äº›å·¥å…·ã€‚

## ğŸ“ æ–°çš„ç›®å½•ç»“æ„

```
aiecs/tools/
â”œâ”€â”€ docs/                          # æ–‡æ¡£å¤„ç†å·¥å…·ä¸“ç”¨ç›®å½• 
â”‚   â”œâ”€â”€ __init__.py                # æ–‡æ¡£å·¥å…·æ¨¡å—åˆå§‹åŒ–
â”‚   â”œâ”€â”€ document_parser_tool.py    # æ ¸å¿ƒæ–‡æ¡£è§£æå·¥å…·
â”‚   â””â”€â”€ ai_document_orchestrator.py # AIæ™ºèƒ½ç¼–æ’å™¨
â”œâ”€â”€ task_tools/                    # å…¶ä»–ä»»åŠ¡å·¥å…·
â”‚   â”œâ”€â”€ chart_tool.py
â”‚   â”œâ”€â”€ scraper_tool.py
â”‚   â””â”€â”€ ...
â””â”€â”€ __init__.py                    # ä¸»å·¥å…·æ³¨å†Œ
```

## ğŸ”§ å®‰è£…å’Œé…ç½®

### 1. åŸºç¡€å®‰è£…
```bash
# é¡¹ç›®å·²åŒ…å«æ‰€æœ‰å¿…è¦ä¾èµ–
pip install -e .

# æˆ–è€…ä»PyPIå®‰è£…
pip install aiecs
```

### 2. ç¯å¢ƒå˜é‡é…ç½®ï¼ˆå¯é€‰ï¼‰
```bash
# æ–‡æ¡£è§£æå™¨é…ç½®
export DOC_PARSER_enable_cloud_storage=true
export DOC_PARSER_gcs_bucket_name=your-bucket-name
export DOC_PARSER_gcs_project_id=your-project-id

# AIç¼–æ’å™¨é…ç½®
export AI_DOC_ORCHESTRATOR_default_ai_provider=openai
export AI_DOC_ORCHESTRATOR_max_chunk_size=4000
```

## ğŸ’» åŸºç¡€ä½¿ç”¨

### 1. å¯¼å…¥å·¥å…·ï¼ˆæ–°è·¯å¾„ï¼‰
```python
# ä»docsç›®å½•å¯¼å…¥æ–‡æ¡£å¤„ç†å·¥å…·
from aiecs.tools.docs.document_parser_tool import DocumentParserTool
from aiecs.tools.docs.ai_document_orchestrator import AIDocumentOrchestrator

# æˆ–è€…ä½¿ç”¨æ‡’åŠ è½½æ–¹å¼
from aiecs.tools.docs import document_parser_tool, ai_document_orchestrator
```

### 2. å¿«é€Ÿå¼€å§‹ç¤ºä¾‹
```python
#!/usr/bin/env python3
"""
æ–‡æ¡£å¤„ç†å¿«é€Ÿå¼€å§‹ç¤ºä¾‹
"""

def quick_start_example():
    # 1. åˆå§‹åŒ–å·¥å…·
    from aiecs.tools.docs.document_parser_tool import DocumentParserTool
    from aiecs.tools.docs.ai_document_orchestrator import AIDocumentOrchestrator
    
    parser = DocumentParserTool()
    orchestrator = AIDocumentOrchestrator()
    
    # 2. å¤„ç†æœ¬åœ°æ–‡æ¡£
    result = orchestrator.process_document(
        source="test_document.txt",
        processing_mode="summarize"
    )
    
    print("AIæ‘˜è¦:", result['ai_result']['ai_response'])

if __name__ == "__main__":
    quick_start_example()
```

### 3. æ”¯æŒçš„æ–‡æ¡£æº
```python
# æ”¯æŒå¤šç§æ–‡æ¡£æº
sources = [
    "/path/to/local/file.pdf",                    # æœ¬åœ°æ–‡ä»¶
    "https://example.com/document.pdf",           # URLé“¾æ¥
    "gs://bucket/document.pdf",                   # Google Cloud Storage
    "s3://bucket/document.pdf",                   # AWS S3
    "azure://container/document.pdf",             # Azure Blob
    "doc_123456789",                              # å­˜å‚¨ID
]

for source in sources:
    try:
        result = parser.parse_document(source=source)
        print(f"âœ“ æˆåŠŸè§£æ: {source}")
    except Exception as e:
        print(f"âœ— è§£æå¤±è´¥: {source} - {e}")
```

## ğŸŒ äº‘å­˜å‚¨é…ç½®

### Google Cloud Storage
```python
config = {
    "enable_cloud_storage": True,
    "gcs_bucket_name": "my-documents",
    "gcs_project_id": "my-project-id"
}

parser = DocumentParserTool(config)
```

### å¤„ç†äº‘å­˜å‚¨æ–‡æ¡£
```python
# ç›´æ¥å¤„ç†äº‘å­˜å‚¨ä¸­çš„æ–‡æ¡£
cloud_doc = "gs://my-bucket/reports/annual_report.pdf"

result = orchestrator.process_document(
    source=cloud_doc,
    processing_mode="extract_info",
    processing_params={
        "extraction_criteria": "è´¢åŠ¡æ•°æ®ã€å…³é”®æŒ‡æ ‡ã€ç»“è®º"
    }
)
```

## ğŸ¯ å®é™…åº”ç”¨ç¤ºä¾‹

### 1. æ‰¹é‡å¤„ç†æ–‡æ¡£
```python
def batch_process_documents():
    orchestrator = AIDocumentOrchestrator()
    
    documents = [
        "gs://docs/report1.pdf",
        "gs://docs/report2.pdf", 
        "s3://legal/contract.docx"
    ]
    
    result = orchestrator.batch_process_documents(
        sources=documents,
        processing_mode="analyze",
        max_concurrent=3
    )
    
    print(f"æˆåŠŸå¤„ç†: {result['successful_documents']}")
    return result

# è¿è¡Œæ‰¹é‡å¤„ç†
batch_result = batch_process_documents()
```

### 2. è‡ªå®šä¹‰AIåˆ†æ
```python
def custom_document_analysis():
    orchestrator = AIDocumentOrchestrator()
    
    # åˆ›å»ºè‡ªå®šä¹‰åˆ†æå™¨
    legal_analyzer = orchestrator.create_custom_processor(
        system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ³•å¾‹æ–‡æ¡£åˆ†æå¸ˆ",
        user_prompt_template="åˆ†æä»¥ä¸‹æ³•å¾‹æ–‡æ¡£å¹¶æå–å…³é”®æ¡æ¬¾ï¼š{content}"
    )
    
    # ä½¿ç”¨è‡ªå®šä¹‰åˆ†æå™¨
    result = legal_analyzer("contract.pdf")
    return result

# è¿è¡Œè‡ªå®šä¹‰åˆ†æ
analysis_result = custom_document_analysis()
```

### 3. å®æ—¶æ–‡æ¡£å¤„ç†
```python
async def realtime_document_processing():
    orchestrator = AIDocumentOrchestrator()
    
    # å¼‚æ­¥å¤„ç†å¤šä¸ªæ–‡æ¡£
    tasks = [
        orchestrator.process_document_async(
            source=doc,
            processing_mode="summarize"
        )
        for doc in ["doc1.pdf", "doc2.pdf", "doc3.pdf"]
    ]
    
    results = await asyncio.gather(*tasks)
    return results

# è¿è¡Œå¼‚æ­¥å¤„ç†
import asyncio
async_results = asyncio.run(realtime_document_processing())
```

## ğŸ” æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

#### 1. å¯¼å…¥é”™è¯¯
```python
# é”™è¯¯çš„æ—§è·¯å¾„
# from aiecs.tools.task_tools.document_parser_tool import DocumentParserTool

# æ­£ç¡®çš„æ–°è·¯å¾„
from aiecs.tools.docs.document_parser_tool import DocumentParserTool
```

#### 2. æƒé™é—®é¢˜
```bash
# å¦‚æœé‡åˆ°ä¸´æ—¶æ–‡ä»¶æƒé™é—®é¢˜
export TMPDIR=/tmp/aiecs_temp
mkdir -p $TMPDIR
chmod 755 $TMPDIR
```

#### 3. äº‘å­˜å‚¨é…ç½®
```python
# ç¡®ä¿äº‘å­˜å‚¨é…ç½®æ­£ç¡®
config = {
    "enable_cloud_storage": True,
    "gcs_bucket_name": "your-bucket",
    "gcs_project_id": "your-project"
}

# æµ‹è¯•é…ç½®
parser = DocumentParserTool(config)
print("äº‘å­˜å‚¨é…ç½®:", parser.settings.enable_cloud_storage)
```

## ğŸ“Š åŠŸèƒ½æ£€æŸ¥æ¸…å•

è¿è¡Œä»¥ä¸‹ä»£ç æ£€æŸ¥æ‰€æœ‰åŠŸèƒ½æ˜¯å¦æ­£å¸¸ï¼š

```python
def system_check():
    """ç³»ç»ŸåŠŸèƒ½æ£€æŸ¥"""
    
    print("ğŸ” AIECSæ–‡æ¡£å¤„ç†ç³»ç»Ÿæ£€æŸ¥")
    print("=" * 40)
    
    # 1. å¯¼å…¥æµ‹è¯•
    try:
        from aiecs.tools.docs.document_parser_tool import DocumentParserTool
        from aiecs.tools.docs.ai_document_orchestrator import AIDocumentOrchestrator
        print("âœ“ æ¨¡å—å¯¼å…¥æˆåŠŸ")
    except ImportError as e:
        print(f"âœ— æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
        return
    
    # 2. åˆå§‹åŒ–æµ‹è¯•
    try:
        parser = DocumentParserTool()
        orchestrator = AIDocumentOrchestrator()
        print("âœ“ å·¥å…·åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âœ— å·¥å…·åˆå§‹åŒ–å¤±è´¥: {e}")
        return
    
    # 3. é…ç½®æµ‹è¯•
    print(f"âœ“ äº‘å­˜å‚¨æ”¯æŒ: {parser.settings.enable_cloud_storage}")
    print(f"âœ“ ä¸´æ—¶ç›®å½•: {parser.settings.temp_dir}")
    print(f"âœ“ AIæä¾›å•†: {orchestrator.settings.default_ai_provider}")
    
    # 4. åŠŸèƒ½æµ‹è¯•
    test_sources = [
        ("æœ¬åœ°è·¯å¾„", "/tmp/test.txt"),
        ("HTTP URL", "https://example.com/file.pdf"),
        ("äº‘å­˜å‚¨", "gs://bucket/file.pdf"),
        ("å­˜å‚¨ID", "doc_123456")
    ]
    
    for name, source in test_sources:
        is_supported = (
            not parser._is_url(source) or
            parser._is_cloud_storage_path(source) or
            parser._is_storage_id(source)
        )
        status = "âœ“" if is_supported else "âœ—"
        print(f"{status} {name}æ”¯æŒ: {source}")
    
    print("\nğŸ‰ ç³»ç»Ÿæ£€æŸ¥å®Œæˆ!")

# è¿è¡Œç³»ç»Ÿæ£€æŸ¥
system_check()
```

## ğŸš€ ç”Ÿäº§éƒ¨ç½²å»ºè®®

### 1. æ€§èƒ½é…ç½®
```python
# ç”Ÿäº§ç¯å¢ƒæ¨èé…ç½®
production_config = {
    "max_file_size": 100 * 1024 * 1024,  # 100MB
    "timeout": 120,                       # 2åˆ†é’Ÿè¶…æ—¶
    "max_concurrent_requests": 10,        # å¹¶å‘è¯·æ±‚é™åˆ¶
    "enable_cloud_storage": True,         # å¯ç”¨äº‘å­˜å‚¨
    "max_chunk_size": 8000               # AIå¤„ç†å—å¤§å°
}
```

### 2. é”™è¯¯å¤„ç†
```python
def robust_document_processing(source):
    """å¥å£®çš„æ–‡æ¡£å¤„ç†"""
    try:
        orchestrator = AIDocumentOrchestrator()
        result = orchestrator.process_document(
            source=source,
            processing_mode="summarize"
        )
        return {"status": "success", "result": result}
    
    except Exception as e:
        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {source} - {e}")
        return {"status": "error", "error": str(e)}
```

### 3. ç›‘æ§å’Œæ—¥å¿—
```python
import logging

# é…ç½®è¯¦ç»†æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# å¯ç”¨ç‰¹å®šæ¨¡å—çš„è°ƒè¯•æ—¥å¿—
logging.getLogger('aiecs.tools.docs').setLevel(logging.DEBUG)
```

## ğŸ“š æ›´å¤šèµ„æº

- å®Œæ•´APIæ–‡æ¡£: `docs/TOOLS_USED_INSTRUCTION/DOCUMENT_PARSER_TOOL.md`
- ç¤ºä¾‹ä»£ç : `examples/document_processing_example.py`
- äº‘å­˜å‚¨ç¤ºä¾‹: `examples/cloud_storage_document_example.py`
- å·¥å…·æ¶æ„è¯´æ˜: `docs/TOOLS_USED_INSTRUCTION/TOOL_SPECIAL_SPECIAL_INSTRUCTIONS.md`

## ğŸ¯ æ€»ç»“

æ–‡æ¡£è§£æç»„ä»¶ç°åœ¨å·²ç»ï¼š

âœ… **å¼€ç®±å³ç”¨** - å¯ç›´æ¥åœ¨é¡¹ç›®ä¸­ä½¿ç”¨  
âœ… **ç»“æ„æ¸…æ™°** - æ–‡æ¡£å·¥å…·ç‹¬ç«‹åœ¨`docs`ç›®å½•  
âœ… **åŠŸèƒ½å®Œæ•´** - æ”¯æŒå¤šç§æ–‡æ¡£æºå’ŒAIå¤„ç†æ¨¡å¼  
âœ… **é«˜æ€§èƒ½** - å¼‚æ­¥å¤„ç†ã€æ™ºèƒ½ç¼“å­˜ã€å¹¶å‘æ§åˆ¶  
âœ… **æ˜“æ‰©å±•** - æ”¯æŒè‡ªå®šä¹‰å¤„ç†æµç¨‹å’ŒAIæä¾›å•†  

å¼€å‘è€…ç°åœ¨å¯ä»¥ç›´æ¥ä½¿ç”¨è¿™å¥—ç°ä»£åŒ–çš„æ–‡æ¡£è§£æç»„ä»¶æ¥æ„å»ºè‡ªå·±çš„AIæ–‡æ¡£å¤„ç†åº”ç”¨ï¼
