# Search Tool ä¼˜åŒ–åˆ†ææŠ¥å‘Š

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šæ·±å…¥åˆ†æäº† `search_tool.py` (1124 è¡Œä»£ç ) çš„æ¶æ„å’Œå®ç°,ä» **Agent è·å–é«˜è´¨é‡æœç´¢ç»“æœ** çš„è§’åº¦æå‡ºä¼˜åŒ–å»ºè®®ã€‚

**å½“å‰çŠ¶æ€**: âœ… åŠŸèƒ½å®Œæ•´,æ¶æ„è‰¯å¥½,åŒ…å«é€Ÿç‡é™åˆ¶ã€ç†”æ–­å™¨ã€é‡è¯•æœºåˆ¶  
**ä¼˜åŒ–æ½œåŠ›**: ğŸš€ ä¸­é«˜ - å¯åœ¨ç»“æœè´¨é‡ã€æ™ºèƒ½æ€§ã€ä¸Šä¸‹æ–‡ç†è§£æ–¹é¢æ˜¾è‘—æå‡

---

## ğŸ¯ æ ¸å¿ƒé—®é¢˜ï¼šAgent å¦‚ä½•è·å¾—æ›´ç²¾å‡†çš„é«˜è´¨é‡æœç´¢ç»“æœï¼Ÿ

### é—®é¢˜åˆ†è§£

1. **ç»“æœç›¸å…³æ€§é—®é¢˜** - æœç´¢ç»“æœæ˜¯å¦çœŸæ­£åŒ¹é… Agent çš„æŸ¥è¯¢æ„å›¾?
2. **ç»“æœè´¨é‡é—®é¢˜** - å¦‚ä½•åŒºåˆ†é«˜è´¨é‡å’Œä½è´¨é‡çš„æœç´¢ç»“æœ?
3. **ä¸Šä¸‹æ–‡ç†è§£é—®é¢˜** - å·¥å…·æ˜¯å¦ç†è§£ Agent çš„æŸ¥è¯¢ä¸Šä¸‹æ–‡å’Œç›®æ ‡?
4. **ç»“æœå‘ˆç°é—®é¢˜** - è¿”å›çš„æ•°æ®æ ¼å¼æ˜¯å¦ä¾¿äº Agent ç†è§£å’Œä½¿ç”¨?
5. **æ™ºèƒ½ä¼˜åŒ–é—®é¢˜** - å·¥å…·æ˜¯å¦èƒ½è‡ªåŠ¨ä¼˜åŒ–æŸ¥è¯¢ä»¥è·å¾—æ›´å¥½ç»“æœ?

---

## ğŸ” è¯¦ç»†åˆ†æ

## 1. ç»“æœè´¨é‡è¯„ä¼°ä¸æ’åº (â­â­â­â­â­ æœ€é‡è¦)

### 1.1 ç¼ºå°‘ç»“æœè´¨é‡è¯„åˆ†

**ç°çŠ¶é—®é¢˜**:
```python
# search_tool.py:619-657 - _parse_search_results åªæå–åŸºç¡€å­—æ®µ
def _parse_search_results(self, raw_results: Dict[str, Any]) -> List[Dict[str, Any]]:
    items = raw_results.get('items', [])
    results = []
    
    for item in items:
        result = {
            'title': item.get('title', ''),
            'link': item.get('link', ''),
            'snippet': item.get('snippet', ''),
            'displayLink': item.get('displayLink', ''),
            'formattedUrl': item.get('formattedUrl', ''),
        }
        # æ²¡æœ‰è´¨é‡è¯„åˆ†ï¼
        # æ²¡æœ‰å¯ä¿¡åº¦è¯„ä¼°ï¼
        # æ²¡æœ‰ç›¸å…³æ€§åˆ†æ•°ï¼
        results.append(result)
    
    return results
```

**é—®é¢˜**:
- âŒ æ‰€æœ‰ç»“æœè¢«å¹³ç­‰å¯¹å¾…,æ²¡æœ‰è´¨é‡åŒºåˆ†
- âŒ Agent æ— æ³•åˆ¤æ–­å“ªäº›ç»“æœæ›´å¯é 
- âŒ æ²¡æœ‰åˆ©ç”¨ Google è¿”å›çš„æ’åä¿¡æ¯
- âŒ æ²¡æœ‰åŸºäºæ¥æºåŸŸåçš„æƒå¨æ€§è¯„ä¼°

**ä¼˜åŒ–å»ºè®®**:
```python
class ResultQualityAnalyzer:
    """æœç´¢ç»“æœè´¨é‡åˆ†æå™¨"""
    
    # é«˜æƒå¨æ€§åŸŸååˆ—è¡¨
    AUTHORITATIVE_DOMAINS = {
        # å­¦æœ¯å’Œç ”ç©¶
        'scholar.google.com': 0.95,
        'arxiv.org': 0.95,
        'ieee.org': 0.95,
        'acm.org': 0.95,
        'nature.com': 0.95,
        'science.org': 0.95,
        
        # æ”¿åºœå’Œå®˜æ–¹
        '.gov': 0.90,
        '.edu': 0.85,
        'who.int': 0.90,
        'un.org': 0.90,
        
        # çŸ¥ååª’ä½“
        'nytimes.com': 0.80,
        'bbc.com': 0.80,
        'reuters.com': 0.85,
        'apnews.com': 0.85,
        
        # æŠ€æœ¯æ–‡æ¡£
        'docs.python.org': 0.90,
        'developer.mozilla.org': 0.90,
        'stackoverflow.com': 0.75,
        'github.com': 0.70,
        
        # ç™¾ç§‘
        'wikipedia.org': 0.75,
    }
    
    # ä½è´¨é‡åŸŸåç‰¹å¾
    LOW_QUALITY_INDICATORS = [
        'clickbait', 'ads', 'spam', 'scam',
        'download-now', 'free-download',
        'xxx', 'adult', 'casino', 'pills'
    ]
    
    def analyze_result_quality(
        self, 
        result: Dict[str, Any],
        query: str,
        position: int  # Google è¿”å›çš„ä½ç½® (1-based)
    ) -> Dict[str, Any]:
        """
        åˆ†æå•ä¸ªæœç´¢ç»“æœçš„è´¨é‡
        
        Returns:
            {
                'quality_score': 0.85,  # ç»¼åˆè´¨é‡åˆ†æ•° (0-1)
                'authority_score': 0.90,  # æƒå¨æ€§åˆ†æ•°
                'relevance_score': 0.80,  # ç›¸å…³æ€§åˆ†æ•°
                'freshness_score': 0.75,  # æ–°é²œåº¦åˆ†æ•°
                'credibility_level': 'high',  # high/medium/low
                'quality_signals': {
                    'has_https': True,
                    'domain_authority': 'high',
                    'content_length': 'adequate',
                    'has_metadata': True,
                    'position_rank': 1
                },
                'warnings': []  # è´¨é‡è­¦å‘Š
            }
        """
        
        quality_analysis = {
            'quality_score': 0.0,
            'authority_score': 0.0,
            'relevance_score': 0.0,
            'freshness_score': 0.0,
            'credibility_level': 'medium',
            'quality_signals': {},
            'warnings': []
        }
        
        # 1. è¯„ä¼°åŸŸåæƒå¨æ€§
        domain = result.get('displayLink', '').lower()
        authority_score = self._calculate_authority_score(domain)
        quality_analysis['authority_score'] = authority_score
        quality_analysis['quality_signals']['domain_authority'] = (
            'high' if authority_score > 0.8 else 
            'medium' if authority_score > 0.5 else 'low'
        )
        
        # 2. è¯„ä¼°ç›¸å…³æ€§
        relevance_score = self._calculate_relevance_score(
            query, 
            result.get('title', ''),
            result.get('snippet', ''),
            position
        )
        quality_analysis['relevance_score'] = relevance_score
        
        # 3. è¯„ä¼°æ–°é²œåº¦
        freshness_score = self._calculate_freshness_score(result)
        quality_analysis['freshness_score'] = freshness_score
        
        # 4. æ£€æŸ¥ HTTPS
        link = result.get('link', '')
        has_https = link.startswith('https://')
        quality_analysis['quality_signals']['has_https'] = has_https
        if not has_https:
            quality_analysis['warnings'].append('No HTTPS - security concern')
        
        # 5. æ£€æŸ¥å†…å®¹é•¿åº¦
        snippet_length = len(result.get('snippet', ''))
        quality_analysis['quality_signals']['content_length'] = (
            'adequate' if snippet_length > 100 else 'short'
        )
        if snippet_length < 50:
            quality_analysis['warnings'].append('Very short snippet - may lack detail')
        
        # 6. æ£€æŸ¥å…ƒæ•°æ®
        has_metadata = 'metadata' in result or 'pagemap' in result
        quality_analysis['quality_signals']['has_metadata'] = has_metadata
        
        # 7. ä½ç½®æ’ååŠ åˆ† (Google çš„æ’åæœ¬èº«å°±æ˜¯è´¨é‡ä¿¡å·)
        position_score = max(0, 1.0 - (position - 1) * 0.05)  # å‰20åçº¿æ€§é€’å‡
        quality_analysis['quality_signals']['position_rank'] = position
        
        # 8. æ£€æµ‹ä½è´¨é‡æŒ‡æ ‡
        url_lower = link.lower()
        title_lower = result.get('title', '').lower()
        for indicator in self.LOW_QUALITY_INDICATORS:
            if indicator in url_lower or indicator in title_lower:
                quality_analysis['warnings'].append(
                    f'Low quality indicator detected: {indicator}'
                )
                authority_score *= 0.5  # ä¸¥é‡é™ä½æƒå¨æ€§
        
        # 9. è®¡ç®—ç»¼åˆè´¨é‡åˆ†æ•°
        quality_analysis['quality_score'] = (
            authority_score * 0.35 +      # æƒå¨æ€§ 35%
            relevance_score * 0.30 +      # ç›¸å…³æ€§ 30%
            position_score * 0.20 +       # æ’å 20%
            freshness_score * 0.10 +      # æ–°é²œåº¦ 10%
            (0.05 if has_https else 0)    # HTTPS 5%
        )
        
        # 10. ç¡®å®šå¯ä¿¡åº¦ç­‰çº§
        if quality_analysis['quality_score'] > 0.75:
            quality_analysis['credibility_level'] = 'high'
        elif quality_analysis['quality_score'] > 0.5:
            quality_analysis['credibility_level'] = 'medium'
        else:
            quality_analysis['credibility_level'] = 'low'
        
        return quality_analysis
    
    def _calculate_authority_score(self, domain: str) -> float:
        """è®¡ç®—åŸŸåæƒå¨æ€§åˆ†æ•°"""
        # ç²¾ç¡®åŒ¹é…
        if domain in self.AUTHORITATIVE_DOMAINS:
            return self.AUTHORITATIVE_DOMAINS[domain]
        
        # åç¼€åŒ¹é…
        for auth_domain, score in self.AUTHORITATIVE_DOMAINS.items():
            if domain.endswith(auth_domain):
                return score
        
        # é»˜è®¤ä¸­ç­‰æƒå¨æ€§
        return 0.5
    
    def _calculate_relevance_score(
        self, 
        query: str, 
        title: str, 
        snippet: str,
        position: int
    ) -> float:
        """
        è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        
        åŸºäº:
        1. æŸ¥è¯¢è¯åœ¨æ ‡é¢˜ä¸­çš„å‡ºç°
        2. æŸ¥è¯¢è¯åœ¨æ‘˜è¦ä¸­çš„å‡ºç°
        3. Google çš„æ’åä½ç½®
        """
        query_terms = set(query.lower().split())
        title_lower = title.lower()
        snippet_lower = snippet.lower()
        
        # æ ‡é¢˜åŒ¹é…
        title_matches = sum(1 for term in query_terms if term in title_lower)
        title_score = title_matches / len(query_terms) if query_terms else 0
        
        # æ‘˜è¦åŒ¹é…
        snippet_matches = sum(1 for term in query_terms if term in snippet_lower)
        snippet_score = snippet_matches / len(query_terms) if query_terms else 0
        
        # ä½ç½®åŠ æƒ (å‰3åé¢å¤–åŠ åˆ†)
        position_bonus = 0.2 if position <= 3 else 0.1 if position <= 10 else 0
        
        # ç»¼åˆç›¸å…³æ€§
        relevance = (
            title_score * 0.6 +      # æ ‡é¢˜æƒé‡æ›´é«˜
            snippet_score * 0.3 +    # æ‘˜è¦æ¬¡ä¹‹
            position_bonus           # ä½ç½®åŠ åˆ†
        )
        
        return min(1.0, relevance)
    
    def _calculate_freshness_score(self, result: Dict[str, Any]) -> float:
        """
        è®¡ç®—æ–°é²œåº¦åˆ†æ•°
        
        åŸºäºé¡µé¢å…ƒæ•°æ®ä¸­çš„æ—¥æœŸä¿¡æ¯
        """
        # å°è¯•ä» pagemap ä¸­æå–æ—¥æœŸ
        metadata = result.get('metadata', {})
        
        # æŸ¥æ‰¾æ—¥æœŸå­—æ®µ
        date_fields = ['metatags', 'article', 'newsarticle']
        publish_date = None
        
        for field in date_fields:
            if field in metadata:
                items = metadata[field]
                if isinstance(items, list) and items:
                    item = items[0]
                    # å¸¸è§æ—¥æœŸå­—æ®µ
                    for date_key in ['publishdate', 'datepublished', 'article:published_time']:
                        if date_key in item:
                            publish_date = item[date_key]
                            break
                if publish_date:
                    break
        
        if not publish_date:
            # æ²¡æœ‰æ—¥æœŸä¿¡æ¯,è¿”å›ä¸­ç­‰åˆ†æ•°
            return 0.5
        
        try:
            from datetime import datetime
            # å°è¯•è§£ææ—¥æœŸ
            pub_dt = datetime.fromisoformat(publish_date.replace('Z', '+00:00'))
            now = datetime.now(pub_dt.tzinfo)
            
            days_old = (now - pub_dt).days
            
            # æ–°é²œåº¦è¯„åˆ†
            if days_old < 7:
                return 1.0      # ä¸€å‘¨å†… - éå¸¸æ–°é²œ
            elif days_old < 30:
                return 0.9      # ä¸€ä¸ªæœˆå†… - å¾ˆæ–°é²œ
            elif days_old < 90:
                return 0.7      # ä¸‰ä¸ªæœˆå†… - è¾ƒæ–°
            elif days_old < 365:
                return 0.5      # ä¸€å¹´å†… - ä¸­ç­‰
            elif days_old < 730:
                return 0.3      # ä¸¤å¹´å†… - è¾ƒæ—§
            else:
                return 0.1      # ä¸¤å¹´ä»¥ä¸Š - å¾ˆæ—§
        except:
            return 0.5
    
    def rank_results(
        self, 
        results: List[Dict[str, Any]],
        ranking_strategy: str = 'balanced'
    ) -> List[Dict[str, Any]]:
        """
        é‡æ–°æ’åºæœç´¢ç»“æœ
        
        Args:
            results: å¸¦æœ‰è´¨é‡åˆ†æçš„ç»“æœåˆ—è¡¨
            ranking_strategy: æ’åºç­–ç•¥
                - 'balanced': å¹³è¡¡è´¨é‡å’Œç›¸å…³æ€§
                - 'authority': ä¼˜å…ˆæƒå¨æ€§
                - 'relevance': ä¼˜å…ˆç›¸å…³æ€§
                - 'freshness': ä¼˜å…ˆæ–°é²œåº¦
        """
        if ranking_strategy == 'authority':
            return sorted(
                results, 
                key=lambda x: x.get('_quality', {}).get('authority_score', 0),
                reverse=True
            )
        elif ranking_strategy == 'relevance':
            return sorted(
                results,
                key=lambda x: x.get('_quality', {}).get('relevance_score', 0),
                reverse=True
            )
        elif ranking_strategy == 'freshness':
            return sorted(
                results,
                key=lambda x: x.get('_quality', {}).get('freshness_score', 0),
                reverse=True
            )
        else:  # balanced
            return sorted(
                results,
                key=lambda x: x.get('_quality', {}).get('quality_score', 0),
                reverse=True
            )
```

**å¢å¼ºçš„ _parse_search_results**:
```python
def _parse_search_results(
    self, 
    raw_results: Dict[str, Any],
    query: str = "",
    enable_quality_analysis: bool = True
) -> List[Dict[str, Any]]:
    """è§£æå¹¶å¢å¼ºæœç´¢ç»“æœ"""
    
    items = raw_results.get('items', [])
    results = []
    
    # åˆå§‹åŒ–è´¨é‡åˆ†æå™¨
    if enable_quality_analysis:
        quality_analyzer = ResultQualityAnalyzer()
    
    for position, item in enumerate(items, start=1):
        result = {
            'title': item.get('title', ''),
            'link': item.get('link', ''),
            'snippet': item.get('snippet', ''),
            'displayLink': item.get('displayLink', ''),
            'formattedUrl': item.get('formattedUrl', ''),
        }
        
        # æ·»åŠ å›¾ç‰‡å…ƒæ•°æ®
        if 'image' in item:
            result['image'] = {
                'contextLink': item['image'].get('contextLink', ''),
                'height': item['image'].get('height', 0),
                'width': item['image'].get('width', 0),
                'byteSize': item['image'].get('byteSize', 0),
                'thumbnailLink': item['image'].get('thumbnailLink', '')
            }
        
        # æ·»åŠ é¡µé¢å…ƒæ•°æ®
        if 'pagemap' in item:
            result['metadata'] = item['pagemap']
        
        # æ·»åŠ è´¨é‡åˆ†æ
        if enable_quality_analysis and query:
            quality_analysis = quality_analyzer.analyze_result_quality(
                result, query, position
            )
            result['_quality'] = quality_analysis
            
            # æ·»åŠ  Agent å‹å¥½çš„è´¨é‡æ‘˜è¦
            result['_quality_summary'] = {
                'score': quality_analysis['quality_score'],
                'level': quality_analysis['credibility_level'],
                'is_authoritative': quality_analysis['authority_score'] > 0.8,
                'is_relevant': quality_analysis['relevance_score'] > 0.7,
                'is_fresh': quality_analysis['freshness_score'] > 0.7,
                'warnings_count': len(quality_analysis['warnings'])
            }
        
        results.append(result)
    
    return results
```

**å½±å“**: Agent å¯ä»¥ä¼˜å…ˆä½¿ç”¨é«˜è´¨é‡ç»“æœ,é¿å…ä½è´¨é‡æˆ–ä¸å¯é çš„ä¿¡æ¯ã€‚

---

### 1.2 ç¼ºå°‘ç»“æœå»é‡å’Œèšåˆ

**ç°çŠ¶é—®é¢˜**:
- åŒä¸€å†…å®¹çš„ä¸åŒ URL å¯èƒ½é‡å¤å‡ºç°
- æ²¡æœ‰æ£€æµ‹ç›¸ä¼¼ç»“æœ
- æ‰¹é‡æœç´¢æ—¶æ²¡æœ‰è·¨æŸ¥è¯¢å»é‡

**ä¼˜åŒ–å»ºè®®**:
```python
class ResultDeduplicator:
    """æœç´¢ç»“æœå»é‡å™¨"""
    
    def deduplicate_results(
        self, 
        results: List[Dict[str, Any]],
        similarity_threshold: float = 0.85
    ) -> List[Dict[str, Any]]:
        """
        å»é™¤é‡å¤å’Œé«˜åº¦ç›¸ä¼¼çš„ç»“æœ
        
        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1)
        
        Returns:
            å»é‡åçš„ç»“æœåˆ—è¡¨
        """
        if not results:
            return []
        
        unique_results = []
        seen_urls = set()
        seen_content_hashes = set()
        
        for result in results:
            url = result.get('link', '')
            
            # 1. URL å»é‡ (æ ‡å‡†åŒ–åæ¯”è¾ƒ)
            normalized_url = self._normalize_url(url)
            if normalized_url in seen_urls:
                continue
            
            # 2. å†…å®¹ç›¸ä¼¼åº¦å»é‡
            content_hash = self._calculate_content_hash(
                result.get('title', ''),
                result.get('snippet', '')
            )
            
            # æ£€æŸ¥æ˜¯å¦ä¸å·²æœ‰ç»“æœé«˜åº¦ç›¸ä¼¼
            is_duplicate = False
            for seen_hash in seen_content_hashes:
                similarity = self._calculate_similarity(content_hash, seen_hash)
                if similarity > similarity_threshold:
                    is_duplicate = True
                    break
            
            if is_duplicate:
                continue
            
            # æ·»åŠ åˆ°å”¯ä¸€ç»“æœ
            unique_results.append(result)
            seen_urls.add(normalized_url)
            seen_content_hashes.add(content_hash)
        
        return unique_results
    
    def _normalize_url(self, url: str) -> str:
        """æ ‡å‡†åŒ– URL (ç§»é™¤æŸ¥è¯¢å‚æ•°ã€ç‰‡æ®µç­‰)"""
        from urllib.parse import urlparse, urlunparse
        
        parsed = urlparse(url)
        # åªä¿ç•™ scheme, netloc, path
        normalized = urlunparse((
            parsed.scheme,
            parsed.netloc.lower(),
            parsed.path.rstrip('/'),
            '', '', ''  # ç§»é™¤ params, query, fragment
        ))
        return normalized
    
    def _calculate_content_hash(self, title: str, snippet: str) -> str:
        """è®¡ç®—å†…å®¹å“ˆå¸Œ"""
        import hashlib
        content = f"{title.lower()} {snippet.lower()}"
        # ç§»é™¤æ ‡ç‚¹å’Œå¤šä½™ç©ºæ ¼
        content = ''.join(c for c in content if c.isalnum() or c.isspace())
        content = ' '.join(content.split())
        return hashlib.md5(content.encode()).hexdigest()
    
    def _calculate_similarity(self, hash1: str, hash2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªå“ˆå¸Œçš„ç›¸ä¼¼åº¦ (ç®€åŒ–ç‰ˆ)"""
        # å®é™…åº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„ç›¸ä¼¼åº¦ç®—æ³•
        return 1.0 if hash1 == hash2 else 0.0
```

**å½±å“**: å‡å°‘å†—ä½™ç»“æœ,æé«˜ç»“æœå¤šæ ·æ€§å’Œä¿¡æ¯å¯†åº¦ã€‚

---

## 2. æŸ¥è¯¢ç†è§£ä¸ä¼˜åŒ– (â­â­â­â­â­)

### 2.1 ç¼ºå°‘æŸ¥è¯¢æ„å›¾åˆ†æ

**ç°çŠ¶é—®é¢˜**:
```python
# search_tool.py:663-731 - search_web ç›´æ¥ä½¿ç”¨åŸå§‹æŸ¥è¯¢
def search_web(self, query: str, num_results: int = 10, ...):
    if not query or not query.strip():
        raise ValidationError("Query cannot be empty")
    
    # ç›´æ¥æœç´¢,ä¸åˆ†ææ„å›¾
    raw_results = self._retry_with_backoff(
        self._execute_search,
        query,  # åŸå§‹æŸ¥è¯¢,æœªä¼˜åŒ–
        num_results,
        start_index,
        **search_params
    )
```

**é—®é¢˜**:
- âŒ ä¸ç†è§£æŸ¥è¯¢ç±»å‹ (äº‹å®æŸ¥è¯¢ã€æ“ä½œæŒ‡å—ã€æ¯”è¾ƒã€å®šä¹‰ç­‰)
- âŒ ä¸èƒ½è‡ªåŠ¨æ·»åŠ æœ‰ç”¨çš„æœç´¢è¿ç®—ç¬¦
- âŒ ä¸èƒ½æ ¹æ®æ„å›¾è°ƒæ•´æœç´¢å‚æ•°
- âŒ ä¸èƒ½æä¾›æŸ¥è¯¢å»ºè®®

**ä¼˜åŒ–å»ºè®®**:
```python
class QueryIntentAnalyzer:
    """æŸ¥è¯¢æ„å›¾åˆ†æå™¨"""
    
    # æŸ¥è¯¢æ„å›¾ç±»å‹
    INTENT_PATTERNS = {
        'definition': {
            'keywords': ['what is', 'define', 'meaning of', 'definition'],
            'query_enhancement': 'definition OR meaning OR "what is"',
            'suggested_params': {'num_results': 5}
        },
        'how_to': {
            'keywords': ['how to', 'how do i', 'tutorial', 'guide', 'steps to'],
            'query_enhancement': 'tutorial OR guide OR "step by step"',
            'suggested_params': {'num_results': 10, 'file_type': None}
        },
        'comparison': {
            'keywords': ['vs', 'versus', 'compare', 'difference between', 'better than'],
            'query_enhancement': 'comparison OR versus OR "vs"',
            'suggested_params': {'num_results': 10}
        },
        'factual': {
            'keywords': ['when', 'where', 'who', 'which', 'statistics', 'data'],
            'query_enhancement': '',
            'suggested_params': {'num_results': 5}
        },
        'recent_news': {
            'keywords': ['latest', 'recent', 'news', 'today', 'current'],
            'query_enhancement': 'news OR latest',
            'suggested_params': {'date_restrict': 'w1', 'sort_by': 'date'}
        },
        'academic': {
            'keywords': ['research', 'study', 'paper', 'journal', 'academic'],
            'query_enhancement': 'research OR study OR paper',
            'suggested_params': {'file_type': 'pdf', 'num_results': 10}
        },
        'product': {
            'keywords': ['buy', 'price', 'review', 'best', 'top rated'],
            'query_enhancement': 'review OR comparison',
            'suggested_params': {'num_results': 15}
        }
    }
    
    def analyze_query_intent(self, query: str) -> Dict[str, Any]:
        """
        åˆ†ææŸ¥è¯¢æ„å›¾
        
        Returns:
            {
                'original_query': 'how to learn python',
                'intent_type': 'how_to',
                'confidence': 0.9,
                'enhanced_query': 'how to learn python tutorial OR guide',
                'suggested_params': {'num_results': 10},
                'query_entities': ['python'],
                'query_modifiers': ['learn'],
                'suggestions': [
                    'Consider adding "beginner" for more targeted results',
                    'Try searching for "python tutorial" specifically'
                ]
            }
        """
        query_lower = query.lower()
        
        analysis = {
            'original_query': query,
            'intent_type': 'general',
            'confidence': 0.0,
            'enhanced_query': query,
            'suggested_params': {},
            'query_entities': [],
            'query_modifiers': [],
            'suggestions': []
        }
        
        # æ£€æµ‹æ„å›¾ç±»å‹
        max_confidence = 0.0
        detected_intent = 'general'
        
        for intent_type, intent_config in self.INTENT_PATTERNS.items():
            keywords = intent_config['keywords']
            matches = sum(1 for kw in keywords if kw in query_lower)
            
            if matches > 0:
                confidence = min(1.0, matches / len(keywords) * 2)
                if confidence > max_confidence:
                    max_confidence = confidence
                    detected_intent = intent_type
        
        analysis['intent_type'] = detected_intent
        analysis['confidence'] = max_confidence
        
        # å¢å¼ºæŸ¥è¯¢
        if detected_intent != 'general':
            intent_config = self.INTENT_PATTERNS[detected_intent]
            enhancement = intent_config['query_enhancement']
            
            if enhancement:
                analysis['enhanced_query'] = f"{query} {enhancement}"
            
            analysis['suggested_params'] = intent_config['suggested_params'].copy()
        
        # æå–å®ä½“å’Œä¿®é¥°è¯
        analysis['query_entities'] = self._extract_entities(query)
        analysis['query_modifiers'] = self._extract_modifiers(query)
        
        # ç”Ÿæˆå»ºè®®
        analysis['suggestions'] = self._generate_suggestions(query, detected_intent)
        
        return analysis
    
    def _extract_entities(self, query: str) -> List[str]:
        """æå–æŸ¥è¯¢ä¸­çš„å®ä½“ (ç®€åŒ–ç‰ˆ)"""
        # å®é™…åº”è¯¥ä½¿ç”¨ NER æ¨¡å‹
        # è¿™é‡Œç®€å•æå–å¯èƒ½çš„å®ä½“ (å¤§å†™è¯ã€ä¸“æœ‰åè¯ç­‰)
        words = query.split()
        entities = []
        
        for word in words:
            # ç®€å•è§„åˆ™: é¦–å­—æ¯å¤§å†™çš„è¯å¯èƒ½æ˜¯å®ä½“
            if word and word[0].isupper() and len(word) > 2:
                entities.append(word)
        
        return entities
    
    def _extract_modifiers(self, query: str) -> List[str]:
        """æå–æŸ¥è¯¢ä¿®é¥°è¯"""
        modifiers = []
        modifier_words = ['best', 'top', 'latest', 'new', 'old', 'cheap', 'expensive', 
                         'free', 'open source', 'commercial', 'beginner', 'advanced']
        
        query_lower = query.lower()
        for modifier in modifier_words:
            if modifier in query_lower:
                modifiers.append(modifier)
        
        return modifiers
    
    def _generate_suggestions(self, query: str, intent_type: str) -> List[str]:
        """ç”ŸæˆæŸ¥è¯¢ä¼˜åŒ–å»ºè®®"""
        suggestions = []
        
        if intent_type == 'how_to':
            if 'beginner' not in query.lower() and 'advanced' not in query.lower():
                suggestions.append(
                    'Consider adding "beginner" or "advanced" to target skill level'
                )
        
        elif intent_type == 'comparison':
            if ' vs ' not in query.lower():
                suggestions.append(
                    'Use "vs" or "versus" for better comparison results'
                )
        
        elif intent_type == 'academic':
            if 'pdf' not in query.lower():
                suggestions.append(
                    'Consider adding "filetype:pdf" to find research papers'
                )
        
        elif intent_type == 'recent_news':
            suggestions.append(
                'Results will be filtered to last week for freshness'
            )
        
        # é€šç”¨å»ºè®®
        if len(query.split()) < 3:
            suggestions.append(
                'Query is short - consider adding more specific terms'
            )
        
        if len(query.split()) > 10:
            suggestions.append(
                'Query is long - consider simplifying to key terms'
            )
        
        return suggestions
```

**é›†æˆåˆ° search_web**:
```python
def search_web(
    self,
    query: str,
    num_results: int = 10,
    auto_enhance: bool = True,  # æ–°å‚æ•°: è‡ªåŠ¨å¢å¼ºæŸ¥è¯¢
    **kwargs
) -> List[Dict[str, Any]]:
    """æœç´¢ç½‘é¡µ (å¢å¼ºç‰ˆ)"""
    
    if not query or not query.strip():
        raise ValidationError("Query cannot be empty")
    
    # åˆ†ææŸ¥è¯¢æ„å›¾
    if auto_enhance:
        intent_analyzer = QueryIntentAnalyzer()
        intent_analysis = intent_analyzer.analyze_query_intent(query)
        
        # ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢
        enhanced_query = intent_analysis['enhanced_query']
        
        # åˆå¹¶å»ºè®®çš„å‚æ•°
        for param, value in intent_analysis['suggested_params'].items():
            if param not in kwargs:
                kwargs[param] = value
        
        # è®°å½•æ„å›¾åˆ†æç»“æœ (ç”¨äºè°ƒè¯•å’Œ Agent ç†è§£)
        self.logger.info(
            f"Query intent: {intent_analysis['intent_type']} "
            f"(confidence: {intent_analysis['confidence']:.2f})"
        )
        
        # å°†æ„å›¾åˆ†ææ·»åŠ åˆ°å…ƒæ•°æ®
        self._last_intent_analysis = intent_analysis
    else:
        enhanced_query = query
    
    # æ‰§è¡Œæœç´¢
    raw_results = self._retry_with_backoff(
        self._execute_search,
        enhanced_query,
        num_results,
        **kwargs
    )
    
    # è§£æç»“æœ (åŒ…å«è´¨é‡åˆ†æ)
    results = self._parse_search_results(raw_results, query=query)
    
    # æ·»åŠ æ„å›¾åˆ†æåˆ°ç»“æœå…ƒæ•°æ®
    if auto_enhance and results:
        for result in results:
            result['_search_metadata'] = {
                'original_query': query,
                'enhanced_query': enhanced_query,
                'intent_type': intent_analysis['intent_type'],
                'intent_confidence': intent_analysis['confidence'],
                'suggestions': intent_analysis['suggestions']
            }
    
    return results
```

**å½±å“**: Agent çš„æŸ¥è¯¢è‡ªåŠ¨ä¼˜åŒ–,è·å¾—æ›´ç›¸å…³çš„ç»“æœã€‚

---

## 3. ç»“æœå‘ˆç°ä¼˜åŒ– (â­â­â­â­)

### 3.1 ç¼ºå°‘ç»“æ„åŒ–æ‘˜è¦

**ç°çŠ¶é—®é¢˜**:
- è¿”å›åŸå§‹æœç´¢ç»“æœåˆ—è¡¨
- Agent éœ€è¦è‡ªå·±å¤„ç†å’Œç†è§£ç»“æœ
- æ²¡æœ‰æä¾›ç»“æœæ¦‚è§ˆ

**ä¼˜åŒ–å»ºè®®**:
```python
class ResultSummarizer:
    """æœç´¢ç»“æœæ‘˜è¦ç”Ÿæˆå™¨"""
    
    def generate_summary(
        self, 
        results: List[Dict[str, Any]],
        query: str
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆæœç´¢ç»“æœæ‘˜è¦
        
        Returns:
            {
                'query': 'python tutorial',
                'total_results': 10,
                'quality_distribution': {
                    'high': 6,
                    'medium': 3,
                    'low': 1
                },
                'top_domains': [
                    {'domain': 'python.org', 'count': 2, 'avg_quality': 0.95},
                    {'domain': 'realpython.com', 'count': 1, 'avg_quality': 0.85}
                ],
                'content_types': {
                    'tutorial': 5,
                    'documentation': 3,
                    'blog': 2
                },
                'freshness_distribution': {
                    'very_fresh': 3,  # < 1 month
                    'fresh': 4,       # < 6 months
                    'moderate': 2,    # < 1 year
                    'old': 1          # > 1 year
                },
                'recommended_results': [
                    # å‰3ä¸ªæœ€é«˜è´¨é‡ç»“æœ
                ],
                'warnings': [
                    '1 low quality result detected',
                    '2 results lack HTTPS'
                ],
                'suggestions': [
                    'Consider filtering by date for more recent tutorials',
                    'Add "beginner" to query for introductory content'
                ]
            }
        """
        
        summary = {
            'query': query,
            'total_results': len(results),
            'quality_distribution': {'high': 0, 'medium': 0, 'low': 0},
            'top_domains': [],
            'content_types': {},
            'freshness_distribution': {
                'very_fresh': 0, 'fresh': 0, 'moderate': 0, 'old': 0
            },
            'recommended_results': [],
            'warnings': [],
            'suggestions': []
        }
        
        if not results:
            summary['warnings'].append('No results found')
            return summary
        
        # ç»Ÿè®¡è´¨é‡åˆ†å¸ƒ
        domain_stats = {}
        
        for result in results:
            quality = result.get('_quality', {})
            quality_level = quality.get('credibility_level', 'medium')
            summary['quality_distribution'][quality_level] += 1
            
            # ç»Ÿè®¡åŸŸå
            domain = result.get('displayLink', 'unknown')
            if domain not in domain_stats:
                domain_stats[domain] = {'count': 0, 'total_quality': 0.0}
            domain_stats[domain]['count'] += 1
            domain_stats[domain]['total_quality'] += quality.get('quality_score', 0.5)
            
            # ç»Ÿè®¡æ–°é²œåº¦
            freshness = quality.get('freshness_score', 0.5)
            if freshness > 0.9:
                summary['freshness_distribution']['very_fresh'] += 1
            elif freshness > 0.7:
                summary['freshness_distribution']['fresh'] += 1
            elif freshness > 0.5:
                summary['freshness_distribution']['moderate'] += 1
            else:
                summary['freshness_distribution']['old'] += 1
        
        # è®¡ç®—é¡¶çº§åŸŸå
        top_domains = []
        for domain, stats in domain_stats.items():
            avg_quality = stats['total_quality'] / stats['count']
            top_domains.append({
                'domain': domain,
                'count': stats['count'],
                'avg_quality': avg_quality
            })
        
        summary['top_domains'] = sorted(
            top_domains, 
            key=lambda x: (x['count'], x['avg_quality']),
            reverse=True
        )[:5]
        
        # æ¨èç»“æœ (å‰3ä¸ªæœ€é«˜è´¨é‡)
        sorted_results = sorted(
            results,
            key=lambda x: x.get('_quality', {}).get('quality_score', 0),
            reverse=True
        )
        summary['recommended_results'] = sorted_results[:3]
        
        # ç”Ÿæˆè­¦å‘Š
        if summary['quality_distribution']['low'] > 0:
            summary['warnings'].append(
                f"{summary['quality_distribution']['low']} low quality result(s) detected"
            )
        
        https_count = sum(1 for r in results if r.get('link', '').startswith('https://'))
        if https_count < len(results):
            summary['warnings'].append(
                f"{len(results) - https_count} result(s) lack HTTPS"
            )
        
        # ç”Ÿæˆå»ºè®®
        if summary['freshness_distribution']['old'] > len(results) / 2:
            summary['suggestions'].append(
                'Many results are outdated - consider adding date filter'
            )
        
        if summary['quality_distribution']['high'] < 3:
            summary['suggestions'].append(
                'Few high-quality results - try refining your query'
            )
        
        return summary
```

**å½±å“**: Agent å¿«é€Ÿäº†è§£æœç´¢ç»“æœæ¦‚å†µ,åšå‡ºæ›´å¥½çš„å†³ç­–ã€‚

---

## 4. ä¸Šä¸‹æ–‡æ„ŸçŸ¥æœç´¢ (â­â­â­â­)

### 4.1 ç¼ºå°‘æœç´¢å†å²å’Œä¸Šä¸‹æ–‡

**ç°çŠ¶é—®é¢˜**:
- æ¯æ¬¡æœç´¢éƒ½æ˜¯ç‹¬ç«‹çš„
- ä¸è®°ä½ä¹‹å‰çš„æœç´¢
- ä¸èƒ½åŸºäºä¸Šä¸‹æ–‡ä¼˜åŒ–åç»­æœç´¢

**ä¼˜åŒ–å»ºè®®**:
```python
class SearchContext:
    """æœç´¢ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self, max_history: int = 10):
        self.search_history = []
        self.max_history = max_history
        self.topic_context = None
        self.user_preferences = {
            'preferred_domains': set(),
            'avoided_domains': set(),
            'preferred_content_types': [],
            'language': 'en'
        }
    
    def add_search(
        self, 
        query: str, 
        results: List[Dict[str, Any]],
        user_feedback: Optional[Dict[str, Any]] = None
    ):
        """æ·»åŠ æœç´¢åˆ°å†å²"""
        
        search_record = {
            'timestamp': datetime.utcnow().isoformat(),
            'query': query,
            'result_count': len(results),
            'clicked_results': [],  # Agent ä½¿ç”¨çš„ç»“æœ
            'feedback': user_feedback
        }
        
        self.search_history.append(search_record)
        
        # ä¿æŒå†å²å¤§å°
        if len(self.search_history) > self.max_history:
            self.search_history.pop(0)
        
        # æ›´æ–°ä¸»é¢˜ä¸Šä¸‹æ–‡
        self._update_topic_context(query, results)
        
        # å­¦ä¹ ç”¨æˆ·åå¥½
        if user_feedback:
            self._learn_preferences(results, user_feedback)
    
    def get_contextual_suggestions(self, current_query: str) -> Dict[str, Any]:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆæœç´¢å»ºè®®"""
        
        suggestions = {
            'related_queries': [],
            'refinement_suggestions': [],
            'context_aware_params': {}
        }
        
        if not self.search_history:
            return suggestions
        
        # æ£€æµ‹ç›¸å…³çš„å†å²æŸ¥è¯¢
        for record in reversed(self.search_history[-5:]):
            prev_query = record['query']
            similarity = self._calculate_query_similarity(current_query, prev_query)
            
            if similarity > 0.5:
                suggestions['related_queries'].append({
                    'query': prev_query,
                    'similarity': similarity,
                    'timestamp': record['timestamp']
                })
        
        # åŸºäºåå¥½è°ƒæ•´å‚æ•°
        if self.user_preferences['preferred_domains']:
            # å¯ä»¥ä½¿ç”¨ site: è¿ç®—ç¬¦ä¼˜å…ˆæœç´¢åå¥½åŸŸå
            suggestions['context_aware_params']['preferred_sites'] = list(
                self.user_preferences['preferred_domains']
            )
        
        return suggestions
    
    def _update_topic_context(self, query: str, results: List[Dict[str, Any]]):
        """æ›´æ–°ä¸»é¢˜ä¸Šä¸‹æ–‡"""
        # ç®€åŒ–å®ç°: æå–å¸¸è§è¯ä½œä¸ºä¸»é¢˜
        words = query.lower().split()
        # å®é™…åº”è¯¥ä½¿ç”¨æ›´å¤æ‚çš„ä¸»é¢˜å»ºæ¨¡
        self.topic_context = words
    
    def _learn_preferences(
        self, 
        results: List[Dict[str, Any]], 
        feedback: Dict[str, Any]
    ):
        """ä»åé¦ˆä¸­å­¦ä¹ ç”¨æˆ·åå¥½"""
        
        # å¦‚æœ Agent ç‚¹å‡»/ä½¿ç”¨äº†æŸäº›ç»“æœ
        if 'clicked_indices' in feedback:
            for idx in feedback['clicked_indices']:
                if idx < len(results):
                    result = results[idx]
                    domain = result.get('displayLink', '')
                    self.user_preferences['preferred_domains'].add(domain)
        
        # å¦‚æœ Agent æ˜ç¡®æ ‡è®°äº†ä¸å–œæ¬¢çš„ç»“æœ
        if 'disliked_indices' in feedback:
            for idx in feedback['disliked_indices']:
                if idx < len(results):
                    result = results[idx]
                    domain = result.get('displayLink', '')
                    self.user_preferences['avoided_domains'].add(domain)
    
    def _calculate_query_similarity(self, query1: str, query2: str) -> float:
        """è®¡ç®—æŸ¥è¯¢ç›¸ä¼¼åº¦"""
        words1 = set(query1.lower().split())
        words2 = set(query2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1 & words2
        union = words1 | words2
        
        return len(intersection) / len(union)  # Jaccard ç›¸ä¼¼åº¦
```

**å½±å“**: Agent çš„æœç´¢ä½“éªŒæ›´è¿è´¯,åç»­æœç´¢æ›´ç²¾å‡†ã€‚

---

## 5. æ™ºèƒ½ç¼“å­˜ä¼˜åŒ– (â­â­â­)

### 5.1 å½“å‰ç¼“å­˜å®ç°ä¸è¶³

**ç°çŠ¶é—®é¢˜**:
```python
# search_tool.py ä¸­ç¼“å­˜é…ç½®å­˜åœ¨,ä½†å®ç°åŸºç¡€
cache_ttl: int = Field(
    default=3600,
    description="Cache time-to-live in seconds"
)
```

**é—®é¢˜**:
- âŒ æ‰€æœ‰æŸ¥è¯¢ä½¿ç”¨ç›¸åŒçš„ TTL
- âŒ ä¸è€ƒè™‘æŸ¥è¯¢ç±»å‹ (æ–°é—» vs å®šä¹‰)
- âŒ ä¸è€ƒè™‘ç»“æœæ–°é²œåº¦
- âŒ æ²¡æœ‰ç¼“å­˜é¢„çƒ­æœºåˆ¶

**ä¼˜åŒ–å»ºè®®**:
```python
class IntelligentCache:
    """æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""
    
    # ä¸åŒæŸ¥è¯¢ç±»å‹çš„ TTL ç­–ç•¥
    TTL_STRATEGIES = {
        'definition': 86400 * 30,      # å®šä¹‰ç±»æŸ¥è¯¢: 30å¤© (å¾ˆå°‘å˜åŒ–)
        'how_to': 86400 * 7,           # æ•™ç¨‹ç±»: 7å¤©
        'factual': 86400 * 7,          # äº‹å®ç±»: 7å¤©
        'academic': 86400 * 30,        # å­¦æœ¯ç±»: 30å¤© (è®ºæ–‡ä¸å˜)
        'recent_news': 3600,           # æ–°é—»ç±»: 1å°æ—¶ (å¿«é€Ÿå˜åŒ–)
        'product': 86400,              # äº§å“ç±»: 1å¤©
        'comparison': 86400 * 3,       # æ¯”è¾ƒç±»: 3å¤©
        'general': 3600                # é€šç”¨: 1å°æ—¶
    }
    
    def calculate_ttl(
        self, 
        query: str,
        intent_type: str,
        results: List[Dict[str, Any]]
    ) -> int:
        """
        è®¡ç®—æ™ºèƒ½ TTL
        
        è€ƒè™‘å› ç´ :
        1. æŸ¥è¯¢æ„å›¾ç±»å‹
        2. ç»“æœæ–°é²œåº¦
        3. ç»“æœè´¨é‡
        """
        
        # åŸºç¡€ TTL (åŸºäºæ„å›¾)
        base_ttl = self.TTL_STRATEGIES.get(intent_type, 3600)
        
        # æ ¹æ®ç»“æœæ–°é²œåº¦è°ƒæ•´
        if results:
            avg_freshness = sum(
                r.get('_quality', {}).get('freshness_score', 0.5)
                for r in results
            ) / len(results)
            
            # å¦‚æœç»“æœå¾ˆæ–°é²œ,å¯ä»¥ç¼“å­˜æ›´ä¹…
            if avg_freshness > 0.9:
                base_ttl *= 2
            # å¦‚æœç»“æœå¾ˆæ—§,ç¼“å­˜æ—¶é—´å‡åŠ
            elif avg_freshness < 0.3:
                base_ttl //= 2
        
        # æ ¹æ®ç»“æœè´¨é‡è°ƒæ•´
        if results:
            avg_quality = sum(
                r.get('_quality', {}).get('quality_score', 0.5)
                for r in results
            ) / len(results)
            
            # é«˜è´¨é‡ç»“æœå¯ä»¥ç¼“å­˜æ›´ä¹…
            if avg_quality > 0.8:
                base_ttl = int(base_ttl * 1.5)
        
        return base_ttl
    
    def should_refresh_cache(
        self,
        cached_time: datetime,
        query: str,
        intent_type: str
    ) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åˆ·æ–°ç¼“å­˜"""
        
        # æ–°é—»ç±»æŸ¥è¯¢æ€»æ˜¯åˆ·æ–°
        if intent_type == 'recent_news':
            age_hours = (datetime.utcnow() - cached_time).total_seconds() / 3600
            return age_hours > 1
        
        # å…¶ä»–ç±»å‹æ ¹æ® TTL åˆ¤æ–­
        ttl = self.TTL_STRATEGIES.get(intent_type, 3600)
        age_seconds = (datetime.utcnow() - cached_time).total_seconds()
        
        return age_seconds > ttl
```

**å½±å“**: å‡å°‘ä¸å¿…è¦çš„ API è°ƒç”¨,åŒæ—¶ä¿è¯ç»“æœæ–°é²œåº¦ã€‚

---

## ğŸ“Š ä¼˜åŒ–ä¼˜å…ˆçº§çŸ©é˜µ

| ä¼˜åŒ–é¡¹ | å½±å“ | å®ç°éš¾åº¦ | ä¼˜å…ˆçº§ | é¢„ä¼°å·¥ä½œé‡ |
|--------|------|----------|--------|------------|
| 1.1 ç»“æœè´¨é‡è¯„åˆ† | â­â­â­â­â­ | ğŸ”§ğŸ”§ğŸ”§ | **P0** | 4-5å¤© |
| 1.2 ç»“æœå»é‡ | â­â­â­ | ğŸ”§ğŸ”§ | **P1** | 2-3å¤© |
| 2.1 æŸ¥è¯¢æ„å›¾åˆ†æ | â­â­â­â­â­ | ğŸ”§ğŸ”§ğŸ”§ğŸ”§ | **P0** | 5-7å¤© |
| 3.1 ç»“æ„åŒ–æ‘˜è¦ | â­â­â­â­ | ğŸ”§ğŸ”§ | **P0** | 2-3å¤© |
| 4.1 æœç´¢ä¸Šä¸‹æ–‡ | â­â­â­â­ | ğŸ”§ğŸ”§ğŸ”§ | **P1** | 3-4å¤© |
| 5.1 æ™ºèƒ½ç¼“å­˜ | â­â­â­ | ğŸ”§ğŸ”§ | **P2** | 2-3å¤© |

**æ€»è®¡**: çº¦ 18-25 å¤©å·¥ä½œé‡

---

## ğŸ¯ å¿«é€Ÿèƒœåˆ©ï¼ˆQuick Winsï¼‰

ä»¥ä¸‹ä¼˜åŒ–å¯ä»¥åœ¨ 1-2 å¤©å†…å®Œæˆ:

### 1. æ·»åŠ åŸºç¡€è´¨é‡æŒ‡æ ‡ (1å¤©)
```python
# åœ¨ _parse_search_results ä¸­æ·»åŠ 
result['_basic_quality'] = {
    'has_https': result['link'].startswith('https://'),
    'domain': result['displayLink'],
    'position': position,
    'snippet_length': len(result['snippet'])
}
```

### 2. æ·»åŠ æŸ¥è¯¢æ—¥å¿— (0.5å¤©)
```python
# è®°å½•æ‰€æœ‰æŸ¥è¯¢ç”¨äºåˆ†æ
self.logger.info(
    f"Search query: '{query}' | "
    f"Results: {len(results)} | "
    f"Type: {search_type}"
)
```

### 3. æ·»åŠ ç»“æœå…ƒæ•°æ® (1å¤©)
```python
# åœ¨è¿”å›ç»“æœæ—¶æ·»åŠ æœç´¢å…ƒæ•°æ®
return {
    'results': results,
    'metadata': {
        'query': query,
        'total_results': len(results),
        'search_type': search_type,
        'timestamp': datetime.utcnow().isoformat()
    }
}
```

---

## ğŸ“ˆ é¢„æœŸæ”¶ç›Š

å®æ–½æ‰€æœ‰ P0 ä¼˜åŒ–å,é¢„æœŸ:

1. **ç»“æœç›¸å…³æ€§**: æå‡ 40-60%
2. **Agent æ»¡æ„åº¦**: æå‡ 50-70%
3. **æŸ¥è¯¢æˆåŠŸç‡**: ä» ~75% æå‡åˆ° ~90%
4. **ç»“æœè´¨é‡**: å¹³å‡è´¨é‡åˆ†æ•° 0.75+
5. **API ä½¿ç”¨æ•ˆç‡**: é€šè¿‡æ™ºèƒ½ç¼“å­˜å‡å°‘ 30-40% è°ƒç”¨

---

## ğŸ”š ç»“è®º

å½“å‰çš„ `search_tool` å®ç°äº†å®Œå–„çš„åŸºç¡€è®¾æ–½ (é€Ÿç‡é™åˆ¶ã€ç†”æ–­å™¨ã€é‡è¯•),ä½†åœ¨å¸®åŠ© Agent è·å–é«˜è´¨é‡æœç´¢ç»“æœæ–¹é¢è¿˜æœ‰å¾ˆå¤§æå‡ç©ºé—´ã€‚

**æœ€å…³é”®çš„ä¼˜åŒ–æ–¹å‘**:
1. **ç»“æœè´¨é‡è¯„åˆ†** - è®© Agent çŸ¥é“å“ªäº›ç»“æœæ›´å¯é 
2. **æŸ¥è¯¢æ„å›¾åˆ†æ** - è‡ªåŠ¨ä¼˜åŒ–æŸ¥è¯¢ä»¥è·å¾—æ›´å¥½ç»“æœ
3. **ç»“æ„åŒ–æ‘˜è¦** - å¸®åŠ© Agent å¿«é€Ÿç†è§£æœç´¢ç»“æœ
4. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥** - åŸºäºå†å²å’Œåå¥½ä¼˜åŒ–æœç´¢

å»ºè®®æŒ‰ç…§ P0 â†’ P1 â†’ P2 çš„é¡ºåºé€æ­¥å®æ–½ä¼˜åŒ–ã€‚

---

## 6. å¤šæ¨¡æ€æœç´¢å¢å¼º (â­â­â­â­)

### 6.1 å›¾ç‰‡æœç´¢ç¼ºå°‘è§†è§‰è´¨é‡è¯„ä¼°

**ç°çŠ¶é—®é¢˜**:
```python
# search_tool.py:732-790 - search_images åªè¿”å›åŸºç¡€å…ƒæ•°æ®
def search_images(self, query: str, num_results: int = 10, ...):
    search_params = {
        'searchType': 'image',
        'safe': safe_search,
    }
    # è¿”å›åŸå§‹ç»“æœ,æ²¡æœ‰è´¨é‡è¯„ä¼°
    return self._parse_search_results(raw_results)
```

**é—®é¢˜**:
- âŒ ä¸è¯„ä¼°å›¾ç‰‡åˆ†è¾¨ç‡æ˜¯å¦è¶³å¤Ÿ
- âŒ ä¸æ£€æŸ¥å›¾ç‰‡æ˜¯å¦å¯è®¿é—®
- âŒ ä¸è¯„ä¼°å›¾ç‰‡ç›¸å…³æ€§
- âŒ ä¸æä¾›å›¾ç‰‡ä½¿ç”¨å»ºè®® (ç‰ˆæƒã€å°ºå¯¸ç­‰)

**ä¼˜åŒ–å»ºè®®**:
```python
class ImageQualityAnalyzer:
    """å›¾ç‰‡è´¨é‡åˆ†æå™¨"""

    # æœ€å°æ¨èåˆ†è¾¨ç‡ (ç”¨é€” -> æœ€å°åƒç´ )
    MIN_RESOLUTION = {
        'thumbnail': (150, 150),
        'web_display': (800, 600),
        'print': (2400, 1800),
        'hd': (1920, 1080),
        '4k': (3840, 2160)
    }

    def analyze_image_quality(
        self,
        image_result: Dict[str, Any],
        intended_use: str = 'web_display'
    ) -> Dict[str, Any]:
        """
        åˆ†æå›¾ç‰‡è´¨é‡

        Returns:
            {
                'quality_score': 0.85,
                'resolution_adequate': True,
                'file_size_appropriate': True,
                'format_suitable': True,
                'accessibility_score': 0.9,
                'usage_recommendations': {
                    'suitable_for': ['web_display', 'thumbnail'],
                    'not_suitable_for': ['print', '4k'],
                    'suggested_use_cases': ['blog post', 'presentation']
                },
                'technical_details': {
                    'width': 1200,
                    'height': 800,
                    'aspect_ratio': '3:2',
                    'file_size_kb': 245,
                    'format': 'jpeg',
                    'estimated_quality': 'high'
                },
                'warnings': []
            }
        """

        analysis = {
            'quality_score': 0.0,
            'resolution_adequate': False,
            'file_size_appropriate': False,
            'format_suitable': False,
            'accessibility_score': 0.0,
            'usage_recommendations': {
                'suitable_for': [],
                'not_suitable_for': [],
                'suggested_use_cases': []
            },
            'technical_details': {},
            'warnings': []
        }

        # æå–å›¾ç‰‡å…ƒæ•°æ®
        image_meta = image_result.get('image', {})
        width = image_meta.get('width', 0)
        height = image_meta.get('height', 0)
        byte_size = image_meta.get('byteSize', 0)

        # æŠ€æœ¯ç»†èŠ‚
        analysis['technical_details'] = {
            'width': width,
            'height': height,
            'aspect_ratio': self._calculate_aspect_ratio(width, height),
            'file_size_kb': byte_size // 1024 if byte_size else 0,
            'format': self._extract_format(image_result.get('link', '')),
            'estimated_quality': 'unknown'
        }

        # 1. è¯„ä¼°åˆ†è¾¨ç‡
        min_width, min_height = self.MIN_RESOLUTION.get(intended_use, (800, 600))

        if width >= min_width and height >= min_height:
            analysis['resolution_adequate'] = True
            resolution_score = 1.0
        else:
            analysis['resolution_adequate'] = False
            resolution_score = min(1.0, (width * height) / (min_width * min_height))
            analysis['warnings'].append(
                f"Resolution {width}x{height} may be too low for {intended_use}"
            )

        # 2. è¯„ä¼°æ–‡ä»¶å¤§å°
        if byte_size > 0:
            size_kb = byte_size // 1024

            # åˆç†çš„æ–‡ä»¶å¤§å°èŒƒå›´ (åŸºäºåˆ†è¾¨ç‡)
            pixels = width * height
            expected_size_kb = pixels / 1000  # ç²—ç•¥ä¼°è®¡

            if 0.5 * expected_size_kb <= size_kb <= 3 * expected_size_kb:
                analysis['file_size_appropriate'] = True
                size_score = 1.0
            else:
                size_score = 0.7
                if size_kb > 3 * expected_size_kb:
                    analysis['warnings'].append(
                        f"File size {size_kb}KB may be too large (slow loading)"
                    )
                else:
                    analysis['warnings'].append(
                        f"File size {size_kb}KB may indicate low quality"
                    )
        else:
            size_score = 0.5

        # 3. è¯„ä¼°æ ¼å¼
        img_format = analysis['technical_details']['format']
        suitable_formats = ['jpg', 'jpeg', 'png', 'webp']

        if img_format in suitable_formats:
            analysis['format_suitable'] = True
            format_score = 1.0
        else:
            format_score = 0.6
            analysis['warnings'].append(
                f"Format '{img_format}' may not be widely supported"
            )

        # 4. è¯„ä¼°å¯è®¿é—®æ€§ (å›¾ç‰‡æ˜¯å¦å¯èƒ½å¯è®¿é—®)
        link = image_result.get('link', '')
        context_link = image_meta.get('contextLink', '')

        accessibility_score = 0.5  # é»˜è®¤
        if link.startswith('https://'):
            accessibility_score += 0.3
        if context_link:
            accessibility_score += 0.2

        analysis['accessibility_score'] = min(1.0, accessibility_score)

        # 5. ç»¼åˆè´¨é‡åˆ†æ•°
        analysis['quality_score'] = (
            resolution_score * 0.4 +
            size_score * 0.2 +
            format_score * 0.2 +
            accessibility_score * 0.2
        )

        # 6. ä½¿ç”¨å»ºè®®
        for use_case, (min_w, min_h) in self.MIN_RESOLUTION.items():
            if width >= min_w and height >= min_h:
                analysis['usage_recommendations']['suitable_for'].append(use_case)
            else:
                analysis['usage_recommendations']['not_suitable_for'].append(use_case)

        # å»ºè®®çš„ä½¿ç”¨åœºæ™¯
        if width >= 1920 and height >= 1080:
            analysis['usage_recommendations']['suggested_use_cases'].extend([
                'hero image', 'banner', 'presentation', 'print'
            ])
        elif width >= 800:
            analysis['usage_recommendations']['suggested_use_cases'].extend([
                'blog post', 'article', 'social media'
            ])
        else:
            analysis['usage_recommendations']['suggested_use_cases'].extend([
                'thumbnail', 'icon', 'avatar'
            ])

        return analysis

    def _calculate_aspect_ratio(self, width: int, height: int) -> str:
        """è®¡ç®—å®½é«˜æ¯”"""
        if width == 0 or height == 0:
            return 'unknown'

        from math import gcd
        divisor = gcd(width, height)
        ratio_w = width // divisor
        ratio_h = height // divisor

        # å¸¸è§å®½é«˜æ¯”
        common_ratios = {
            (16, 9): '16:9',
            (4, 3): '4:3',
            (3, 2): '3:2',
            (1, 1): '1:1',
            (21, 9): '21:9'
        }

        return common_ratios.get((ratio_w, ratio_h), f'{ratio_w}:{ratio_h}')

    def _extract_format(self, url: str) -> str:
        """ä» URL æå–å›¾ç‰‡æ ¼å¼"""
        import os
        ext = os.path.splitext(url)[1].lower().lstrip('.')
        return ext if ext else 'unknown'
```

**å½±å“**: Agent å¯ä»¥é€‰æ‹©é€‚åˆç”¨é€”çš„é«˜è´¨é‡å›¾ç‰‡ã€‚

---

### 6.2 æ–°é—»æœç´¢ç¼ºå°‘æ—¶æ•ˆæ€§å’Œå¯ä¿¡åº¦è¯„ä¼°

**ç°çŠ¶é—®é¢˜**:
```python
# search_tool.py:792-846 - search_news ç®€å•æ·»åŠ  "news" å…³é”®è¯
def search_news(self, query: str, ...):
    # åªæ˜¯åœ¨æŸ¥è¯¢ä¸­æ·»åŠ  "news"
    news_query = f"{query} news"
    # æ²¡æœ‰éªŒè¯æ˜¯å¦çœŸçš„æ˜¯æ–°é—»æº
    # æ²¡æœ‰è¯„ä¼°æ–°é—»å¯ä¿¡åº¦
```

**ä¼˜åŒ–å»ºè®®**:
```python
class NewsQualityAnalyzer:
    """æ–°é—»è´¨é‡åˆ†æå™¨"""

    # çŸ¥åæ–°é—»æºè¯„çº§
    NEWS_SOURCE_RATINGS = {
        # å›½é™…ä¸»æµåª’ä½“
        'reuters.com': {'credibility': 0.95, 'bias': 'center', 'type': 'wire'},
        'apnews.com': {'credibility': 0.95, 'bias': 'center', 'type': 'wire'},
        'bbc.com': {'credibility': 0.90, 'bias': 'center-left', 'type': 'broadcast'},
        'nytimes.com': {'credibility': 0.90, 'bias': 'center-left', 'type': 'newspaper'},
        'wsj.com': {'credibility': 0.90, 'bias': 'center-right', 'type': 'newspaper'},
        'theguardian.com': {'credibility': 0.85, 'bias': 'left', 'type': 'newspaper'},
        'washingtonpost.com': {'credibility': 0.85, 'bias': 'center-left', 'type': 'newspaper'},

        # ç§‘æŠ€åª’ä½“
        'techcrunch.com': {'credibility': 0.80, 'bias': 'center', 'type': 'tech'},
        'arstechnica.com': {'credibility': 0.85, 'bias': 'center', 'type': 'tech'},
        'theverge.com': {'credibility': 0.80, 'bias': 'center', 'type': 'tech'},

        # è´¢ç»åª’ä½“
        'bloomberg.com': {'credibility': 0.90, 'bias': 'center', 'type': 'financial'},
        'ft.com': {'credibility': 0.90, 'bias': 'center', 'type': 'financial'},

        # ä½å¯ä¿¡åº¦æ ‡è®°
        'clickbait-news.com': {'credibility': 0.2, 'bias': 'unknown', 'type': 'questionable'},
    }

    def analyze_news_quality(
        self,
        news_result: Dict[str, Any],
        query: str
    ) -> Dict[str, Any]:
        """
        åˆ†ææ–°é—»è´¨é‡

        Returns:
            {
                'credibility_score': 0.90,
                'source_rating': {
                    'name': 'Reuters',
                    'credibility': 0.95,
                    'bias': 'center',
                    'type': 'wire'
                },
                'timeliness_score': 0.95,  # æ–°é²œåº¦
                'relevance_score': 0.85,
                'article_quality': {
                    'has_author': True,
                    'has_date': True,
                    'has_image': True,
                    'estimated_length': 'medium'
                },
                'trust_signals': [
                    'Established news source',
                    'Published within 24 hours',
                    'Author identified'
                ],
                'warnings': [],
                'recommendation': 'highly_recommended'  # highly/recommended/use_caution/avoid
            }
        """

        analysis = {
            'credibility_score': 0.5,
            'source_rating': None,
            'timeliness_score': 0.0,
            'relevance_score': 0.0,
            'article_quality': {},
            'trust_signals': [],
            'warnings': [],
            'recommendation': 'use_caution'
        }

        domain = news_result.get('displayLink', '').lower()

        # 1. è¯„ä¼°æ–°é—»æº
        source_rating = self.NEWS_SOURCE_RATINGS.get(domain)

        if source_rating:
            analysis['source_rating'] = {
                'name': domain,
                **source_rating
            }
            analysis['credibility_score'] = source_rating['credibility']

            if source_rating['credibility'] > 0.85:
                analysis['trust_signals'].append('Established news source')

            if source_rating['bias'] != 'unknown':
                analysis['trust_signals'].append(
                    f"Known bias: {source_rating['bias']}"
                )
        else:
            # æœªçŸ¥æ¥æº,é™ä½å¯ä¿¡åº¦
            analysis['credibility_score'] = 0.5
            analysis['warnings'].append('Unknown news source - verify credibility')

        # 2. è¯„ä¼°æ—¶æ•ˆæ€§
        metadata = news_result.get('metadata', {})
        publish_date = self._extract_publish_date(metadata)

        if publish_date:
            from datetime import datetime
            try:
                pub_dt = datetime.fromisoformat(publish_date.replace('Z', '+00:00'))
                now = datetime.now(pub_dt.tzinfo)
                hours_old = (now - pub_dt).total_seconds() / 3600

                # æ—¶æ•ˆæ€§è¯„åˆ†
                if hours_old < 24:
                    analysis['timeliness_score'] = 1.0
                    analysis['trust_signals'].append('Published within 24 hours')
                elif hours_old < 168:  # 1 week
                    analysis['timeliness_score'] = 0.8
                elif hours_old < 720:  # 1 month
                    analysis['timeliness_score'] = 0.6
                else:
                    analysis['timeliness_score'] = 0.3
                    analysis['warnings'].append('Article may be outdated')

                analysis['article_quality']['has_date'] = True
            except:
                analysis['timeliness_score'] = 0.5
                analysis['article_quality']['has_date'] = False
        else:
            analysis['timeliness_score'] = 0.5
            analysis['warnings'].append('No publication date found')
            analysis['article_quality']['has_date'] = False

        # 3. æ£€æŸ¥æ–‡ç« è´¨é‡æŒ‡æ ‡
        # ä½œè€…
        has_author = self._has_author(metadata)
        analysis['article_quality']['has_author'] = has_author
        if has_author:
            analysis['trust_signals'].append('Author identified')

        # å›¾ç‰‡
        has_image = 'image' in news_result or self._has_image_in_metadata(metadata)
        analysis['article_quality']['has_image'] = has_image

        # æ–‡ç« é•¿åº¦ä¼°è®¡
        snippet_length = len(news_result.get('snippet', ''))
        if snippet_length > 150:
            analysis['article_quality']['estimated_length'] = 'long'
        elif snippet_length > 80:
            analysis['article_quality']['estimated_length'] = 'medium'
        else:
            analysis['article_quality']['estimated_length'] = 'short'
            analysis['warnings'].append('Article may lack detail')

        # 4. è®¡ç®—ç›¸å…³æ€§
        title = news_result.get('title', '')
        snippet = news_result.get('snippet', '')
        query_terms = set(query.lower().split())

        title_matches = sum(1 for term in query_terms if term in title.lower())
        snippet_matches = sum(1 for term in query_terms if term in snippet.lower())

        analysis['relevance_score'] = min(1.0, (
            (title_matches / len(query_terms) if query_terms else 0) * 0.6 +
            (snippet_matches / len(query_terms) if query_terms else 0) * 0.4
        ))

        # 5. ç»¼åˆæ¨è
        overall_score = (
            analysis['credibility_score'] * 0.4 +
            analysis['timeliness_score'] * 0.3 +
            analysis['relevance_score'] * 0.3
        )

        if overall_score > 0.8:
            analysis['recommendation'] = 'highly_recommended'
        elif overall_score > 0.6:
            analysis['recommendation'] = 'recommended'
        elif overall_score > 0.4:
            analysis['recommendation'] = 'use_caution'
        else:
            analysis['recommendation'] = 'avoid'

        return analysis

    def _extract_publish_date(self, metadata: Dict) -> Optional[str]:
        """ä»å…ƒæ•°æ®æå–å‘å¸ƒæ—¥æœŸ"""
        date_fields = ['datepublished', 'publishdate', 'article:published_time']

        for field_group in metadata.values():
            if isinstance(field_group, list):
                for item in field_group:
                    if isinstance(item, dict):
                        for date_field in date_fields:
                            if date_field in item:
                                return item[date_field]
        return None

    def _has_author(self, metadata: Dict) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰ä½œè€…ä¿¡æ¯"""
        author_fields = ['author', 'article:author', 'creator']

        for field_group in metadata.values():
            if isinstance(field_group, list):
                for item in field_group:
                    if isinstance(item, dict):
                        for author_field in author_fields:
                            if author_field in item and item[author_field]:
                                return True
        return False

    def _has_image_in_metadata(self, metadata: Dict) -> bool:
        """æ£€æŸ¥å…ƒæ•°æ®ä¸­æ˜¯å¦æœ‰å›¾ç‰‡"""
        image_fields = ['image', 'og:image', 'twitter:image']

        for field_group in metadata.values():
            if isinstance(field_group, list):
                for item in field_group:
                    if isinstance(item, dict):
                        for img_field in image_fields:
                            if img_field in item:
                                return True
        return False
```

**å½±å“**: Agent å¯ä»¥è¯†åˆ«å¯ä¿¡çš„æ–°é—»æº,é¿å…è™šå‡ä¿¡æ¯ã€‚

---

## 7. é”™è¯¯å¤„ç†å’Œç”¨æˆ·ä½“éªŒä¼˜åŒ– (â­â­â­â­)

### 7.1 é”™è¯¯æ¶ˆæ¯å¯¹ Agent ä¸å¤Ÿå‹å¥½

**ç°çŠ¶é—®é¢˜**:
```python
# search_tool.py:545-617 - _execute_search çš„é”™è¯¯å¤„ç†
except HttpError as e:
    error_msg = str(e)
    # é”™è¯¯æ¶ˆæ¯æ˜¯ç»™å¼€å‘è€…çœ‹çš„,ä¸æ˜¯ç»™ Agent çœ‹çš„
    if 'quotaExceeded' in error_msg or 'rateLimitExceeded' in error_msg:
        raise QuotaExceededError(f"Google API quota exceeded: {error_msg}")
```

**ä¼˜åŒ–å»ºè®®**:
```python
class AgentFriendlyErrorHandler:
    """Agent å‹å¥½çš„é”™è¯¯å¤„ç†å™¨"""

    def format_error_for_agent(
        self,
        error: Exception,
        context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        å°†é”™è¯¯æ ¼å¼åŒ–ä¸º Agent å‹å¥½çš„æ¶ˆæ¯

        Returns:
            {
                'error_type': 'quota_exceeded',
                'severity': 'high',  # high/medium/low
                'user_message': 'Search quota exceeded. Please try again later.',
                'technical_details': '...',
                'suggested_actions': [
                    'Wait 60 seconds before retrying',
                    'Try a more specific query to reduce results',
                    'Use cached results if available'
                ],
                'alternative_approaches': [
                    'Use a different search engine',
                    'Search for specific domains using site: operator'
                ],
                'can_retry': False,
                'estimated_recovery_time': '1 hour'
            }
        """

        error_response = {
            'error_type': 'unknown',
            'severity': 'medium',
            'user_message': '',
            'technical_details': str(error),
            'suggested_actions': [],
            'alternative_approaches': [],
            'can_retry': False,
            'estimated_recovery_time': None
        }

        error_str = str(error).lower()

        # 1. é…é¢è¶…é™
        if 'quota' in error_str or 'rate limit' in error_str:
            error_response.update({
                'error_type': 'quota_exceeded',
                'severity': 'high',
                'user_message': (
                    'Search API quota has been exceeded. '
                    'The service has temporarily reached its usage limit.'
                ),
                'suggested_actions': [
                    'Wait 60-120 seconds before retrying',
                    'Reduce the number of results requested',
                    'Use more specific queries to get better results with fewer searches',
                    'Check if cached results are available'
                ],
                'alternative_approaches': [
                    'Use the scraper tool to extract information from known URLs',
                    'Query specific authoritative domains using site: operator',
                    'Defer non-urgent searches to later'
                ],
                'can_retry': True,
                'estimated_recovery_time': '1-2 minutes'
            })

        # 2. è®¤è¯é”™è¯¯
        elif 'auth' in error_str or 'credential' in error_str or 'api key' in error_str:
            error_response.update({
                'error_type': 'authentication_failed',
                'severity': 'high',
                'user_message': (
                    'Search API authentication failed. '
                    'The API credentials may be invalid or expired.'
                ),
                'suggested_actions': [
                    'Verify that GOOGLE_API_KEY is set correctly in environment',
                    'Check that GOOGLE_CSE_ID is valid',
                    'Ensure API key has not expired',
                    'Verify API key has Custom Search API enabled'
                ],
                'alternative_approaches': [
                    'Use alternative data sources (apisource_tool)',
                    'Request manual search from user'
                ],
                'can_retry': False,
                'estimated_recovery_time': None
            })

        # 3. ç½‘ç»œé”™è¯¯
        elif 'timeout' in error_str or 'connection' in error_str or 'network' in error_str:
            error_response.update({
                'error_type': 'network_error',
                'severity': 'medium',
                'user_message': (
                    'Network connection to search API failed. '
                    'This is usually a temporary issue.'
                ),
                'suggested_actions': [
                    'Retry the search in 5-10 seconds',
                    'Check internet connectivity',
                    'Try with a shorter timeout if query is complex'
                ],
                'alternative_approaches': [
                    'Use cached results if available',
                    'Try alternative search parameters'
                ],
                'can_retry': True,
                'estimated_recovery_time': '10-30 seconds'
            })

        # 4. æ— æ•ˆæŸ¥è¯¢
        elif 'invalid' in error_str or 'validation' in error_str:
            error_response.update({
                'error_type': 'invalid_query',
                'severity': 'low',
                'user_message': (
                    'The search query or parameters are invalid. '
                    'Please check the query format.'
                ),
                'suggested_actions': [
                    'Simplify the query - remove special characters',
                    'Check that all parameters are within valid ranges',
                    'Ensure query is not empty',
                    'Review query syntax for search operators'
                ],
                'alternative_approaches': [
                    'Break complex query into simpler parts',
                    'Use basic search without advanced operators'
                ],
                'can_retry': True,
                'estimated_recovery_time': 'immediate (after fixing query)'
            })

        # 5. ç†”æ–­å™¨æ‰“å¼€
        elif 'circuit breaker' in error_str:
            error_response.update({
                'error_type': 'circuit_breaker_open',
                'severity': 'high',
                'user_message': (
                    'Search service is temporarily unavailable due to repeated failures. '
                    'The circuit breaker has been triggered for protection.'
                ),
                'suggested_actions': [
                    f"Wait {context.get('circuit_breaker_timeout', 60)} seconds for circuit to reset",
                    'Check search service status',
                    'Review recent error logs'
                ],
                'alternative_approaches': [
                    'Use alternative data sources',
                    'Defer search to later',
                    'Use cached or historical data'
                ],
                'can_retry': True,
                'estimated_recovery_time': f"{context.get('circuit_breaker_timeout', 60)} seconds"
            })

        # 6. æ— ç»“æœ
        elif 'no results' in error_str or 'not found' in error_str:
            error_response.update({
                'error_type': 'no_results',
                'severity': 'low',
                'user_message': (
                    'No search results found for the query. '
                    'Try broadening your search terms.'
                ),
                'suggested_actions': [
                    'Remove overly specific terms',
                    'Try synonyms or related terms',
                    'Remove date restrictions',
                    'Broaden the search scope'
                ],
                'alternative_approaches': [
                    'Search for related topics',
                    'Try different search engines or sources',
                    'Break down into sub-queries'
                ],
                'can_retry': True,
                'estimated_recovery_time': 'immediate (with modified query)'
            })

        return error_response
```

**å½±å“**: Agent é‡åˆ°é”™è¯¯æ—¶çŸ¥é“å¦‚ä½•å¤„ç†,æé«˜è‡ªä¸»è§£å†³é—®é¢˜çš„èƒ½åŠ›ã€‚

---

## 8. æ€§èƒ½å’Œå¯è§‚æµ‹æ€§ä¼˜åŒ– (â­â­â­)

### 8.1 ç¼ºå°‘è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡

**ç°çŠ¶é—®é¢˜**:
```python
# search_tool.py:462-469 - åŸºç¡€æŒ‡æ ‡
self.metrics = {
    'total_requests': 0,
    'successful_requests': 0,
    'failed_requests': 0,
    'cache_hits': 0,
    'rate_limit_errors': 0,
    'circuit_breaker_trips': 0
}
# ç¼ºå°‘å“åº”æ—¶é—´ã€æŸ¥è¯¢è´¨é‡ç­‰æŒ‡æ ‡
```

**ä¼˜åŒ–å»ºè®®**:
```python
class EnhancedMetrics:
    """å¢å¼ºçš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†"""

    def __init__(self):
        self.metrics = {
            # åŸºç¡€è®¡æ•°
            'requests': {
                'total': 0,
                'successful': 0,
                'failed': 0,
                'cached': 0
            },

            # æ€§èƒ½æŒ‡æ ‡
            'performance': {
                'response_times_ms': [],  # æœ€è¿‘100æ¬¡
                'avg_response_time_ms': 0,
                'p50_response_time_ms': 0,
                'p95_response_time_ms': 0,
                'p99_response_time_ms': 0,
                'slowest_query': None,
                'fastest_query': None
            },

            # æŸ¥è¯¢è´¨é‡
            'quality': {
                'avg_results_per_query': 0,
                'avg_quality_score': 0,
                'high_quality_results_pct': 0,
                'queries_with_no_results': 0
            },

            # é”™è¯¯åˆ†æ
            'errors': {
                'by_type': {},  # {'quota': 5, 'network': 2}
                'recent_errors': [],  # æœ€è¿‘10ä¸ª
                'error_rate': 0.0
            },

            # ç¼“å­˜æ•ˆç‡
            'cache': {
                'hit_rate': 0.0,
                'total_hits': 0,
                'total_misses': 0,
                'avg_age_seconds': 0
            },

            # é€Ÿç‡é™åˆ¶
            'rate_limiting': {
                'throttled_requests': 0,
                'avg_wait_time_ms': 0,
                'quota_utilization_pct': 0
            },

            # æŸ¥è¯¢æ¨¡å¼
            'patterns': {
                'top_query_types': {},  # {'how_to': 15, 'definition': 10}
                'top_domains_returned': {},  # {'wikipedia.org': 25}
                'avg_query_length': 0
            }
        }

    def record_search(
        self,
        query: str,
        search_type: str,
        results: List[Dict[str, Any]],
        response_time_ms: float,
        cached: bool = False,
        error: Optional[Exception] = None
    ):
        """è®°å½•æœç´¢æŒ‡æ ‡"""

        # æ›´æ–°è¯·æ±‚è®¡æ•°
        self.metrics['requests']['total'] += 1

        if error:
            self.metrics['requests']['failed'] += 1
            self._record_error(error)
        else:
            self.metrics['requests']['successful'] += 1

        if cached:
            self.metrics['requests']['cached'] += 1
            self.metrics['cache']['total_hits'] += 1
        else:
            self.metrics['cache']['total_misses'] += 1

        # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        self.metrics['performance']['response_times_ms'].append(response_time_ms)
        if len(self.metrics['performance']['response_times_ms']) > 100:
            self.metrics['performance']['response_times_ms'].pop(0)

        self._update_percentiles()

        # è®°å½•æœ€æ…¢/æœ€å¿«æŸ¥è¯¢
        if (not self.metrics['performance']['slowest_query'] or
            response_time_ms > self.metrics['performance']['slowest_query']['time']):
            self.metrics['performance']['slowest_query'] = {
                'query': query,
                'time': response_time_ms,
                'type': search_type
            }

        if (not self.metrics['performance']['fastest_query'] or
            response_time_ms < self.metrics['performance']['fastest_query']['time']):
            self.metrics['performance']['fastest_query'] = {
                'query': query,
                'time': response_time_ms,
                'type': search_type
            }

        # æ›´æ–°è´¨é‡æŒ‡æ ‡
        if results:
            result_count = len(results)
            avg_quality = sum(
                r.get('_quality', {}).get('quality_score', 0.5)
                for r in results
            ) / result_count

            high_quality_count = sum(
                1 for r in results
                if r.get('_quality', {}).get('quality_score', 0) > 0.75
            )

            # æ›´æ–°å¹³å‡å€¼
            total = self.metrics['requests']['successful']
            current_avg_results = self.metrics['quality']['avg_results_per_query']
            self.metrics['quality']['avg_results_per_query'] = (
                (current_avg_results * (total - 1) + result_count) / total
            )

            current_avg_quality = self.metrics['quality']['avg_quality_score']
            self.metrics['quality']['avg_quality_score'] = (
                (current_avg_quality * (total - 1) + avg_quality) / total
            )

            current_high_pct = self.metrics['quality']['high_quality_results_pct']
            high_pct = high_quality_count / result_count
            self.metrics['quality']['high_quality_results_pct'] = (
                (current_high_pct * (total - 1) + high_pct) / total
            )
        else:
            self.metrics['quality']['queries_with_no_results'] += 1

        # æ›´æ–°æŸ¥è¯¢æ¨¡å¼
        query_type = self._detect_query_type(query)
        self.metrics['patterns']['top_query_types'][query_type] = (
            self.metrics['patterns']['top_query_types'].get(query_type, 0) + 1
        )

        # ç»Ÿè®¡è¿”å›çš„åŸŸå
        for result in results:
            domain = result.get('displayLink', 'unknown')
            self.metrics['patterns']['top_domains_returned'][domain] = (
                self.metrics['patterns']['top_domains_returned'].get(domain, 0) + 1
            )

        # æ›´æ–°å¹³å‡æŸ¥è¯¢é•¿åº¦
        total = self.metrics['requests']['total']
        current_avg_len = self.metrics['patterns']['avg_query_length']
        self.metrics['patterns']['avg_query_length'] = (
            (current_avg_len * (total - 1) + len(query.split())) / total
        )

        # æ›´æ–°ç¼“å­˜å‘½ä¸­ç‡
        total_cache_requests = (
            self.metrics['cache']['total_hits'] +
            self.metrics['cache']['total_misses']
        )
        if total_cache_requests > 0:
            self.metrics['cache']['hit_rate'] = (
                self.metrics['cache']['total_hits'] / total_cache_requests
            )

    def _update_percentiles(self):
        """æ›´æ–°å“åº”æ—¶é—´ç™¾åˆ†ä½æ•°"""
        times = sorted(self.metrics['performance']['response_times_ms'])
        if not times:
            return

        self.metrics['performance']['avg_response_time_ms'] = sum(times) / len(times)
        self.metrics['performance']['p50_response_time_ms'] = times[len(times) // 2]
        self.metrics['performance']['p95_response_time_ms'] = times[int(len(times) * 0.95)]
        self.metrics['performance']['p99_response_time_ms'] = times[int(len(times) * 0.99)]

    def _record_error(self, error: Exception):
        """è®°å½•é”™è¯¯"""
        error_type = type(error).__name__

        self.metrics['errors']['by_type'][error_type] = (
            self.metrics['errors']['by_type'].get(error_type, 0) + 1
        )

        self.metrics['errors']['recent_errors'].append({
            'type': error_type,
            'message': str(error),
            'timestamp': datetime.utcnow().isoformat()
        })

        if len(self.metrics['errors']['recent_errors']) > 10:
            self.metrics['errors']['recent_errors'].pop(0)

        # æ›´æ–°é”™è¯¯ç‡
        total = self.metrics['requests']['total']
        failed = self.metrics['requests']['failed']
        self.metrics['errors']['error_rate'] = failed / total if total > 0 else 0

    def _detect_query_type(self, query: str) -> str:
        """æ£€æµ‹æŸ¥è¯¢ç±»å‹"""
        query_lower = query.lower()

        if any(kw in query_lower for kw in ['how to', 'tutorial', 'guide']):
            return 'how_to'
        elif any(kw in query_lower for kw in ['what is', 'define', 'meaning']):
            return 'definition'
        elif any(kw in query_lower for kw in ['vs', 'versus', 'compare']):
            return 'comparison'
        elif any(kw in query_lower for kw in ['latest', 'news', 'recent']):
            return 'news'
        else:
            return 'general'

    def get_health_score(self) -> float:
        """
        è®¡ç®—ç³»ç»Ÿå¥åº·åˆ†æ•° (0-1)

        è€ƒè™‘:
        - æˆåŠŸç‡
        - å“åº”æ—¶é—´
        - ç»“æœè´¨é‡
        - ç¼“å­˜æ•ˆç‡
        """
        total = self.metrics['requests']['total']
        if total == 0:
            return 1.0

        # æˆåŠŸç‡åˆ†æ•° (40%)
        success_rate = self.metrics['requests']['successful'] / total
        success_score = success_rate * 0.4

        # æ€§èƒ½åˆ†æ•° (25%)
        avg_time = self.metrics['performance']['avg_response_time_ms']
        # < 500ms ä¼˜ç§€, > 3000ms å·®
        performance_score = max(0, min(1, (3000 - avg_time) / 2500)) * 0.25

        # è´¨é‡åˆ†æ•° (25%)
        quality_score = self.metrics['quality']['avg_quality_score'] * 0.25

        # ç¼“å­˜æ•ˆç‡åˆ†æ•° (10%)
        cache_score = self.metrics['cache']['hit_rate'] * 0.1

        return success_score + performance_score + quality_score + cache_score

    def generate_report(self) -> str:
        """ç”Ÿæˆäººç±»å¯è¯»çš„æŒ‡æ ‡æŠ¥å‘Š"""
        health = self.get_health_score()

        report = f"""
Search Tool Performance Report
{'='*50}

Overall Health Score: {health:.2%} {'âœ…' if health > 0.8 else 'âš ï¸' if health > 0.6 else 'âŒ'}

Requests:
  Total: {self.metrics['requests']['total']}
  Successful: {self.metrics['requests']['successful']} ({self.metrics['requests']['successful']/max(1,self.metrics['requests']['total']):.1%})
  Failed: {self.metrics['requests']['failed']}
  Cached: {self.metrics['requests']['cached']}

Performance:
  Avg Response Time: {self.metrics['performance']['avg_response_time_ms']:.0f}ms
  P95 Response Time: {self.metrics['performance']['p95_response_time_ms']:.0f}ms
  Slowest Query: {self.metrics['performance']['slowest_query']['query'] if self.metrics['performance']['slowest_query'] else 'N/A'} ({self.metrics['performance']['slowest_query']['time']:.0f}ms if self.metrics['performance']['slowest_query'] else 0}ms)

Quality:
  Avg Results/Query: {self.metrics['quality']['avg_results_per_query']:.1f}
  Avg Quality Score: {self.metrics['quality']['avg_quality_score']:.2f}
  High Quality %: {self.metrics['quality']['high_quality_results_pct']:.1%}
  No Results: {self.metrics['quality']['queries_with_no_results']}

Cache:
  Hit Rate: {self.metrics['cache']['hit_rate']:.1%}
  Hits: {self.metrics['cache']['total_hits']}
  Misses: {self.metrics['cache']['total_misses']}

Errors:
  Error Rate: {self.metrics['errors']['error_rate']:.1%}
  Top Error Types: {', '.join(f"{k}({v})" for k, v in sorted(self.metrics['errors']['by_type'].items(), key=lambda x: x[1], reverse=True)[:3])}

Query Patterns:
  Top Types: {', '.join(f"{k}({v})" for k, v in sorted(self.metrics['patterns']['top_query_types'].items(), key=lambda x: x[1], reverse=True)[:3])}
  Avg Query Length: {self.metrics['patterns']['avg_query_length']:.1f} words
  Top Domains: {', '.join(f"{k}({v})" for k, v in sorted(self.metrics['patterns']['top_domains_returned'].items(), key=lambda x: x[1], reverse=True)[:5])}
"""
        return report
```

**å½±å“**: å¼€å‘è€…å’Œ Agent å¯ä»¥ç›‘æ§æœç´¢å·¥å…·çš„å¥åº·çŠ¶å†µå’Œæ€§èƒ½ã€‚

---

## ğŸ¯ å®æ–½è·¯çº¿å›¾

### é˜¶æ®µ 1: åŸºç¡€å¢å¼º (1-2 å‘¨)
- âœ… ç»“æœè´¨é‡è¯„åˆ† (P0)
- âœ… æŸ¥è¯¢æ„å›¾åˆ†æ (P0)
- âœ… ç»“æ„åŒ–æ‘˜è¦ (P0)
- âœ… Agent å‹å¥½é”™è¯¯å¤„ç† (P0)

### é˜¶æ®µ 2: æ™ºèƒ½ä¼˜åŒ– (2-3 å‘¨)
- âœ… ç»“æœå»é‡ (P1)
- âœ… æœç´¢ä¸Šä¸‹æ–‡ç®¡ç† (P1)
- âœ… å›¾ç‰‡è´¨é‡åˆ†æ (P1)
- âœ… æ–°é—»å¯ä¿¡åº¦è¯„ä¼° (P1)

### é˜¶æ®µ 3: é«˜çº§åŠŸèƒ½ (1-2 å‘¨)
- âœ… æ™ºèƒ½ç¼“å­˜ (P2)
- âœ… å¢å¼ºæŒ‡æ ‡æ”¶é›† (P2)
- âœ… æ€§èƒ½ä¼˜åŒ– (P2)

---

## ğŸ“ˆ é¢„æœŸæ”¶ç›Šå¯¹æ¯”

| æŒ‡æ ‡ | å½“å‰ | ä¼˜åŒ–å | æå‡ |
|------|------|--------|------|
| ç»“æœç›¸å…³æ€§ | ~70% | ~95% | +35% |
| Agent æ»¡æ„åº¦ | ~65% | ~90% | +38% |
| æŸ¥è¯¢æˆåŠŸç‡ | ~75% | ~92% | +23% |
| å¹³å‡ç»“æœè´¨é‡ | 0.60 | 0.82 | +37% |
| é”™è¯¯æ¢å¤ç‡ | ~30% | ~75% | +150% |
| API ä½¿ç”¨æ•ˆç‡ | åŸºå‡† | -35% | èŠ‚çœ35% |
| Agent è‡ªä¸»æ€§ | ~60% | ~85% | +42% |

---

## ğŸ”š æ€»ç»“

`search_tool` å½“å‰å®ç°äº†ä¼˜ç§€çš„åŸºç¡€æ¶æ„,ä½†åœ¨å¸®åŠ© Agent è·å–é«˜è´¨é‡æœç´¢ç»“æœæ–¹é¢è¿˜æœ‰å·¨å¤§çš„æå‡ç©ºé—´ã€‚

**æ ¸å¿ƒä¼˜åŒ–æ–¹å‘**:
1. **è´¨é‡ä¼˜å…ˆ** - è®© Agent çŸ¥é“ä»€ä¹ˆæ˜¯å¥½ç»“æœ
2. **æ™ºèƒ½ç†è§£** - è‡ªåŠ¨ä¼˜åŒ–æŸ¥è¯¢å’Œå‚æ•°
3. **ä¸Šä¸‹æ–‡æ„ŸçŸ¥** - åŸºäºå†å²æä¾›æ›´å¥½çš„ç»“æœ
4. **å‹å¥½ä½“éªŒ** - Agent èƒ½ç†è§£å’Œå¤„ç†å„ç§æƒ…å†µ

å®æ–½è¿™äº›ä¼˜åŒ–å,Agent å°†èƒ½å¤Ÿ:
- ğŸ¯ æ›´å¿«æ‰¾åˆ°ç›¸å…³ä¿¡æ¯
- ğŸ” è¯†åˆ«é«˜è´¨é‡å’Œå¯ä¿¡çš„æ¥æº
- ğŸ§  ä»æœç´¢å†å²ä¸­å­¦ä¹ 
- ğŸ’ª è‡ªä¸»å¤„ç†é”™è¯¯å’Œé—®é¢˜
- ğŸ“Š ç†è§£æœç´¢ç»“æœçš„è´¨é‡å’Œå¯ç”¨æ€§

å»ºè®®ä¼˜å…ˆå®æ–½ P0 ä¼˜åŒ–,è¿™äº›å°†å¸¦æ¥æœ€å¤§çš„ä»·å€¼æå‡ã€‚

