# APISource Tool ä¼˜åŒ–åˆ†ææŠ¥å‘Š

## ğŸ“‹ æ‰§è¡Œæ‘˜è¦

æœ¬æŠ¥å‘Šæ·±å…¥åˆ†æäº† `apisource_tool` å’Œ `api_providers` çš„æ¶æ„å’Œå®ç°ï¼Œä» **Agent è·å–é«˜è´¨é‡ç»“æœ** çš„è§’åº¦æå‡ºä¼˜åŒ–å»ºè®®ã€‚åˆ†æåŸºäºå¯¹ 601 è¡Œä»£ç çš„å…¨é¢å®¡æŸ¥å’Œ 79 ä¸ªæµ‹è¯•ç”¨ä¾‹çš„æ‰§è¡Œç»“æœã€‚

**å½“å‰çŠ¶æ€**: âœ… åŠŸèƒ½å®Œæ•´ï¼Œæµ‹è¯•è¦†ç›–ç‡ 86.69%  
**ä¼˜åŒ–æ½œåŠ›**: ğŸš€ é«˜ - å¯åœ¨æ•°æ®è´¨é‡ã€æ™ºèƒ½æ€§ã€å¯ç”¨æ€§æ–¹é¢æ˜¾è‘—æå‡

---

## ğŸ¯ æ ¸å¿ƒé—®é¢˜ï¼šAgent å¦‚ä½•è·å¾—æ›´ç²¾å‡†çš„é«˜è´¨é‡ç»“æœï¼Ÿ

### é—®é¢˜åˆ†è§£

1. **æ•°æ®è´¨é‡é—®é¢˜** - è¿”å›çš„æ•°æ®æ˜¯å¦è¶³å¤Ÿç²¾å‡†ã€ç›¸å…³ã€å®Œæ•´ï¼Ÿ
2. **æ™ºèƒ½æ€§é—®é¢˜** - å·¥å…·æ˜¯å¦èƒ½ç†è§£ Agent çš„æ„å›¾å¹¶è‡ªåŠ¨ä¼˜åŒ–æŸ¥è¯¢ï¼Ÿ
3. **å¯ç”¨æ€§é—®é¢˜** - Agent æ˜¯å¦å®¹æ˜“æ­£ç¡®ä½¿ç”¨å·¥å…·ï¼Ÿ
4. **å¯é æ€§é—®é¢˜** - å·¥å…·æ˜¯å¦èƒ½ç¨³å®šè¿”å›ç»“æœï¼Ÿ
5. **å¯è§‚æµ‹æ€§é—®é¢˜** - Agent æ˜¯å¦èƒ½ç†è§£è¿”å›æ•°æ®çš„è´¨é‡å’Œå¯ä¿¡åº¦ï¼Ÿ

---

## ğŸ” è¯¦ç»†åˆ†æ

## 1. æ•°æ®è´¨é‡ä¼˜åŒ– (â­â­â­â­â­ æœ€é‡è¦)

### 1.1 ç¼ºå°‘æ•°æ®è´¨é‡å…ƒæ•°æ®

**ç°çŠ¶é—®é¢˜**:
```python
# å½“å‰è¿”å›æ ¼å¼ (base_provider.py:228-237)
return {
    'provider': self.name,
    'operation': operation,
    'data': data,
    'metadata': {
        'timestamp': datetime.utcnow().isoformat(),
        'source': source or f'{self.name} API',
        'cached': False  # å§‹ç»ˆä¸º Falseï¼Œæœªå®ç°ç¼“å­˜
    }
}
```

**é—®é¢˜**:
- âŒ æ²¡æœ‰æ•°æ®è´¨é‡è¯„åˆ†
- âŒ æ²¡æœ‰æ•°æ®æ–°é²œåº¦æŒ‡æ ‡
- âŒ æ²¡æœ‰æ•°æ®å®Œæ•´æ€§ä¿¡æ¯
- âŒ æ²¡æœ‰ç½®ä¿¡åº¦è¯„åˆ†
- âŒ æ²¡æœ‰æ•°æ®æ¥æºçš„æƒå¨æ€§è¯„çº§

**ä¼˜åŒ–å»ºè®®**:
```python
# å»ºè®®çš„å¢å¼ºå…ƒæ•°æ®ç»“æ„
'metadata': {
    'timestamp': '2025-10-15T16:00:00Z',
    'source': 'FRED API - https://api.stlouisfed.org/fred/series/observations',
    'cached': False,
    
    # æ–°å¢ï¼šæ•°æ®è´¨é‡æŒ‡æ ‡
    'quality': {
        'score': 0.95,  # 0-1 è´¨é‡è¯„åˆ†
        'completeness': 1.0,  # æ•°æ®å®Œæ•´æ€§
        'freshness_hours': 2,  # æ•°æ®æ–°é²œåº¦ï¼ˆå°æ—¶ï¼‰
        'confidence': 0.98,  # ç½®ä¿¡åº¦
        'authority_level': 'official',  # official/verified/community
    },
    
    # æ–°å¢ï¼šæ•°æ®èŒƒå›´ä¿¡æ¯
    'coverage': {
        'start_date': '1947-01-01',
        'end_date': '2025-10-15',
        'total_records': 318,
        'missing_records': 0,
        'frequency': 'quarterly'  # daily/weekly/monthly/quarterly/annual
    },
    
    # æ–°å¢ï¼šAPI å“åº”ä¿¡æ¯
    'api_info': {
        'response_time_ms': 245,
        'rate_limit_remaining': 95,
        'rate_limit_reset': '2025-10-15T17:00:00Z',
        'api_version': 'v2'
    }
}
```

**å½±å“**: Agent å¯ä»¥æ ¹æ®è´¨é‡è¯„åˆ†é€‰æ‹©æœ€ä½³æ•°æ®æºï¼Œé¿å…ä½¿ç”¨ä½è´¨é‡æ•°æ®ã€‚

---

### 1.2 ç¼ºå°‘æ•°æ®éªŒè¯å’Œæ¸…æ´—

**ç°çŠ¶é—®é¢˜**:
```python
# fred_provider.py:166-180 - ç›´æ¥è¿”å›åŸå§‹æ•°æ®
data = response.json()
if operation in ['get_series', 'get_series_observations']:
    result_data = data.get('observations', [])
# æ²¡æœ‰ä»»ä½•éªŒè¯æˆ–æ¸…æ´—
return self._format_response(operation=operation, data=result_data, ...)
```

**é—®é¢˜**:
- âŒ ä¸æ£€æŸ¥ç©ºå€¼ã€å¼‚å¸¸å€¼
- âŒ ä¸éªŒè¯æ•°æ®ç±»å‹
- âŒ ä¸å¤„ç†ç¼ºå¤±æ•°æ®
- âŒ ä¸æ ‡è®°å¼‚å¸¸æ•°æ®ç‚¹

**ä¼˜åŒ–å»ºè®®**:
```python
# å»ºè®®æ·»åŠ æ•°æ®éªŒè¯å±‚
def _validate_and_clean_data(self, operation: str, raw_data: Any) -> Dict[str, Any]:
    """éªŒè¯å’Œæ¸…æ´—æ•°æ®"""
    
    validation_result = {
        'data': raw_data,
        'issues': [],
        'warnings': [],
        'statistics': {}
    }
    
    if operation == 'get_series_observations':
        # æ£€æŸ¥æ—¶é—´åºåˆ—æ•°æ®
        if isinstance(raw_data, list):
            # ç»Ÿè®¡ç¼ºå¤±å€¼
            missing_count = sum(1 for item in raw_data if item.get('value') == '.')
            
            # æ£€æµ‹å¼‚å¸¸å€¼ï¼ˆä½¿ç”¨ IQR æ–¹æ³•ï¼‰
            numeric_values = [float(item['value']) for item in raw_data 
                            if item.get('value') != '.']
            outliers = self._detect_outliers(numeric_values)
            
            # æ£€æŸ¥æ—¶é—´åºåˆ—è¿ç»­æ€§
            gaps = self._detect_time_gaps(raw_data)
            
            validation_result['statistics'] = {
                'total_records': len(raw_data),
                'missing_values': missing_count,
                'outliers_count': len(outliers),
                'time_gaps': len(gaps),
                'value_range': {
                    'min': min(numeric_values) if numeric_values else None,
                    'max': max(numeric_values) if numeric_values else None,
                    'mean': sum(numeric_values) / len(numeric_values) if numeric_values else None
                }
            }
            
            if missing_count > 0:
                validation_result['warnings'].append(
                    f"{missing_count} missing values detected"
                )
            
            if outliers:
                validation_result['warnings'].append(
                    f"{len(outliers)} potential outliers detected at indices: {outliers[:5]}"
                )
    
    return validation_result
```

**å½±å“**: Agent å¯ä»¥äº†è§£æ•°æ®è´¨é‡é—®é¢˜ï¼Œåšå‡ºæ›´æ˜æ™ºçš„å†³ç­–ã€‚

---

### 1.3 ç¼ºå°‘æ™ºèƒ½æ•°æ®è¿‡æ»¤å’Œæ’åº

**ç°çŠ¶é—®é¢˜**:
```python
# apisource_tool.py:255-259 - æœç´¢åŠŸèƒ½è¿‡äºç®€å•
if provider_name == 'fred':
    result = provider_instance.execute(
        'search_series',
        {'search_text': query, 'limit': limit}
    )
```

**é—®é¢˜**:
- âŒ ä¸æ”¯æŒç›¸å…³æ€§æ’åº
- âŒ ä¸æ”¯æŒæŒ‰è´¨é‡è¿‡æ»¤
- âŒ ä¸æ”¯æŒæŒ‰æ—¶é—´èŒƒå›´è¿‡æ»¤
- âŒ ä¸æ”¯æŒå¤šæ¡ä»¶ç»„åˆæŸ¥è¯¢

**ä¼˜åŒ–å»ºè®®**:
```python
# å»ºè®®æ·»åŠ æ™ºèƒ½æœç´¢å¢å¼º
class SearchEnhancer:
    """æœç´¢ç»“æœå¢å¼ºå™¨"""
    
    def enhance_search_results(
        self, 
        query: str, 
        results: List[Dict], 
        options: Dict[str, Any]
    ) -> List[Dict]:
        """
        å¢å¼ºæœç´¢ç»“æœ
        
        Args:
            query: åŸå§‹æŸ¥è¯¢
            results: åŸå§‹ç»“æœ
            options: å¢å¼ºé€‰é¡¹
                - relevance_threshold: ç›¸å…³æ€§é˜ˆå€¼ (0-1)
                - sort_by: æ’åºæ–¹å¼ (relevance/popularity/recency)
                - date_range: æ—¶é—´èŒƒå›´è¿‡æ»¤
                - min_quality_score: æœ€å°è´¨é‡åˆ†æ•°
        """
        enhanced = []
        
        for result in results:
            # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
            relevance = self._calculate_relevance(query, result)
            
            # è®¡ç®—æµè¡Œåº¦åˆ†æ•°ï¼ˆåŸºäºä½¿ç”¨é¢‘ç‡ï¼‰
            popularity = self._get_popularity_score(result)
            
            # è®¡ç®—æ–°é²œåº¦åˆ†æ•°
            recency = self._calculate_recency(result)
            
            # ç»¼åˆè¯„åˆ†
            composite_score = (
                relevance * 0.5 + 
                popularity * 0.3 + 
                recency * 0.2
            )
            
            # åº”ç”¨è¿‡æ»¤å™¨
            if composite_score >= options.get('relevance_threshold', 0.3):
                result['_search_metadata'] = {
                    'relevance_score': relevance,
                    'popularity_score': popularity,
                    'recency_score': recency,
                    'composite_score': composite_score,
                    'match_type': self._get_match_type(query, result)
                }
                enhanced.append(result)
        
        # æ’åº
        sort_by = options.get('sort_by', 'relevance')
        if sort_by == 'relevance':
            enhanced.sort(
                key=lambda x: x['_search_metadata']['composite_score'], 
                reverse=True
            )
        
        return enhanced
    
    def _calculate_relevance(self, query: str, result: Dict) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°ï¼ˆä½¿ç”¨ TF-IDF æˆ–è¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰"""
        # ç®€å•å®ç°ï¼šå…³é”®è¯åŒ¹é…
        query_terms = set(query.lower().split())
        
        # æ£€æŸ¥æ ‡é¢˜åŒ¹é…
        title = result.get('title', '').lower()
        title_matches = sum(1 for term in query_terms if term in title)
        
        # æ£€æŸ¥æè¿°åŒ¹é…
        desc = result.get('notes', '').lower()
        desc_matches = sum(1 for term in query_terms if term in desc)
        
        # è®¡ç®—åˆ†æ•°
        title_score = min(title_matches / len(query_terms), 1.0) if query_terms else 0
        desc_score = min(desc_matches / len(query_terms), 1.0) if query_terms else 0
        
        # æ ‡é¢˜åŒ¹é…æƒé‡æ›´é«˜
        return title_score * 0.7 + desc_score * 0.3
```

**å½±å“**: Agent è·å¾—æ›´ç›¸å…³ã€æ›´é«˜è´¨é‡çš„æœç´¢ç»“æœã€‚

---

## 2. æ™ºèƒ½æ€§ä¼˜åŒ– (â­â­â­â­)

### 2.1 ç¼ºå°‘æŸ¥è¯¢æ„å›¾ç†è§£

**ç°çŠ¶é—®é¢˜**:
```python
# apisource_tool.py:167-196 - ç›´æ¥ä¼ é€’å‚æ•°ï¼Œä¸ç†è§£æ„å›¾
def query(self, provider: str, operation: str, params: Dict[str, Any]):
    # ç›´æ¥æ‰§è¡Œï¼Œä¸åˆ†ææŸ¥è¯¢æ„å›¾
    result = provider_instance.execute(operation, params)
    return result
```

**é—®é¢˜**:
- âŒ ä¸ç†è§£ Agent æƒ³è¦ä»€ä¹ˆç±»å‹çš„æ•°æ®
- âŒ ä¸èƒ½è‡ªåŠ¨é€‰æ‹©æœ€ä½³æ“ä½œ
- âŒ ä¸èƒ½è‡ªåŠ¨è¡¥å…¨ç¼ºå¤±å‚æ•°
- âŒ ä¸èƒ½æä¾›æŸ¥è¯¢å»ºè®®

**ä¼˜åŒ–å»ºè®®**:
```python
class QueryIntentAnalyzer:
    """æŸ¥è¯¢æ„å›¾åˆ†æå™¨"""
    
    def analyze_intent(self, query_text: str) -> Dict[str, Any]:
        """
        åˆ†ææŸ¥è¯¢æ„å›¾
        
        Returns:
            {
                'intent_type': 'time_series' | 'comparison' | 'search' | 'metadata',
                'entities': ['GDP', 'unemployment'],
                'time_range': {'start': '2020', 'end': '2025'},
                'geographic_scope': 'US',
                'suggested_providers': ['fred', 'worldbank'],
                'suggested_operations': ['get_series', 'get_indicator'],
                'confidence': 0.85
            }
        """
        intent = {
            'intent_type': None,
            'entities': [],
            'time_range': None,
            'geographic_scope': None,
            'suggested_providers': [],
            'suggested_operations': [],
            'confidence': 0.0
        }
        
        # æ£€æµ‹æ—¶é—´åºåˆ—æ„å›¾
        time_keywords = ['trend', 'over time', 'historical', 'series', 'change']
        if any(kw in query_text.lower() for kw in time_keywords):
            intent['intent_type'] = 'time_series'
            intent['confidence'] += 0.3
        
        # æ£€æµ‹æ¯”è¾ƒæ„å›¾
        comparison_keywords = ['compare', 'versus', 'vs', 'difference', 'between']
        if any(kw in query_text.lower() for kw in comparison_keywords):
            intent['intent_type'] = 'comparison'
            intent['confidence'] += 0.3
        
        # æå–å®ä½“ï¼ˆç»æµæŒ‡æ ‡ï¼‰
        economic_indicators = {
            'gdp': ['fred', 'worldbank'],
            'unemployment': ['fred'],
            'inflation': ['fred', 'worldbank'],
            'population': ['census', 'worldbank']
        }
        
        for indicator, providers in economic_indicators.items():
            if indicator in query_text.lower():
                intent['entities'].append(indicator)
                intent['suggested_providers'].extend(providers)
                intent['confidence'] += 0.2
        
        # æå–æ—¶é—´èŒƒå›´
        import re
        year_pattern = r'\b(19|20)\d{2}\b'
        years = re.findall(year_pattern, query_text)
        if len(years) >= 2:
            intent['time_range'] = {
                'start': min(years),
                'end': max(years)
            }
            intent['confidence'] += 0.2
        
        return intent
    
    def auto_complete_params(
        self, 
        provider: str, 
        operation: str, 
        params: Dict[str, Any],
        intent: Dict[str, Any]
    ) -> Dict[str, Any]:
        """æ ¹æ®æ„å›¾è‡ªåŠ¨è¡¥å…¨å‚æ•°"""
        
        completed_params = params.copy()
        
        # è‡ªåŠ¨æ·»åŠ æ—¶é—´èŒƒå›´
        if intent.get('time_range') and 'observation_start' not in params:
            completed_params['observation_start'] = intent['time_range']['start']
            completed_params['observation_end'] = intent['time_range']['end']
        
        # è‡ªåŠ¨æ·»åŠ åˆç†çš„é™åˆ¶
        if 'limit' not in params:
            if intent['intent_type'] == 'time_series':
                completed_params['limit'] = 1000  # æ—¶é—´åºåˆ—éœ€è¦æ›´å¤šæ•°æ®
            else:
                completed_params['limit'] = 10  # æœç´¢ç»“æœé»˜è®¤10æ¡
        
        # è‡ªåŠ¨æ·»åŠ æ’åº
        if 'sort_order' not in params and provider == 'fred':
            completed_params['sort_order'] = 'desc'  # æœ€æ–°æ•°æ®ä¼˜å…ˆ
        
        return completed_params
```

**å½±å“**: Agent å¯ä»¥ç”¨æ›´è‡ªç„¶çš„æ–¹å¼æŸ¥è¯¢ï¼Œå·¥å…·è‡ªåŠ¨ç†è§£æ„å›¾å¹¶ä¼˜åŒ–å‚æ•°ã€‚

---

### 2.2 ç¼ºå°‘è·¨æä¾›è€…æ•°æ®èåˆ

**ç°çŠ¶é—®é¢˜**:
```python
# apisource_tool.py:229-280 - æœç´¢è¿”å›ç‹¬ç«‹ç»“æœï¼Œä¸èåˆ
def search(self, query: str, providers: Optional[List[str]] = None, limit: int = 10):
    results = []
    for provider_name in providers:
        result = provider_instance.execute(...)
        results.append(result)  # ç‹¬ç«‹æ·»åŠ ï¼Œä¸èåˆ
    return results
```

**é—®é¢˜**:
- âŒ ä¸åˆå¹¶æ¥è‡ªä¸åŒæä¾›è€…çš„ç›¸åŒæ•°æ®
- âŒ ä¸è§£å†³æ•°æ®å†²çª
- âŒ ä¸æä¾›ç»Ÿä¸€è§†å›¾
- âŒ Agent éœ€è¦è‡ªå·±å¤„ç†å¤šä¸ªæ•°æ®æº

**ä¼˜åŒ–å»ºè®®**:
```python
class DataFusionEngine:
    """æ•°æ®èåˆå¼•æ“"""
    
    def fuse_multi_provider_results(
        self, 
        results: List[Dict[str, Any]],
        fusion_strategy: str = 'best_quality'
    ) -> Dict[str, Any]:
        """
        èåˆå¤šä¸ªæä¾›è€…çš„ç»“æœ
        
        Args:
            results: æ¥è‡ªä¸åŒæä¾›è€…çš„ç»“æœåˆ—è¡¨
            fusion_strategy: èåˆç­–ç•¥
                - 'best_quality': é€‰æ‹©è´¨é‡æœ€é«˜çš„
                - 'merge_all': åˆå¹¶æ‰€æœ‰æ•°æ®
                - 'consensus': åŸºäºå…±è¯†
                - 'weighted_average': åŠ æƒå¹³å‡
        """
        
        if not results:
            return None
        
        if fusion_strategy == 'best_quality':
            # é€‰æ‹©è´¨é‡åˆ†æ•°æœ€é«˜çš„ç»“æœ
            return max(
                results, 
                key=lambda r: r['metadata'].get('quality', {}).get('score', 0)
            )
        
        elif fusion_strategy == 'merge_all':
            # åˆå¹¶æ‰€æœ‰æ•°æ®ï¼Œæ ‡è®°æ¥æº
            merged = {
                'operation': 'multi_provider_search',
                'data': [],
                'metadata': {
                    'fusion_strategy': 'merge_all',
                    'sources': [],
                    'total_providers': len(results)
                }
            }
            
            for result in results:
                # ä¸ºæ¯æ¡æ•°æ®æ·»åŠ æ¥æºæ ‡è®°
                provider = result['provider']
                for item in result.get('data', []):
                    if isinstance(item, dict):
                        item['_source_provider'] = provider
                        item['_source_quality'] = result['metadata'].get('quality', {})
                    merged['data'].append(item)
                
                merged['metadata']['sources'].append({
                    'provider': provider,
                    'count': len(result.get('data', [])),
                    'quality': result['metadata'].get('quality', {})
                })
            
            return merged
        
        elif fusion_strategy == 'consensus':
            # åŸºäºå¤šæ•°å…±è¯†èåˆæ•°æ®
            return self._consensus_fusion(results)
    
    def _detect_duplicate_data(
        self, 
        data1: Dict, 
        data2: Dict
    ) -> Tuple[bool, float]:
        """
        æ£€æµ‹é‡å¤æ•°æ®
        
        Returns:
            (is_duplicate, similarity_score)
        """
        # æ£€æŸ¥å…³é”®å­—æ®µç›¸ä¼¼åº¦
        key_fields = ['id', 'series_id', 'indicator_code', 'title', 'name']
        
        matches = 0
        total_fields = 0
        
        for field in key_fields:
            if field in data1 and field in data2:
                total_fields += 1
                if data1[field] == data2[field]:
                    matches += 1
        
        if total_fields == 0:
            return False, 0.0
        
        similarity = matches / total_fields
        return similarity > 0.8, similarity
    
    def _resolve_conflict(
        self, 
        value1: Any, 
        value2: Any, 
        quality1: float, 
        quality2: float
    ) -> Any:
        """è§£å†³æ•°æ®å†²çª - é€‰æ‹©è´¨é‡æ›´é«˜çš„æ•°æ®æº"""
        return value1 if quality1 >= quality2 else value2
```

**å½±å“**: Agent è·å¾—èåˆåçš„é«˜è´¨é‡æ•°æ®ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†å¤šä¸ªæ•°æ®æºã€‚

---

## 3. å¯ç”¨æ€§ä¼˜åŒ– (â­â­â­â­)

### 3.1 ç¼ºå°‘æ“ä½œç¤ºä¾‹å’Œæ–‡æ¡£

**ç°çŠ¶é—®é¢˜**:
```python
# base_provider.py:188-199 - get_operation_schema è¿”å› None
def get_operation_schema(self, operation: str) -> Optional[Dict[str, Any]]:
    """Get schema for a specific operation."""
    # Override in subclass to provide operation-specific schemas
    return None  # æ‰€æœ‰æä¾›è€…éƒ½è¿”å› Noneï¼
```

**é—®é¢˜**:
- âŒ Agent ä¸çŸ¥é“æ¯ä¸ªæ“ä½œéœ€è¦ä»€ä¹ˆå‚æ•°
- âŒ æ²¡æœ‰å‚æ•°ç¤ºä¾‹
- âŒ æ²¡æœ‰ä½¿ç”¨è¯´æ˜
- âŒ é”™è¯¯æ¶ˆæ¯ä¸å¤Ÿè¯¦ç»†

**ä¼˜åŒ–å»ºè®®**:
```python
# ä¸ºæ¯ä¸ªæ“ä½œæä¾›è¯¦ç»†çš„ schema
def get_operation_schema(self, operation: str) -> Optional[Dict[str, Any]]:
    """è·å–æ“ä½œçš„è¯¦ç»† schema"""
    
    schemas = {
        'get_series': {
            'description': 'Get economic time series data from FRED',
            'parameters': {
                'series_id': {
                    'type': 'string',
                    'required': True,
                    'description': 'FRED series ID (e.g., GDP, UNRATE, CPIAUCSL)',
                    'examples': ['GDP', 'UNRATE', 'CPIAUCSL', 'DGS10'],
                    'validation': {
                        'pattern': r'^[A-Z0-9]+$',
                        'max_length': 50
                    }
                },
                'observation_start': {
                    'type': 'string',
                    'required': False,
                    'description': 'Start date for observations (YYYY-MM-DD)',
                    'examples': ['2020-01-01', '2015-06-15'],
                    'default': 'earliest available',
                    'validation': {
                        'pattern': r'^\d{4}-\d{2}-\d{2}$'
                    }
                },
                'limit': {
                    'type': 'integer',
                    'required': False,
                    'description': 'Maximum number of observations to return',
                    'examples': [100, 1000],
                    'default': 100000,
                    'validation': {
                        'min': 1,
                        'max': 100000
                    }
                }
            },
            'returns': {
                'type': 'object',
                'description': 'Standardized response with time series data',
                'structure': {
                    'provider': 'fred',
                    'operation': 'get_series',
                    'data': [
                        {
                            'date': '2025-01-01',
                            'value': '28000.5',
                            'realtime_start': '2025-10-15',
                            'realtime_end': '2025-10-15'
                        }
                    ],
                    'metadata': {
                        'timestamp': '2025-10-15T16:00:00Z',
                        'source': 'FRED API',
                        'quality': {...}
                    }
                }
            },
            'examples': [
                {
                    'description': 'Get GDP data for last 5 years',
                    'params': {
                        'series_id': 'GDP',
                        'observation_start': '2020-01-01',
                        'limit': 100
                    }
                },
                {
                    'description': 'Get unemployment rate',
                    'params': {
                        'series_id': 'UNRATE'
                    }
                }
            ],
            'common_errors': [
                {
                    'error': 'Bad Request: series does not exist',
                    'cause': 'Invalid series_id',
                    'solution': 'Use search_series to find valid series IDs'
                }
            ]
        }
    }
    
    return schemas.get(operation)
```

**å½±å“**: Agent å¯ä»¥æ­£ç¡®ä½¿ç”¨å·¥å…·ï¼Œå‡å°‘é”™è¯¯ï¼Œæé«˜æˆåŠŸç‡ã€‚

---

### 3.2 ç¼ºå°‘æ™ºèƒ½é”™è¯¯å¤„ç†å’Œé‡è¯•

**ç°çŠ¶é—®é¢˜**:
```python
# base_provider.py:290-299 - ç®€å•çš„é”™è¯¯å¤„ç†
try:
    result = self.fetch(operation, params)
    self._update_stats(success=True)
    return result
except Exception as e:
    self._update_stats(success=False)
    self.logger.error(f"Error executing {self.name}.{operation}: {e}")
    raise  # ç›´æ¥æŠ›å‡ºå¼‚å¸¸
```

**é—®é¢˜**:
- âŒ ä¸åŒºåˆ†å¯é‡è¯•å’Œä¸å¯é‡è¯•çš„é”™è¯¯
- âŒ ä¸æä¾›é”™è¯¯æ¢å¤å»ºè®®
- âŒ ä¸è‡ªåŠ¨é‡è¯•ä¸´æ—¶æ€§é”™è¯¯
- âŒ é”™è¯¯æ¶ˆæ¯å¯¹ Agent ä¸å‹å¥½

**ä¼˜åŒ–å»ºè®®**:
```python
class SmartErrorHandler:
    """æ™ºèƒ½é”™è¯¯å¤„ç†å™¨"""
    
    RETRYABLE_ERRORS = [
        'timeout',
        'connection',
        'rate limit',
        '429',  # Too Many Requests
        '503',  # Service Unavailable
        '504'   # Gateway Timeout
    ]
    
    def execute_with_retry(
        self, 
        operation_func: Callable,
        max_retries: int = 3,
        backoff_factor: float = 2.0
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ“ä½œå¹¶æ™ºèƒ½é‡è¯•
        
        Returns:
            {
                'success': True/False,
                'data': ...,  # å¦‚æœæˆåŠŸ
                'error': {...},  # å¦‚æœå¤±è´¥
                'retry_info': {
                    'attempts': 2,
                    'last_error': '...',
                    'recovery_suggestions': [...]
                }
            }
        """
        
        last_error = None
        retry_info = {
            'attempts': 0,
            'errors': [],
            'recovery_suggestions': []
        }
        
        for attempt in range(max_retries):
            retry_info['attempts'] = attempt + 1
            
            try:
                result = operation_func()
                return {
                    'success': True,
                    'data': result,
                    'retry_info': retry_info
                }
            
            except Exception as e:
                last_error = e
                error_msg = str(e).lower()
                retry_info['errors'].append({
                    'attempt': attempt + 1,
                    'error': str(e),
                    'timestamp': datetime.utcnow().isoformat()
                })
                
                # åˆ¤æ–­æ˜¯å¦å¯é‡è¯•
                is_retryable = any(
                    err_type in error_msg 
                    for err_type in self.RETRYABLE_ERRORS
                )
                
                if not is_retryable or attempt == max_retries - 1:
                    # ä¸å¯é‡è¯•æˆ–å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°
                    break
                
                # è®¡ç®—é€€é¿æ—¶é—´
                wait_time = backoff_factor ** attempt
                time.sleep(wait_time)
        
        # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥ï¼Œç”Ÿæˆæ¢å¤å»ºè®®
        retry_info['recovery_suggestions'] = self._generate_recovery_suggestions(
            last_error
        )
        
        return {
            'success': False,
            'error': {
                'type': type(last_error).__name__,
                'message': str(last_error),
                'details': self._parse_error_details(last_error)
            },
            'retry_info': retry_info
        }
    
    def _generate_recovery_suggestions(self, error: Exception) -> List[str]:
        """ç”Ÿæˆé”™è¯¯æ¢å¤å»ºè®®"""
        suggestions = []
        error_msg = str(error).lower()
        
        if 'api key' in error_msg or 'authentication' in error_msg:
            suggestions.append(
                "Check that your API key is valid and properly configured"
            )
            suggestions.append(
                "Verify the API key has not expired"
            )
        
        elif 'rate limit' in error_msg or '429' in error_msg:
            suggestions.append(
                "Wait before making more requests (rate limit exceeded)"
            )
            suggestions.append(
                "Consider using a different provider for this data"
            )
            suggestions.append(
                "Reduce the frequency of requests"
            )
        
        elif 'not found' in error_msg or '404' in error_msg:
            suggestions.append(
                "Verify the resource ID or parameter is correct"
            )
            suggestions.append(
                "Use search operation to find valid resource IDs"
            )
        
        elif 'timeout' in error_msg:
            suggestions.append(
                "Try again with a smaller date range or limit"
            )
            suggestions.append(
                "Increase the timeout setting"
            )
        
        elif 'invalid parameter' in error_msg:
            suggestions.append(
                "Check the operation schema for valid parameters"
            )
            suggestions.append(
                "Review parameter examples in documentation"
            )
        
        return suggestions
```

**å½±å“**: Agent é‡åˆ°é”™è¯¯æ—¶èƒ½è·å¾—æ¸…æ™°çš„æŒ‡å¯¼ï¼Œæé«˜é—®é¢˜è§£å†³æ•ˆç‡ã€‚

---

## 4. å¯é æ€§ä¼˜åŒ– (â­â­â­)

### 4.1 ç¼ºå°‘çœŸæ­£çš„ç¼“å­˜å®ç°

**ç°çŠ¶é—®é¢˜**:
```python
# base_provider.py:235 - cached å§‹ç»ˆä¸º False
'metadata': {
    'timestamp': datetime.utcnow().isoformat(),
    'source': source or f'{self.name} API',
    'cached': False  # ç¡¬ç¼–ç ä¸º Falseï¼
}
```

**é—®é¢˜**:
- âŒ æ²¡æœ‰å®ç°ç¼“å­˜åŠŸèƒ½
- âŒ é‡å¤è¯·æ±‚æµªè´¹ API é…é¢
- âŒ å“åº”æ—¶é—´æ…¢
- âŒ ä¸å¿…è¦çš„ç½‘ç»œè¯·æ±‚

**ä¼˜åŒ–å»ºè®®**:
```python
class SmartCache:
    """æ™ºèƒ½ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, ttl_seconds: int = 300):
        self.cache = {}
        self.ttl_seconds = ttl_seconds
        self.access_stats = {}
    
    def get_cache_key(
        self, 
        provider: str, 
        operation: str, 
        params: Dict[str, Any]
    ) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        import json
        
        # æ ‡å‡†åŒ–å‚æ•°ï¼ˆæ’åºï¼‰
        sorted_params = json.dumps(params, sort_keys=True)
        key_string = f"{provider}:{operation}:{sorted_params}"
        
        return hashlib.md5(key_string.encode()).hexdigest()
    
    def get(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """è·å–ç¼“å­˜æ•°æ®"""
        if cache_key not in self.cache:
            return None
        
        cached_item = self.cache[cache_key]
        
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        age_seconds = (datetime.utcnow() - cached_item['cached_at']).total_seconds()
        
        # æ™ºèƒ½ TTLï¼šæ ¹æ®æ•°æ®ç±»å‹è°ƒæ•´
        effective_ttl = self._calculate_effective_ttl(
            cached_item['data'],
            cached_item['metadata']
        )
        
        if age_seconds > effective_ttl:
            # è¿‡æœŸï¼Œåˆ é™¤ç¼“å­˜
            del self.cache[cache_key]
            return None
        
        # æ›´æ–°è®¿é—®ç»Ÿè®¡
        self.access_stats[cache_key] = self.access_stats.get(cache_key, 0) + 1
        
        # æ·»åŠ ç¼“å­˜å…ƒæ•°æ®
        result = cached_item['data'].copy()
        result['metadata']['cached'] = True
        result['metadata']['cache_age_seconds'] = age_seconds
        result['metadata']['cache_hit_count'] = self.access_stats[cache_key]
        
        return result
    
    def set(
        self, 
        cache_key: str, 
        data: Dict[str, Any],
        metadata: Dict[str, Any]
    ):
        """è®¾ç½®ç¼“å­˜"""
        self.cache[cache_key] = {
            'data': data,
            'metadata': metadata,
            'cached_at': datetime.utcnow()
        }
    
    def _calculate_effective_ttl(
        self, 
        data: Any, 
        metadata: Dict[str, Any]
    ) -> int:
        """
        æ ¹æ®æ•°æ®ç‰¹æ€§è®¡ç®—æœ‰æ•ˆ TTL
        
        - å†å²æ•°æ®ï¼šæ›´é•¿çš„ TTLï¼ˆä¸ä¼šæ”¹å˜ï¼‰
        - å®æ—¶æ•°æ®ï¼šæ›´çŸ­çš„ TTL
        - é«˜è´¨é‡æ•°æ®ï¼šæ›´é•¿çš„ TTL
        """
        base_ttl = self.ttl_seconds
        
        # æ£€æŸ¥æ•°æ®æ—¶é—´èŒƒå›´
        if 'coverage' in metadata:
            end_date = metadata['coverage'].get('end_date')
            if end_date:
                # å¦‚æœæ•°æ®ç»“æŸæ—¥æœŸæ˜¯è¿‡å»ï¼Œå»¶é•¿ TTL
                try:
                    end_dt = datetime.fromisoformat(end_date)
                    if end_dt < datetime.utcnow() - timedelta(days=30):
                        # å†å²æ•°æ®ï¼Œç¼“å­˜æ›´ä¹…
                        base_ttl *= 10
                except:
                    pass
        
        # æ ¹æ®è´¨é‡åˆ†æ•°è°ƒæ•´
        if 'quality' in metadata:
            quality_score = metadata['quality'].get('score', 0.5)
            if quality_score > 0.9:
                # é«˜è´¨é‡æ•°æ®ï¼Œç¼“å­˜æ›´ä¹…
                base_ttl *= 2
        
        return base_ttl
```

**å½±å“**: å‡å°‘ API è°ƒç”¨ï¼Œæé«˜å“åº”é€Ÿåº¦ï¼ŒèŠ‚çœé…é¢ã€‚

---

### 4.2 ç¼ºå°‘é™çº§å’Œå¤‡ç”¨ç­–ç•¥

**ç°çŠ¶é—®é¢˜**:
- å•ä¸ªæä¾›è€…å¤±è´¥æ—¶ï¼Œæ•´ä¸ªæŸ¥è¯¢å¤±è´¥
- æ²¡æœ‰å¤‡ç”¨æ•°æ®æº
- æ²¡æœ‰éƒ¨åˆ†ç»“æœè¿”å›æœºåˆ¶

**ä¼˜åŒ–å»ºè®®**:
```python
class FallbackStrategy:
    """é™çº§å’Œå¤‡ç”¨ç­–ç•¥"""
    
    # å®šä¹‰æä¾›è€…ä¹‹é—´çš„å¤‡ç”¨å…³ç³»
    FALLBACK_MAP = {
        'fred': ['worldbank'],  # FRED å¤±è´¥æ—¶å°è¯• World Bank
        'newsapi': [],  # News API æ²¡æœ‰å¤‡ç”¨
        'census': ['worldbank'],  # Census å¤±è´¥æ—¶å°è¯• World Bank
        'worldbank': []
    }
    
    # å®šä¹‰æ“ä½œæ˜ å°„ï¼ˆä¸åŒæä¾›è€…çš„ç­‰æ•ˆæ“ä½œï¼‰
    OPERATION_MAP = {
        ('fred', 'get_series'): [
            ('worldbank', 'get_indicator')
        ],
        ('census', 'get_population'): [
            ('worldbank', 'get_indicator')
        ]
    }
    
    def execute_with_fallback(
        self,
        primary_provider: str,
        operation: str,
        params: Dict[str, Any],
        providers_dict: Dict[str, BaseAPIProvider]
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œæ“ä½œï¼Œå¤±è´¥æ—¶è‡ªåŠ¨é™çº§åˆ°å¤‡ç”¨æä¾›è€…
        """
        
        result = {
            'success': False,
            'data': None,
            'attempts': [],
            'fallback_used': False
        }
        
        # å°è¯•ä¸»æä¾›è€…
        try:
            primary = providers_dict[primary_provider]
            data = primary.execute(operation, params)
            
            result['success'] = True
            result['data'] = data
            result['attempts'].append({
                'provider': primary_provider,
                'operation': operation,
                'status': 'success'
            })
            
            return result
        
        except Exception as primary_error:
            result['attempts'].append({
                'provider': primary_provider,
                'operation': operation,
                'status': 'failed',
                'error': str(primary_error)
            })
            
            # å°è¯•å¤‡ç”¨æä¾›è€…
            fallback_providers = self.FALLBACK_MAP.get(primary_provider, [])
            
            for fallback_provider in fallback_providers:
                if fallback_provider not in providers_dict:
                    continue
                
                # æŸ¥æ‰¾ç­‰æ•ˆæ“ä½œ
                fallback_ops = self.OPERATION_MAP.get(
                    (primary_provider, operation),
                    []
                )
                
                for fb_provider, fb_operation in fallback_ops:
                    if fb_provider != fallback_provider:
                        continue
                    
                    try:
                        # è½¬æ¢å‚æ•°
                        fb_params = self._convert_params(
                            primary_provider, operation, params,
                            fb_provider, fb_operation
                        )
                        
                        fb_instance = providers_dict[fb_provider]
                        data = fb_instance.execute(fb_operation, fb_params)
                        
                        result['success'] = True
                        result['data'] = data
                        result['fallback_used'] = True
                        result['attempts'].append({
                            'provider': fb_provider,
                            'operation': fb_operation,
                            'status': 'success'
                        })
                        
                        # æ·»åŠ é™çº§è­¦å‘Š
                        if 'metadata' in data:
                            data['metadata']['fallback_warning'] = (
                                f"Primary provider {primary_provider} failed, "
                                f"using fallback {fb_provider}"
                            )
                        
                        return result
                    
                    except Exception as fb_error:
                        result['attempts'].append({
                            'provider': fb_provider,
                            'operation': fb_operation,
                            'status': 'failed',
                            'error': str(fb_error)
                        })
        
        return result
```

**å½±å“**: æé«˜ç³»ç»Ÿå¯é æ€§ï¼Œå³ä½¿éƒ¨åˆ†æä¾›è€…å¤±è´¥ä¹Ÿèƒ½è¿”å›ç»“æœã€‚

---

## 5. å¯è§‚æµ‹æ€§ä¼˜åŒ– (â­â­â­)

### 5.1 ç¼ºå°‘è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡

**ç°çŠ¶é—®é¢˜**:
```python
# base_provider.py:112-117 - ç»Ÿè®¡ä¿¡æ¯è¿‡äºç®€å•
self.stats = {
    'total_requests': 0,
    'successful_requests': 0,
    'failed_requests': 0,
    'last_request_time': None
}
```

**é—®é¢˜**:
- âŒ æ²¡æœ‰å“åº”æ—¶é—´ç»Ÿè®¡
- âŒ æ²¡æœ‰æ•°æ®é‡ç»Ÿè®¡
- âŒ æ²¡æœ‰é”™è¯¯ç±»å‹åˆ†å¸ƒ
- âŒ æ²¡æœ‰æ€§èƒ½è¶‹åŠ¿

**ä¼˜åŒ–å»ºè®®**:
```python
class DetailedMetrics:
    """è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡æ”¶é›†"""
    
    def __init__(self):
        self.metrics = {
            'requests': {
                'total': 0,
                'successful': 0,
                'failed': 0,
                'cached': 0
            },
            'performance': {
                'response_times': [],  # æœ€è¿‘100æ¬¡
                'avg_response_time_ms': 0,
                'p50_response_time_ms': 0,
                'p95_response_time_ms': 0,
                'p99_response_time_ms': 0
            },
            'data_volume': {
                'total_records_fetched': 0,
                'total_bytes_transferred': 0,
                'avg_records_per_request': 0
            },
            'errors': {
                'by_type': {},  # {'timeout': 5, 'auth': 2, ...}
                'recent_errors': []  # æœ€è¿‘10ä¸ªé”™è¯¯
            },
            'rate_limiting': {
                'throttled_requests': 0,
                'avg_wait_time_ms': 0
            }
        }
    
    def record_request(
        self,
        success: bool,
        response_time_ms: float,
        record_count: int,
        bytes_transferred: int,
        cached: bool = False,
        error_type: Optional[str] = None
    ):
        """è®°å½•è¯·æ±‚æŒ‡æ ‡"""
        
        # æ›´æ–°è¯·æ±‚è®¡æ•°
        self.metrics['requests']['total'] += 1
        if success:
            self.metrics['requests']['successful'] += 1
        else:
            self.metrics['requests']['failed'] += 1
        
        if cached:
            self.metrics['requests']['cached'] += 1
        
        # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
        self.metrics['performance']['response_times'].append(response_time_ms)
        if len(self.metrics['performance']['response_times']) > 100:
            self.metrics['performance']['response_times'].pop(0)
        
        # è®¡ç®—ç™¾åˆ†ä½æ•°
        sorted_times = sorted(self.metrics['performance']['response_times'])
        if sorted_times:
            self.metrics['performance']['avg_response_time_ms'] = (
                sum(sorted_times) / len(sorted_times)
            )
            self.metrics['performance']['p50_response_time_ms'] = (
                sorted_times[len(sorted_times) // 2]
            )
            self.metrics['performance']['p95_response_time_ms'] = (
                sorted_times[int(len(sorted_times) * 0.95)]
            )
            self.metrics['performance']['p99_response_time_ms'] = (
                sorted_times[int(len(sorted_times) * 0.99)]
            )
        
        # æ›´æ–°æ•°æ®é‡æŒ‡æ ‡
        self.metrics['data_volume']['total_records_fetched'] += record_count
        self.metrics['data_volume']['total_bytes_transferred'] += bytes_transferred
        
        if self.metrics['requests']['total'] > 0:
            self.metrics['data_volume']['avg_records_per_request'] = (
                self.metrics['data_volume']['total_records_fetched'] /
                self.metrics['requests']['total']
            )
        
        # è®°å½•é”™è¯¯
        if not success and error_type:
            self.metrics['errors']['by_type'][error_type] = (
                self.metrics['errors']['by_type'].get(error_type, 0) + 1
            )
            
            self.metrics['errors']['recent_errors'].append({
                'type': error_type,
                'timestamp': datetime.utcnow().isoformat(),
                'response_time_ms': response_time_ms
            })
            
            if len(self.metrics['errors']['recent_errors']) > 10:
                self.metrics['errors']['recent_errors'].pop(0)
    
    def get_health_score(self) -> float:
        """
        è®¡ç®—å¥åº·åˆ†æ•° (0-1)
        
        è€ƒè™‘å› ç´ ï¼š
        - æˆåŠŸç‡
        - å“åº”æ—¶é—´
        - é”™è¯¯ç‡
        - ç¼“å­˜å‘½ä¸­ç‡
        """
        total = self.metrics['requests']['total']
        if total == 0:
            return 1.0
        
        # æˆåŠŸç‡åˆ†æ•° (40%)
        success_rate = self.metrics['requests']['successful'] / total
        success_score = success_rate * 0.4
        
        # æ€§èƒ½åˆ†æ•° (30%)
        avg_time = self.metrics['performance']['avg_response_time_ms']
        # å‡è®¾ < 200ms æ˜¯ä¼˜ç§€ï¼Œ> 2000ms æ˜¯å·®
        performance_score = max(0, min(1, (2000 - avg_time) / 1800)) * 0.3
        
        # ç¼“å­˜å‘½ä¸­ç‡åˆ†æ•° (20%)
        cache_rate = self.metrics['requests']['cached'] / total
        cache_score = cache_rate * 0.2
        
        # é”™è¯¯å¤šæ ·æ€§åˆ†æ•° (10%) - é”™è¯¯ç±»å‹è¶Šå°‘è¶Šå¥½
        error_types = len(self.metrics['errors']['by_type'])
        error_score = max(0, (5 - error_types) / 5) * 0.1
        
        return success_score + performance_score + cache_score + error_score
```

**å½±å“**: Agent å’Œå¼€å‘è€…å¯ä»¥ç›‘æ§ç³»ç»Ÿå¥åº·çŠ¶å†µï¼ŒåŠæ—¶å‘ç°é—®é¢˜ã€‚

---

## ğŸ“Š ä¼˜åŒ–ä¼˜å…ˆçº§çŸ©é˜µ

| ä¼˜åŒ–é¡¹ | å½±å“ | å®ç°éš¾åº¦ | ä¼˜å…ˆçº§ | é¢„ä¼°å·¥ä½œé‡ |
|--------|------|----------|--------|------------|
| 1.1 æ•°æ®è´¨é‡å…ƒæ•°æ® | â­â­â­â­â­ | ğŸ”§ğŸ”§ | **P0** | 2-3å¤© |
| 1.2 æ•°æ®éªŒè¯å’Œæ¸…æ´— | â­â­â­â­ | ğŸ”§ğŸ”§ğŸ”§ | **P0** | 3-5å¤© |
| 1.3 æ™ºèƒ½æ•°æ®è¿‡æ»¤ | â­â­â­â­ | ğŸ”§ğŸ”§ğŸ”§ğŸ”§ | **P1** | 5-7å¤© |
| 2.1 æŸ¥è¯¢æ„å›¾ç†è§£ | â­â­â­â­â­ | ğŸ”§ğŸ”§ğŸ”§ğŸ”§ | **P0** | 5-7å¤© |
| 2.2 è·¨æä¾›è€…èåˆ | â­â­â­â­ | ğŸ”§ğŸ”§ğŸ”§ğŸ”§ | **P1** | 4-6å¤© |
| 3.1 æ“ä½œæ–‡æ¡£å’Œç¤ºä¾‹ | â­â­â­â­â­ | ğŸ”§ğŸ”§ | **P0** | 2-3å¤© |
| 3.2 æ™ºèƒ½é”™è¯¯å¤„ç† | â­â­â­â­ | ğŸ”§ğŸ”§ğŸ”§ | **P0** | 3-4å¤© |
| 4.1 ç¼“å­˜å®ç° | â­â­â­ | ğŸ”§ğŸ”§ | **P1** | 2-3å¤© |
| 4.2 é™çº§å¤‡ç”¨ç­–ç•¥ | â­â­â­ | ğŸ”§ğŸ”§ğŸ”§ | **P2** | 3-4å¤© |
| 5.1 è¯¦ç»†æ€§èƒ½æŒ‡æ ‡ | â­â­â­ | ğŸ”§ğŸ”§ | **P2** | 2-3å¤© |

**æ€»è®¡**: çº¦ 31-45 å¤©å·¥ä½œé‡

---

## ğŸ¯ å¿«é€Ÿèƒœåˆ©ï¼ˆQuick Winsï¼‰

ä»¥ä¸‹ä¼˜åŒ–å¯ä»¥åœ¨ 1-2 å¤©å†…å®Œæˆï¼Œç«‹å³æå‡ Agent ä½“éªŒï¼š

### 1. æ·»åŠ åŸºç¡€è´¨é‡å…ƒæ•°æ® (1å¤©)
```python
# åœ¨ _format_response ä¸­æ·»åŠ 
'metadata': {
    'timestamp': datetime.utcnow().isoformat(),
    'source': source,
    'cached': False,
    'record_count': len(data) if isinstance(data, list) else 1,  # æ–°å¢
    'response_time_ms': response_time,  # æ–°å¢
}
```

### 2. æ”¹è¿›é”™è¯¯æ¶ˆæ¯ (1å¤©)
```python
# å°†é€šç”¨é”™è¯¯æ”¹ä¸ºå…·ä½“å»ºè®®
# ä¹‹å‰ï¼šraise ValueError(f"Invalid parameters: {error_msg}")
# ä¹‹åï¼š
raise ValueError(
    f"Invalid parameters for {operation}: {error_msg}\n"
    f"Required parameters: {required_params}\n"
    f"See schema: tool.get_operation_schema('{operation}')"
)
```

### 3. æ·»åŠ å‚æ•°éªŒè¯æç¤º (0.5å¤©)
```python
# åœ¨ validate_params ä¸­æ·»åŠ è¯¦ç»†æç¤º
if 'series_id' not in params:
    return False, (
        "Missing required parameter: series_id\n"
        "Example: {'series_id': 'GDP'}\n"
        "Use search_series to find valid series IDs"
    )
```

---

## ğŸ“ˆ é¢„æœŸæ”¶ç›Š

å®æ–½æ‰€æœ‰ P0 ä¼˜åŒ–åï¼Œé¢„æœŸï¼š

1. **Agent æŸ¥è¯¢æˆåŠŸç‡**: ä» ~70% æå‡åˆ° ~90%
2. **æ•°æ®è´¨é‡è¯„åˆ†**: ä»æ— è¯„åˆ†åˆ°å¹³å‡ 0.85+
3. **é”™è¯¯æ¢å¤ç‡**: ä» ~20% æå‡åˆ° ~60%
4. **å“åº”æ—¶é—´**: é€šè¿‡ç¼“å­˜å‡å°‘ 40-60%
5. **API é…é¢ä½¿ç”¨**: å‡å°‘ 30-50%

---

## ğŸ”š ç»“è®º

å½“å‰çš„ `apisource_tool` å’Œ `api_providers` å®ç°äº†åŸºç¡€åŠŸèƒ½ï¼Œä½†åœ¨å¸®åŠ© Agent è·å–é«˜è´¨é‡ç»“æœæ–¹é¢è¿˜æœ‰å¾ˆå¤§æå‡ç©ºé—´ã€‚

**æœ€å…³é”®çš„ä¼˜åŒ–æ–¹å‘**:
1. **æ•°æ®è´¨é‡å…ƒæ•°æ®** - è®© Agent çŸ¥é“æ•°æ®çš„å¯ä¿¡åº¦
2. **æŸ¥è¯¢æ„å›¾ç†è§£** - è®©å·¥å…·ç†è§£ Agent çœŸæ­£æƒ³è¦ä»€ä¹ˆ
3. **æ“ä½œæ–‡æ¡£** - è®© Agent çŸ¥é“å¦‚ä½•æ­£ç¡®ä½¿ç”¨å·¥å…·
4. **æ™ºèƒ½é”™è¯¯å¤„ç†** - è®© Agent åœ¨é‡åˆ°é—®é¢˜æ—¶çŸ¥é“å¦‚ä½•è§£å†³

å»ºè®®æŒ‰ç…§ P0 â†’ P1 â†’ P2 çš„é¡ºåºé€æ­¥å®æ–½ä¼˜åŒ–ã€‚

