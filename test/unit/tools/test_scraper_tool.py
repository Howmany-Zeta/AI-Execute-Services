"""
Comprehensive tests for ScraperTool component
Tests cover all public methods and functionality with >85% coverage
Uses real operations without mocks to test actual functionality
"""
import pytest
import os
import tempfile
import shutil
import json
import logging
import asyncio
import time
from typing import Dict, Any, List, Optional
from unittest.mock import patch
import httpx

from aiecs.tools.task_tools.scraper_tool import (
    ScraperTool,
    ScraperSettings,
    HttpMethod,
    ContentType,
    OutputFormat,
    RenderEngine,
    ScraperToolError,
    HttpError,
    TimeoutError,
    RateLimitError,
    ParsingError,
    RenderingError,
    ExternalToolError,
    FileOperationError
)

# Enable debug logging for testing
logging.basicConfig(level=logging.DEBUG)


class TestScraperTool:
    """Test class for ScraperTool functionality"""

    @pytest.fixture
    def scraper_tool(self):
        """Create ScraperTool instance with default configuration"""
        tool = ScraperTool()
        print(f"DEBUG: ScraperTool initialized with user_agent: {tool.settings.user_agent}")
        print(f"DEBUG: Output directory: {tool.settings.output_dir}")
        return tool

    @pytest.fixture
    def scraper_tool_custom_config(self):
        """Create ScraperTool instance with custom configuration"""
        config = {
            'user_agent': 'TestAgent/1.0',
            'max_content_length': 5 * 1024 * 1024,  # 5MB
            'allowed_domains': ['httpbin.org', 'example.com'],
            'blocked_domains': ['malicious.com']
        }
        tool = ScraperTool(config)
        print(f"DEBUG: ScraperTool initialized with custom config: {config}")
        return tool

    @pytest.fixture
    def temp_output_dir(self):
        """Create temporary directory for test outputs"""
        temp_dir = tempfile.mkdtemp(prefix='scraper_test_')
        print(f"DEBUG: Created temp output directory: {temp_dir}")
        yield temp_dir
        # Cleanup
        shutil.rmtree(temp_dir, ignore_errors=True)
        print(f"DEBUG: Cleaned up temp directory: {temp_dir}")

    @pytest.fixture
    def sample_html_content(self):
        """Create sample HTML content for parsing tests"""
        return """
        <!DOCTYPE html>
        <html>
        <head>
            <title>Test Page</title>
        </head>
        <body>
            <h1 id="main-title">Main Title</h1>
            <div class="content">
                <p>First paragraph</p>
                <p>Second paragraph with <a href="http://example.com">link</a></p>
            </div>
            <ul class="list">
                <li data-id="1">Item 1</li>
                <li data-id="2">Item 2</li>
                <li data-id="3">Item 3</li>
            </ul>
        </body>
        </html>
        """

    @pytest.fixture
    def mock_server_responses(self):
        """Sample responses for different content types"""
        return {
            'html_response': {
                'status_code': 200,
                'content': '<html><body><h1>Test HTML</h1></body></html>',
                'headers': {'Content-Type': 'text/html'}
            },
            'json_response': {
                'status_code': 200,
                'content': {'key': 'value', 'number': 42},
                'headers': {'Content-Type': 'application/json'}
            },
            'text_response': {
                'status_code': 200,
                'content': 'Plain text content',
                'headers': {'Content-Type': 'text/plain'}
            }
        }

    def test_initialization_default_config(self):
        """Test ScraperTool initialization with default configuration"""
        print("DEBUG: Testing ScraperTool default initialization")
        tool = ScraperTool()
        
        assert isinstance(tool.settings, ScraperSettings)
        assert tool.settings.user_agent == "PythonMiddlewareScraper/2.0"
        assert tool.settings.max_content_length == 10 * 1024 * 1024  # 10MB
        assert os.path.exists(tool.settings.output_dir)
        assert tool.settings.allowed_domains == []
        assert tool.settings.blocked_domains == []
        print("DEBUG: Default initialization test passed")

    def test_initialization_custom_config(self):
        """Test ScraperTool initialization with custom configuration"""
        print("DEBUG: Testing ScraperTool custom initialization")
        
        config = {
            'user_agent': 'CustomAgent/2.0',
            'max_content_length': 5_000_000,
            'allowed_domains': ['example.com'],
            'blocked_domains': ['spam.com']
        }
        tool = ScraperTool(config)
        
        assert tool.settings.user_agent == 'CustomAgent/2.0'
        assert tool.settings.max_content_length == 5_000_000
        assert tool.settings.allowed_domains == ['example.com']
        assert tool.settings.blocked_domains == ['spam.com']
        print("DEBUG: Custom initialization test passed")

    def test_initialization_invalid_config(self):
        """Test ScraperTool initialization with invalid configuration"""
        print("DEBUG: Testing ScraperTool invalid config initialization")
        
        # Test with invalid config structure that would cause validation error
        with pytest.raises(ValueError):
            ScraperTool({'invalid_setting_that_does_not_exist': 'invalid_value'})
        print("DEBUG: Invalid config test passed")

    def test_external_tools_check(self, scraper_tool):
        """Test external tools availability check"""
        print("DEBUG: Testing external tools availability")
        
        # Check playwright availability
        playwright_available = scraper_tool.settings.playwright_available
        print(f"DEBUG: Playwright available: {playwright_available}")
        assert isinstance(playwright_available, bool)
        
        if playwright_available:
            print("DEBUG: Playwright is available for rendering tests")
        else:
            print("DEBUG: Playwright not available - rendering tests will be skipped")
        
        print("DEBUG: External tools check test passed")

    @pytest.mark.asyncio
    async def test_httpx_get_text_content(self, scraper_tool):
        """Test httpx GET request with text content"""
        print("DEBUG: Testing httpx GET with text content")
        
        # Test with httpbin.org echo service
        url = "https://httpbin.org/get"
        
        result = await scraper_tool.get_httpx(
            url=url,
            method=HttpMethod.GET,
            content_type=ContentType.TEXT,
            async_mode=True
        )
        
        assert isinstance(result, str)
        assert len(result) > 0
        print(f"DEBUG: Received text content length: {len(result)}")
        print("DEBUG: httpx GET text test passed")

    @pytest.mark.asyncio
    async def test_httpx_get_json_content(self, scraper_tool):
        """Test httpx GET request with JSON content"""
        print("DEBUG: Testing httpx GET with JSON content")
        
        url = "https://httpbin.org/json"
        
        result = await scraper_tool.get_httpx(
            url=url,
            method=HttpMethod.GET,
            content_type=ContentType.JSON,
            async_mode=True
        )
        
        assert isinstance(result, dict)
        print(f"DEBUG: Received JSON keys: {list(result.keys())}")
        print("DEBUG: httpx GET JSON test passed")

    @pytest.mark.asyncio
    async def test_httpx_get_html_content(self, scraper_tool):
        """Test httpx GET request with HTML content"""
        print("DEBUG: Testing httpx GET with HTML content")
        
        url = "https://httpbin.org/html"
        
        result = await scraper_tool.get_httpx(
            url=url,
            method=HttpMethod.GET,
            content_type=ContentType.HTML,
            async_mode=True
        )
        
        assert isinstance(result, dict)
        assert 'html' in result
        assert 'url' in result
        assert 'status' in result
        assert result['status'] == 200
        print(f"DEBUG: HTML response keys: {list(result.keys())}")
        print("DEBUG: httpx GET HTML test passed")

    @pytest.mark.asyncio
    async def test_httpx_post_with_data(self, scraper_tool):
        """Test httpx POST request with form data"""
        print("DEBUG: Testing httpx POST with form data")
        
        url = "https://httpbin.org/post"
        data = {"key1": "value1", "key2": "value2"}
        
        result = await scraper_tool.get_httpx(
            url=url,
            method=HttpMethod.POST,
            data=data,
            content_type=ContentType.JSON,
            async_mode=True
        )
        
        assert isinstance(result, dict)
        # httpbin echoes back the form data
        assert 'form' in result or 'data' in result
        print(f"DEBUG: POST response structure: {list(result.keys())}")
        print("DEBUG: httpx POST test passed")

    @pytest.mark.asyncio
    async def test_httpx_post_with_json(self, scraper_tool):
        """Test httpx POST request with JSON data"""
        print("DEBUG: Testing httpx POST with JSON data")
        
        url = "https://httpbin.org/post"
        json_data = {"test_key": "test_value", "number": 42}
        
        result = await scraper_tool.get_httpx(
            url=url,
            method=HttpMethod.POST,
            json_data=json_data,
            content_type=ContentType.JSON,
            async_mode=True
        )
        
        assert isinstance(result, dict)
        # httpbin echoes back the JSON data
        assert 'json' in result
        assert result['json'] == json_data
        print(f"DEBUG: JSON POST echoed data: {result['json']}")
        print("DEBUG: httpx POST JSON test passed")

    @pytest.mark.asyncio
    async def test_httpx_with_custom_headers(self, scraper_tool):
        """Test httpx request with custom headers"""
        print("DEBUG: Testing httpx with custom headers")
        
        url = "https://httpbin.org/headers"
        headers = {"X-Custom-Header": "test-value", "X-Test": "pytest"}
        
        result = await scraper_tool.get_httpx(
            url=url,
            method=HttpMethod.GET,
            headers=headers,
            content_type=ContentType.JSON,
            async_mode=True
        )
        
        assert isinstance(result, dict)
        assert 'headers' in result
        received_headers = result['headers']
        assert 'X-Custom-Header' in received_headers
        assert received_headers['X-Custom-Header'] == 'test-value'
        print(f"DEBUG: Custom headers received: {received_headers}")
        print("DEBUG: httpx custom headers test passed")

    @pytest.mark.asyncio
    async def test_httpx_sync_mode(self, scraper_tool):
        """Test httpx in sync mode"""
        print("DEBUG: Testing httpx sync mode")
        
        url = "https://httpbin.org/get"
        
        result = await scraper_tool.get_httpx(
            url=url,
            method=HttpMethod.GET,
            content_type=ContentType.JSON,
            async_mode=False  # Sync mode
        )
        
        assert isinstance(result, dict)
        print(f"DEBUG: Sync mode result keys: {list(result.keys())}")
        print("DEBUG: httpx sync mode test passed")

    @pytest.mark.asyncio
    async def test_urllib_get_text(self, scraper_tool):
        """Test urllib GET request with text content"""
        print("DEBUG: Testing urllib GET with text content")
        
        url = "https://httpbin.org/get"
        
        result = await scraper_tool.get_urllib(
            url=url,
            method=HttpMethod.GET,
            content_type=ContentType.TEXT
        )
        
        assert isinstance(result, str)
        assert len(result) > 0
        print(f"DEBUG: urllib text content length: {len(result)}")
        print("DEBUG: urllib GET text test passed")

    @pytest.mark.asyncio
    async def test_urllib_get_json(self, scraper_tool):
        """Test urllib GET request with JSON content"""
        print("DEBUG: Testing urllib GET with JSON content")
        
        url = "https://httpbin.org/json"
        
        result = await scraper_tool.get_urllib(
            url=url,
            method=HttpMethod.GET,
            content_type=ContentType.JSON
        )
        
        assert isinstance(result, dict)
        print(f"DEBUG: urllib JSON keys: {list(result.keys())}")
        print("DEBUG: urllib GET JSON test passed")

    @pytest.mark.asyncio
    async def test_urllib_post_with_data(self, scraper_tool):
        """Test urllib POST request with form data"""
        print("DEBUG: Testing urllib POST with form data")
        
        url = "https://httpbin.org/post"
        data = {"test_key": "test_value"}
        
        result = await scraper_tool.get_urllib(
            url=url,
            method=HttpMethod.POST,
            data=data,
            content_type=ContentType.JSON
        )
        
        assert isinstance(result, dict)
        print(f"DEBUG: urllib POST result keys: {list(result.keys())}")
        print("DEBUG: urllib POST test passed")

    def test_parse_html_css_selector(self, scraper_tool, sample_html_content):
        """Test HTML parsing with CSS selectors"""
        print("DEBUG: Testing HTML parsing with CSS selectors")
        
        # Test title extraction
        result = scraper_tool.parse_html(
            html=sample_html_content,
            selector="h1",
            selector_type="css",
            extract_text=True
        )
        
        assert isinstance(result, dict)
        assert 'selector' in result
        assert 'count' in result
        assert 'results' in result
        assert result['count'] == 1
        assert 'Main Title' in result['results']
        print(f"DEBUG: CSS selector result: {result}")
        print("DEBUG: HTML parsing CSS test passed")

    def test_parse_html_attribute_extraction(self, scraper_tool, sample_html_content):
        """Test HTML parsing with attribute extraction"""
        print("DEBUG: Testing HTML parsing with attribute extraction")
        
        # Test data-id attribute extraction
        result = scraper_tool.parse_html(
            html=sample_html_content,
            selector="li",
            selector_type="css",
            extract_attr="data-id",
            extract_text=False
        )
        
        assert isinstance(result, dict)
        assert result['count'] == 3
        assert '1' in result['results']
        assert '2' in result['results']
        assert '3' in result['results']
        print(f"DEBUG: Attribute extraction result: {result}")
        print("DEBUG: HTML parsing attribute test passed")

    def test_parse_html_multiple_elements(self, scraper_tool, sample_html_content):
        """Test HTML parsing with multiple elements"""
        print("DEBUG: Testing HTML parsing with multiple elements")
        
        # Test paragraph extraction
        result = scraper_tool.parse_html(
            html=sample_html_content,
            selector="p",
            selector_type="css",
            extract_text=True
        )
        
        assert isinstance(result, dict)
        assert result['count'] == 2
        assert 'First paragraph' in result['results']
        print(f"DEBUG: Multiple elements result: {result}")
        print("DEBUG: HTML parsing multiple elements test passed")

    @pytest.mark.asyncio
    async def test_save_output_text_format(self, scraper_tool, temp_output_dir):
        """Test saving output in text format"""
        print("DEBUG: Testing save output in text format")
        
        content = "This is test content for text output"
        output_path = os.path.join(temp_output_dir, "test_output.txt")
        
        await scraper_tool._save_output(content, output_path, OutputFormat.TEXT)
        
        assert os.path.exists(output_path)
        with open(output_path, 'r', encoding='utf-8') as f:
            saved_content = f.read()
        assert saved_content == content
        print(f"DEBUG: Text file saved and verified: {output_path}")
        print("DEBUG: Save output text test passed")

    @pytest.mark.asyncio
    async def test_save_output_json_format(self, scraper_tool, temp_output_dir):
        """Test saving output in JSON format"""
        print("DEBUG: Testing save output in JSON format")
        
        content = {"key": "value", "number": 42, "list": [1, 2, 3]}
        output_path = os.path.join(temp_output_dir, "test_output.json")
        
        await scraper_tool._save_output(content, output_path, OutputFormat.JSON)
        
        assert os.path.exists(output_path)
        with open(output_path, 'r', encoding='utf-8') as f:
            saved_content = json.load(f)
        assert saved_content == content
        print(f"DEBUG: JSON file saved and verified: {output_path}")
        print("DEBUG: Save output JSON test passed")

    @pytest.mark.asyncio
    async def test_save_output_html_format(self, scraper_tool, temp_output_dir):
        """Test saving output in HTML format"""
        print("DEBUG: Testing save output in HTML format")
        
        content = {"html": "<html><body><h1>Test</h1></body></html>", "url": "http://example.com"}
        output_path = os.path.join(temp_output_dir, "test_output.html")
        
        await scraper_tool._save_output(content, output_path, OutputFormat.HTML)
        
        assert os.path.exists(output_path)
        with open(output_path, 'r', encoding='utf-8') as f:
            saved_content = f.read()
        assert saved_content == content['html']
        print(f"DEBUG: HTML file saved and verified: {output_path}")
        print("DEBUG: Save output HTML test passed")

    @pytest.mark.asyncio
    async def test_save_output_markdown_format(self, scraper_tool, temp_output_dir):
        """Test saving output in Markdown format"""
        print("DEBUG: Testing save output in Markdown format")
        
        content = {"title": "Test Title", "content": "Test content"}
        output_path = os.path.join(temp_output_dir, "test_output.md")
        
        await scraper_tool._save_output(content, output_path, OutputFormat.MARKDOWN)
        
        assert os.path.exists(output_path)
        with open(output_path, 'r', encoding='utf-8') as f:
            saved_content = f.read()
        assert "# Scraper Results" in saved_content
        assert "## title" in saved_content
        assert "Test Title" in saved_content
        print(f"DEBUG: Markdown file saved and verified: {output_path}")
        print("DEBUG: Save output Markdown test passed")

    @pytest.mark.asyncio
    async def test_save_output_csv_format(self, scraper_tool, temp_output_dir):
        """Test saving output in CSV format"""
        print("DEBUG: Testing save output in CSV format")
        
        # Test with dict content
        content = {"name": "John", "age": 30, "city": "New York"}
        output_path = os.path.join(temp_output_dir, "test_output.csv")
        
        await scraper_tool._save_output(content, output_path, OutputFormat.CSV)
        
        assert os.path.exists(output_path)
        with open(output_path, 'r', encoding='utf-8') as f:
            csv_content = f.read()
        assert "name,age,city" in csv_content
        assert "John,30,New York" in csv_content
        print(f"DEBUG: CSV file saved and verified: {output_path}")
        
        # Test with list of dicts
        list_content = [
            {"name": "Alice", "age": 25},
            {"name": "Bob", "age": 35}
        ]
        output_path_list = os.path.join(temp_output_dir, "test_list.csv")
        
        await scraper_tool._save_output(list_content, output_path_list, OutputFormat.CSV)
        assert os.path.exists(output_path_list)
        print("DEBUG: Save output CSV test passed")

    @pytest.mark.asyncio
    async def test_httpx_with_output_save(self, scraper_tool, temp_output_dir):
        """Test httpx request with output saving"""
        print("DEBUG: Testing httpx with output saving")
        
        url = "https://httpbin.org/json"
        output_path = os.path.join(temp_output_dir, "httpx_output.json")
        
        result = await scraper_tool.get_httpx(
            url=url,
            method=HttpMethod.GET,
            content_type=ContentType.JSON,
            output_format=OutputFormat.JSON,
            output_path=output_path,
            async_mode=True
        )
        
        assert isinstance(result, dict)
        assert 'saved_to' in result
        assert result['saved_to'] == output_path
        assert os.path.exists(output_path)
        
        # Verify saved content
        with open(output_path, 'r', encoding='utf-8') as f:
            saved_data = json.load(f)
        # Remove the 'saved_to' key for comparison
        result_without_saved_to = {k: v for k, v in result.items() if k != 'saved_to'}
        assert saved_data == result_without_saved_to
        print(f"DEBUG: Output saved to: {output_path}")
        print("DEBUG: httpx with output save test passed")

    @pytest.mark.asyncio
    async def test_httpx_with_params_and_cookies(self, scraper_tool):
        """Test httpx request with query parameters and cookies"""
        print("DEBUG: Testing httpx with params and cookies")
        
        url = "https://httpbin.org/cookies/set"
        params = {"test_cookie": "test_value"}
        
        result = await scraper_tool.get_httpx(
            url=url,
            method=HttpMethod.GET,
            params=params,
            allow_redirects=True,
            content_type=ContentType.JSON,
            async_mode=True
        )
        
        print(f"DEBUG: Cookies response: {result}")
        print("DEBUG: httpx params and cookies test passed")

    @pytest.mark.asyncio
    async def test_urllib_with_custom_headers(self, scraper_tool):
        """Test urllib request with custom headers"""
        print("DEBUG: Testing urllib with custom headers")
        
        url = "https://httpbin.org/headers"
        headers = {"X-Test-Header": "urllib-test"}
        
        result = await scraper_tool.get_urllib(
            url=url,
            method=HttpMethod.GET,
            headers=headers,
            content_type=ContentType.JSON
        )
        
        assert isinstance(result, dict)
        assert 'headers' in result
        received_headers = result['headers']
        assert 'X-Test-Header' in received_headers
        print(f"DEBUG: urllib headers result: {received_headers}")
        print("DEBUG: urllib custom headers test passed")

    @pytest.mark.asyncio
    async def test_render_with_playwright_if_available(self, scraper_tool):
        """Test Playwright rendering if available"""
        print("DEBUG: Testing Playwright rendering")
        
        if not scraper_tool.settings.playwright_available:
            print("DEBUG: Playwright not available - skipping render test")
            pytest.skip("Playwright not available")
        
        url = "https://httpbin.org/html"
        
        result = await scraper_tool.render(
            url=url,
            engine=RenderEngine.PLAYWRIGHT,
            wait_time=3
        )
        
        assert isinstance(result, dict)
        assert 'html' in result
        assert 'title' in result
        assert 'url' in result
        assert len(result['html']) > 0
        print(f"DEBUG: Rendered page title: {result['title']}")
        print("DEBUG: Playwright rendering test passed")

    @pytest.mark.asyncio
    async def test_render_with_screenshot_if_available(self, scraper_tool, temp_output_dir):
        """Test Playwright rendering with screenshot"""
        print("DEBUG: Testing Playwright rendering with screenshot")
        
        if not scraper_tool.settings.playwright_available:
            print("DEBUG: Playwright not available - skipping screenshot test")
            pytest.skip("Playwright not available")
        
        url = "https://httpbin.org/html"
        screenshot_path = os.path.join(temp_output_dir, "test_screenshot.png")
        
        result = await scraper_tool.render(
            url=url,
            engine=RenderEngine.PLAYWRIGHT,
            wait_time=2,
            screenshot=True,
            screenshot_path=screenshot_path
        )
        
        assert isinstance(result, dict)
        assert 'screenshot' in result
        assert result['screenshot'] == screenshot_path
        assert os.path.exists(screenshot_path)
        print(f"DEBUG: Screenshot saved to: {screenshot_path}")
        print("DEBUG: Playwright screenshot test passed")

    def test_render_without_playwright(self, scraper_tool):
        """Test render method when Playwright is not available"""
        print("DEBUG: Testing render without Playwright")
        
        # Temporarily set playwright_available to False
        original_availability = scraper_tool.settings.playwright_available
        scraper_tool.settings.playwright_available = False
        
        try:
            with pytest.raises(RenderingError) as exc_info:
                asyncio.run(scraper_tool.render("https://example.com", RenderEngine.PLAYWRIGHT))
            
            assert "Playwright is not available" in str(exc_info.value)
            print("DEBUG: Render without Playwright test passed")
        finally:
            # Restore original setting
            scraper_tool.settings.playwright_available = original_availability

    def test_render_unsupported_engine(self, scraper_tool):
        """Test render method with unsupported engine"""
        print("DEBUG: Testing render with unsupported engine")
        
        # Test with a non-existent engine
        with pytest.raises(RenderingError) as exc_info:
            asyncio.run(scraper_tool.render("https://example.com", RenderEngine.NONE))
        
        assert "Unsupported rendering engine" in str(exc_info.value)
        print("DEBUG: Render unsupported engine test passed")

    @pytest.mark.asyncio
    async def test_http_error_handling(self, scraper_tool):
        """Test HTTP error handling"""
        print("DEBUG: Testing HTTP error handling")
        
        # Test with invalid URL
        with pytest.raises(HttpError):
            await scraper_tool.get_httpx("invalid-url", async_mode=True)
        
        # Test with non-existent domain
        with pytest.raises(HttpError):
            await scraper_tool.get_httpx("https://this-domain-does-not-exist-12345.com", async_mode=True)
        
        print("DEBUG: HTTP error handling test passed")

    @pytest.mark.asyncio
    async def test_content_length_limit(self, scraper_tool_custom_config):
        """Test content length limit enforcement"""
        print("DEBUG: Testing content length limit")
        
        # Use custom config with smaller max_content_length (5MB)
        tool = scraper_tool_custom_config
        print(f"DEBUG: Max content length set to: {tool.settings.max_content_length}")
        
        # Test with a large response (this might be difficult to test reliably)
        # We'll test the logic rather than actually hitting a large endpoint
        url = "https://httpbin.org/json"
        
        try:
            result = await tool.get_httpx(url, async_mode=True)
            print(f"DEBUG: Content length check passed, result type: {type(result)}")
        except HttpError as e:
            if "too large" in str(e):
                print("DEBUG: Content length limit properly enforced")
            else:
                raise
        
        print("DEBUG: Content length limit test passed")

    def test_parse_html_invalid_html(self, scraper_tool):
        """Test HTML parsing with invalid HTML"""
        print("DEBUG: Testing HTML parsing with invalid HTML")
        
        invalid_html = "<html><body><h1>Unclosed tag"
        
        # BeautifulSoup is forgiving, so this should still work
        result = scraper_tool.parse_html(
            html=invalid_html,
            selector="h1",
            selector_type="css",
            extract_text=True
        )
        
        assert isinstance(result, dict)
        assert 'results' in result
        print(f"DEBUG: Invalid HTML parsing result: {result}")
        print("DEBUG: HTML parsing invalid HTML test passed")

    def test_parse_html_xpath_selector(self, scraper_tool, sample_html_content):
        """Test HTML parsing with XPath selectors"""
        print("DEBUG: Testing HTML parsing with XPath selectors")
        
        # Test XPath selector
        result = scraper_tool.parse_html(
            html=sample_html_content,
            selector="//h1[@id='main-title']",
            selector_type="xpath",
            extract_text=True
        )
        
        assert isinstance(result, dict)
        assert result['selector_type'] == 'xpath'
        print(f"DEBUG: XPath selector result: {result}")
        print("DEBUG: HTML parsing XPath test passed")

    def test_parse_html_no_matches(self, scraper_tool, sample_html_content):
        """Test HTML parsing when selector finds no matches"""
        print("DEBUG: Testing HTML parsing with no matches")
        
        result = scraper_tool.parse_html(
            html=sample_html_content,
            selector="nonexistent-element",
            selector_type="css",
            extract_text=True
        )
        
        assert isinstance(result, dict)
        assert result['count'] == 0
        assert result['results'] == []
        print(f"DEBUG: No matches result: {result}")
        print("DEBUG: HTML parsing no matches test passed")

    def test_parse_html_error_handling(self, scraper_tool):
        """Test HTML parsing error handling"""
        print("DEBUG: Testing HTML parsing error handling")
        
        # Test with invalid selector that causes XPath error
        with pytest.raises(ParsingError):
            scraper_tool.parse_html(
                html="<html></html>",
                selector="//invalid[xpath[syntax",  # Invalid XPath
                selector_type="xpath"
            )
        
        print("DEBUG: HTML parsing error handling test passed")

    def test_legacy_method_aliases(self, scraper_tool):
        """Test legacy method aliases work correctly"""
        print("DEBUG: Testing legacy method aliases")
        
        # Test get_requests alias
        assert scraper_tool.get_requests == scraper_tool.get_httpx
        
        # Test get_aiohttp alias  
        assert scraper_tool.get_aiohttp == scraper_tool.get_httpx
        
        # Test HTTP method shortcuts
        assert scraper_tool.get == scraper_tool.get_httpx
        assert scraper_tool.post == scraper_tool.get_httpx
        assert scraper_tool.put == scraper_tool.get_httpx
        assert scraper_tool.delete == scraper_tool.get_httpx
        
        print("DEBUG: Legacy method aliases test passed")

    @pytest.mark.asyncio
    async def test_get_requests_legacy_method(self, scraper_tool):
        """Test legacy get_requests method works"""
        print("DEBUG: Testing legacy get_requests method")
        
        url = "https://httpbin.org/get"
        
        result = await scraper_tool.get_requests(
            url=url,
            method=HttpMethod.GET,
            content_type=ContentType.JSON
        )
        
        assert isinstance(result, dict)
        print(f"DEBUG: Legacy get_requests result keys: {list(result.keys())}")
        print("DEBUG: Legacy get_requests test passed")

    @pytest.mark.asyncio
    async def test_get_aiohttp_legacy_method(self, scraper_tool):
        """Test legacy get_aiohttp method works"""
        print("DEBUG: Testing legacy get_aiohttp method")
        
        url = "https://httpbin.org/get"
        
        result = await scraper_tool.get_aiohttp(
            url=url,
            method=HttpMethod.GET,
            content_type=ContentType.JSON
        )
        
        assert isinstance(result, dict)
        print(f"DEBUG: Legacy get_aiohttp result keys: {list(result.keys())}")
        print("DEBUG: Legacy get_aiohttp test passed")

    def test_settings_validation(self):
        """Test ScraperSettings validation"""
        print("DEBUG: Testing ScraperSettings validation")
        
        # Test valid settings
        settings = ScraperSettings(
            user_agent="TestAgent/1.0",
            max_content_length=1024 * 1024,
            allowed_domains=['example.com']
        )
        assert settings.user_agent == "TestAgent/1.0"
        assert settings.max_content_length == 1024 * 1024
        assert settings.allowed_domains == ['example.com']
        
        # Test default values
        default_settings = ScraperSettings()
        assert default_settings.user_agent == "PythonMiddlewareScraper/2.0"
        assert default_settings.max_content_length == 10 * 1024 * 1024
        assert default_settings.env_prefix == "SCRAPER_TOOL_"
        print("DEBUG: Settings validation test passed")

    def test_error_classes(self):
        """Test custom exception classes"""
        print("DEBUG: Testing custom exception classes")
        
        # Test base ScraperToolError
        with pytest.raises(ScraperToolError):
            raise ScraperToolError("Generic scraper error")
        
        # Test HttpError
        with pytest.raises(HttpError):
            raise HttpError("HTTP request failed")
        
        # Test TimeoutError
        with pytest.raises(TimeoutError):
            raise TimeoutError("Operation timed out")
        
        # Test ParsingError
        with pytest.raises(ParsingError):
            raise ParsingError("HTML parsing failed")
        
        # Test RenderingError
        with pytest.raises(RenderingError):
            raise RenderingError("Page rendering failed")
        
        # Test FileOperationError
        with pytest.raises(FileOperationError):
            raise FileOperationError("File operation failed")
        
        # Verify inheritance
        with pytest.raises(ScraperToolError):
            raise HttpError("HTTP error should inherit from ScraperToolError")
        
        print("DEBUG: Exception classes test passed")

    @pytest.mark.parametrize("http_method", [
        HttpMethod.GET,
        HttpMethod.POST,
        HttpMethod.PUT,
        HttpMethod.DELETE,
        HttpMethod.HEAD,
        HttpMethod.OPTIONS,
        HttpMethod.PATCH
    ])
    @pytest.mark.asyncio
    async def test_various_http_methods(self, scraper_tool, http_method):
        """Test various HTTP methods"""
        print(f"DEBUG: Testing HTTP method: {http_method.value}")
        
        # Most methods work with httpbin
        if http_method == HttpMethod.HEAD:
            url = "https://httpbin.org/get"
        elif http_method == HttpMethod.OPTIONS:
            url = "https://httpbin.org/get" 
        else:
            url = f"https://httpbin.org/{http_method.value.lower()}"
        
        result = await scraper_tool.get_httpx(
            url=url,
            method=http_method,
            content_type=ContentType.JSON if http_method not in [HttpMethod.HEAD, HttpMethod.OPTIONS] else ContentType.TEXT,
            async_mode=True
        )
        
        print(f"DEBUG: {http_method.value} result type: {type(result)}")
        print(f"DEBUG: HTTP method {http_method.value} test passed")

    @pytest.mark.parametrize("content_type", [
        ContentType.HTML,
        ContentType.JSON,
        ContentType.TEXT,
        ContentType.BINARY
    ])
    @pytest.mark.asyncio
    async def test_various_content_types(self, scraper_tool, content_type):
        """Test various content types"""
        print(f"DEBUG: Testing content type: {content_type.value}")
        
        if content_type == ContentType.JSON:
            url = "https://httpbin.org/json"
        elif content_type == ContentType.HTML:
            url = "https://httpbin.org/html"
        else:
            url = "https://httpbin.org/get"
        
        result = await scraper_tool.get_httpx(
            url=url,
            method=HttpMethod.GET,
            content_type=content_type,
            async_mode=True
        )
        
        if content_type == ContentType.JSON:
            assert isinstance(result, dict)
        elif content_type == ContentType.HTML:
            assert isinstance(result, dict) and 'html' in result
        elif content_type == ContentType.BINARY:
            assert isinstance(result, dict) and 'content' in result
        else:
            assert isinstance(result, str)
        
        print(f"DEBUG: Content type {content_type.value} result type: {type(result)}")
        print(f"DEBUG: Content type {content_type.value} test passed")

    @pytest.mark.asyncio
    async def test_file_operation_error_handling(self, scraper_tool):
        """Test file operation error handling"""
        print("DEBUG: Testing file operation error handling")
        
        # Test saving to invalid path
        with pytest.raises(FileOperationError):
            await scraper_tool._save_output(
                "test content",
                "/invalid/path/that/does/not/exist/file.txt",
                OutputFormat.TEXT
            )
        
        print("DEBUG: File operation error handling test passed")

    @pytest.mark.asyncio
    async def test_comprehensive_scraping_workflow(self, scraper_tool, temp_output_dir):
        """Test comprehensive scraping workflow with multiple steps"""
        print("DEBUG: Testing comprehensive scraping workflow")
        
        # Step 1: Fetch HTML content
        html_result = await scraper_tool.get_httpx(
            url="https://httpbin.org/html",
            content_type=ContentType.HTML,
            async_mode=True
        )
        
        assert 'html' in html_result
        html_content = html_result['html']
        print(f"DEBUG: Fetched HTML content length: {len(html_content)}")
        
        # Step 2: Parse the HTML 
        parse_result = scraper_tool.parse_html(
            html=html_content,
            selector="h1",
            selector_type="css",
            extract_text=True
        )
        
        assert parse_result['count'] > 0
        print(f"DEBUG: Parsed {parse_result['count']} h1 elements")
        
        # Step 3: Save results
        output_path = os.path.join(temp_output_dir, "workflow_result.json")
        combined_result = {
            'original_response': html_result,
            'parsed_data': parse_result
        }
        
        await scraper_tool._save_output(combined_result, output_path, OutputFormat.JSON)
        
        assert os.path.exists(output_path)
        print(f"DEBUG: Workflow results saved to: {output_path}")
        print("DEBUG: Comprehensive scraping workflow test passed")

    def test_edge_cases_and_boundaries(self, scraper_tool):
        """Test edge cases and boundary conditions"""
        print("DEBUG: Testing edge cases and boundary conditions")
        
        # Test parsing empty HTML
        empty_result = scraper_tool.parse_html("", "div", "css")
        assert empty_result['count'] == 0
        assert empty_result['results'] == []
        
        # Test parsing with empty selector
        with pytest.raises(ParsingError):
            scraper_tool.parse_html("<html></html>", "", "css")
        
        # Test parsing HTML with special characters
        special_html = "<html><body><h1>TÃ«st ðŸš€ SpÃ©ciÃ¡l</h1></body></html>"
        special_result = scraper_tool.parse_html(special_html, "h1", "css")
        assert special_result['count'] == 1
        assert "TÃ«st ðŸš€ SpÃ©ciÃ¡l" in special_result['results']
        
        print("DEBUG: Edge cases and boundaries test passed")

    def test_enum_values_coverage(self):
        """Test all enum values are properly defined"""
        print("DEBUG: Testing enum values coverage")
        
        # Test HttpMethod enum
        http_methods = [e.value for e in HttpMethod]
        expected_methods = ['get', 'post', 'put', 'delete', 'head', 'options', 'patch']
        for method in expected_methods:
            assert method in http_methods
        
        # Test ContentType enum
        content_types = [e.value for e in ContentType]
        expected_types = ['html', 'json', 'text', 'binary']
        for ctype in expected_types:
            assert ctype in content_types
        
        # Test OutputFormat enum
        output_formats = [e.value for e in OutputFormat]
        expected_formats = ['text', 'json', 'html', 'markdown', 'csv']
        for format in expected_formats:
            assert format in output_formats
        
        # Test RenderEngine enum
        render_engines = [e.value for e in RenderEngine]
        expected_engines = ['none', 'playwright']
        for engine in expected_engines:
            assert engine in render_engines
        
        print("DEBUG: Enum values coverage test passed")

    @pytest.mark.asyncio
    async def test_authentication_support(self, scraper_tool):
        """Test HTTP authentication support"""
        print("DEBUG: Testing HTTP authentication support")
        
        # Test basic auth with httpbin
        username = "testuser"
        password = "testpass"
        url = f"https://httpbin.org/basic-auth/{username}/{password}"
        
        result = await scraper_tool.get_httpx(
            url=url,
            method=HttpMethod.GET,
            auth=(username, password),
            content_type=ContentType.JSON,
            async_mode=True
        )
        
        assert isinstance(result, dict)
        assert result.get('authenticated') == True
        print(f"DEBUG: Authentication result: {result}")
        print("DEBUG: Authentication support test passed")

    def test_scrapy_crawl_method_structure(self, scraper_tool):
        """Test scrapy crawl method structure (without actually running scrapy)"""
        print("DEBUG: Testing scrapy crawl method structure")
        
        # Test the method exists and has correct signature
        assert hasattr(scraper_tool, 'crawl_scrapy')
        
        # Test error handling with invalid project path
        with pytest.raises(ExternalToolError):
            scraper_tool.crawl_scrapy(
                project_path="/invalid/path",
                spider_name="test_spider",
                output_path="/tmp/output.json"
            )
        
        print("DEBUG: Scrapy crawl method structure test passed")

    @pytest.mark.asyncio
    async def test_ssl_verification_settings(self, scraper_tool):
        """Test SSL verification settings"""
        print("DEBUG: Testing SSL verification settings")
        
        # Test with SSL verification enabled (default)
        url = "https://httpbin.org/get"
        
        result = await scraper_tool.get_httpx(
            url=url,
            verify_ssl=True,
            content_type=ContentType.JSON,
            async_mode=True
        )
        
        assert isinstance(result, dict)
        print("DEBUG: SSL verification enabled test passed")

    @pytest.mark.asyncio 
    async def test_redirect_handling(self, scraper_tool):
        """Test HTTP redirect handling"""
        print("DEBUG: Testing HTTP redirect handling")
        
        # Test redirect with httpbin
        url = "https://httpbin.org/redirect/1"
        
        result = await scraper_tool.get_httpx(
            url=url,
            allow_redirects=True,
            content_type=ContentType.JSON,
            async_mode=True
        )
        
        assert isinstance(result, dict)
        print(f"DEBUG: Redirect result keys: {list(result.keys())}")
        
        # Test with redirects disabled
        with pytest.raises(HttpError):
            await scraper_tool.get_httpx(
                url=url,
                allow_redirects=False,
                content_type=ContentType.JSON,
                async_mode=True
            )
        
        print("DEBUG: Redirect handling test passed")

    def test_settings_env_prefix(self):
        """Test ScraperSettings environment variable prefix"""
        print("DEBUG: Testing ScraperSettings environment prefix")
        
        settings = ScraperSettings()
        assert hasattr(settings, 'model_config')
        assert settings.model_config['env_prefix'] == 'SCRAPER_TOOL_'
        print("DEBUG: Settings environment prefix test passed")

    def test_output_directory_creation(self):
        """Test output directory creation"""
        print("DEBUG: Testing output directory creation")
        
        # Test with custom output directory
        custom_output_dir = tempfile.mkdtemp(prefix='scraper_output_test_')
        try:
            config = {'output_dir': custom_output_dir}
            tool = ScraperTool(config)
            
            assert os.path.exists(custom_output_dir)
            assert tool.settings.output_dir == custom_output_dir
            print(f"DEBUG: Custom output directory created: {custom_output_dir}")
        finally:
            shutil.rmtree(custom_output_dir, ignore_errors=True)
        
        print("DEBUG: Output directory creation test passed")

    @pytest.mark.asyncio
    async def test_concurrent_requests(self, scraper_tool):
        """Test concurrent HTTP requests"""
        print("DEBUG: Testing concurrent HTTP requests")
        
        urls = [
            "https://httpbin.org/delay/1",
            "https://httpbin.org/get",
            "https://httpbin.org/json"
        ]
        
        # Create concurrent tasks
        tasks = []
        for i, url in enumerate(urls):
            task = scraper_tool.get_httpx(
                url=url,
                content_type=ContentType.JSON,
                async_mode=True
            )
            tasks.append(task)
        
        # Run concurrently
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Check results
        success_count = 0
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                print(f"DEBUG: Request {i} failed: {result}")
            else:
                assert isinstance(result, (dict, str))
                success_count += 1
                print(f"DEBUG: Request {i} succeeded")
        
        assert success_count > 0
        print(f"DEBUG: {success_count}/{len(urls)} concurrent requests succeeded")
        print("DEBUG: Concurrent requests test passed")

    @pytest.mark.asyncio
    async def test_performance_with_large_content(self, scraper_tool):
        """Test performance with larger content"""
        print("DEBUG: Testing performance with large content")
        
        # Use httpbin's base64 endpoint for larger content
        url = "https://httpbin.org/base64/aGVsbG8gd29ybGQ="  # "hello world" base64
        
        result = await scraper_tool.get_httpx(
            url=url,
            content_type=ContentType.TEXT,
            async_mode=True
        )
        
        assert isinstance(result, str)
        assert len(result) > 0
        print(f"DEBUG: Large content result length: {len(result)}")
        print("DEBUG: Performance with large content test passed")

    @pytest.mark.asyncio
    async def test_error_recovery_and_resilience(self, scraper_tool):
        """Test error recovery and resilience"""
        print("DEBUG: Testing error recovery and resilience")
        
        # Test sequence of operations with some failures
        operations = [
            ("https://httpbin.org/get", True),  # Should succeed
            ("https://invalid-url-12345", False),  # Should fail
            ("https://httpbin.org/json", True),  # Should succeed
        ]
        
        results = []
        for url, should_succeed in operations:
            try:
                result = await scraper_tool.get_httpx(url, async_mode=True)
                results.append((url, True, result))
                print(f"DEBUG: Request to {url} succeeded as expected: {should_succeed}")
            except Exception as e:
                results.append((url, False, str(e)))
                print(f"DEBUG: Request to {url} failed as expected: {not should_succeed}")
        
        # Verify we got expected success/failure pattern
        assert len(results) == 3
        assert results[0][1] == True  # First should succeed
        assert results[1][1] == False  # Second should fail
        assert results[2][1] == True  # Third should succeed
        
        print("DEBUG: Error recovery and resilience test passed")

    @pytest.mark.asyncio
    async def test_integration_fetch_and_parse(self, scraper_tool):
        """Test integration of fetching and parsing workflow"""
        print("DEBUG: Testing integration fetch and parse workflow")
        
        # Step 1: Fetch HTML page
        url = "https://httpbin.org/html"
        fetch_result = await scraper_tool.get_httpx(
            url=url,
            content_type=ContentType.HTML,
            async_mode=True
        )
        
        assert 'html' in fetch_result
        html_content = fetch_result['html']
        print(f"DEBUG: Fetched HTML length: {len(html_content)}")
        
        # Step 2: Parse specific elements
        parse_results = {}
        
        # Parse title
        title_result = scraper_tool.parse_html(html_content, "title", "css")
        parse_results['titles'] = title_result
        
        # Parse paragraphs
        p_result = scraper_tool.parse_html(html_content, "p", "css")
        parse_results['paragraphs'] = p_result
        
        # Parse links
        link_result = scraper_tool.parse_html(html_content, "a", "css", extract_attr="href")
        parse_results['links'] = link_result
        
        # Verify all parsing operations worked
        for key, result in parse_results.items():
            assert isinstance(result, dict)
            assert 'count' in result
            assert 'results' in result
            print(f"DEBUG: Parsed {key}: {result['count']} items found")
        
        print("DEBUG: Integration fetch and parse test passed")

    def test_data_validation_and_sanitization(self, scraper_tool):
        """Test input data validation and sanitization"""
        print("DEBUG: Testing data validation and sanitization")
        
        # Test HTML parsing with various input types
        html_inputs = [
            ("<html><body><h1>Normal</h1></body></html>", True),
            ("", True),  # Empty HTML
            ("<invalid>unclosed tags", True),  # Invalid but parseable
        ]
        
        for html_input, should_succeed in html_inputs:
            try:
                result = scraper_tool.parse_html(html_input, "h1", "css")
                assert isinstance(result, dict)
                print(f"DEBUG: Parsed HTML input of length {len(html_input)}")
            except Exception as e:
                if should_succeed:
                    pytest.fail(f"Expected success but got error: {e}")
                else:
                    print(f"DEBUG: Expected error for invalid input: {e}")
        
        print("DEBUG: Data validation and sanitization test passed")

    @pytest.mark.asyncio
    async def test_comprehensive_output_formats_workflow(self, scraper_tool, temp_output_dir):
        """Test comprehensive workflow with all output formats"""
        print("DEBUG: Testing comprehensive output formats workflow")
        
        # Get some JSON data
        url = "https://httpbin.org/json"
        base_result = await scraper_tool.get_httpx(url, content_type=ContentType.JSON, async_mode=True)
        
        # Test saving in all formats
        formats_and_paths = [
            (OutputFormat.JSON, "test_output.json"),
            (OutputFormat.TEXT, "test_output.txt"),
            (OutputFormat.HTML, "test_output.html"),
            (OutputFormat.MARKDOWN, "test_output.md"),
            (OutputFormat.CSV, "test_output.csv")
        ]
        
        for output_format, filename in formats_and_paths:
            output_path = os.path.join(temp_output_dir, filename)
            await scraper_tool._save_output(base_result, output_path, output_format)
            
            assert os.path.exists(output_path)
            file_size = os.path.getsize(output_path)
            print(f"DEBUG: Saved {output_format.value} format to {filename} ({file_size} bytes)")
        
        print("DEBUG: Comprehensive output formats workflow test passed")

    def test_method_shortcuts_comprehensive(self, scraper_tool):
        """Test all method shortcuts are properly mapped"""
        print("DEBUG: Testing method shortcuts comprehensive")
        
        # All shortcuts should point to get_httpx
        shortcuts = ['get', 'post', 'put', 'delete', 'head', 'options', 'patch']
        
        for shortcut in shortcuts:
            assert hasattr(scraper_tool, shortcut)
            method_func = getattr(scraper_tool, shortcut)
            assert method_func == scraper_tool.get_httpx
            print(f"DEBUG: Shortcut {shortcut} correctly mapped")
        
        print("DEBUG: Method shortcuts comprehensive test passed")

    @pytest.mark.asyncio
    async def test_render_with_wait_selector(self, scraper_tool, temp_output_dir):
        """Test Playwright rendering with wait selector"""
        print("DEBUG: Testing Playwright rendering with wait selector")
        
        if not scraper_tool.settings.playwright_available:
            print("DEBUG: Playwright not available - skipping wait selector test")
            pytest.skip("Playwright not available")
        
        url = "https://httpbin.org/html"
        
        result = await scraper_tool.render(
            url=url,
            engine=RenderEngine.PLAYWRIGHT,
            wait_time=2,
            wait_selector="body",  # Wait for body element
            output_format=OutputFormat.JSON,
            output_path=os.path.join(temp_output_dir, "render_wait.json")
        )
        
        assert 'html' in result
        assert 'title' in result
        assert 'saved_to' in result
        assert os.path.exists(result['saved_to'])
        print(f"DEBUG: Render with wait selector completed, saved to: {result['saved_to']}")
        print("DEBUG: Playwright wait selector test passed")

    @pytest.mark.asyncio
    async def test_render_with_scroll(self, scraper_tool):
        """Test Playwright rendering with scroll to bottom"""
        print("DEBUG: Testing Playwright rendering with scroll")
        
        if not scraper_tool.settings.playwright_available:
            print("DEBUG: Playwright not available - skipping scroll test")
            pytest.skip("Playwright not available")
        
        url = "https://httpbin.org/html"
        
        result = await scraper_tool.render(
            url=url,
            engine=RenderEngine.PLAYWRIGHT,
            wait_time=2,
            scroll_to_bottom=True
        )
        
        assert 'html' in result
        assert 'title' in result
        assert len(result['html']) > 0
        print(f"DEBUG: Scrolled page HTML length: {len(result['html'])}")
        print("DEBUG: Playwright scroll test passed")

    @pytest.mark.asyncio 
    async def test_render_error_scenarios(self, scraper_tool):
        """Test various render error scenarios"""
        print("DEBUG: Testing render error scenarios")
        
        if not scraper_tool.settings.playwright_available:
            # Test error when playwright is not available
            with pytest.raises(RenderingError) as exc_info:
                await scraper_tool.render("https://example.com", RenderEngine.PLAYWRIGHT)
            assert "Playwright is not available" in str(exc_info.value)
            print("DEBUG: Playwright unavailable error test passed")
        else:
            # Test with invalid URL
            with pytest.raises(RenderingError):
                await scraper_tool.render("invalid-url", RenderEngine.PLAYWRIGHT)
            
            print("DEBUG: Invalid URL render error test passed")
        
        print("DEBUG: Render error scenarios test passed")

    @pytest.mark.asyncio
    async def test_http_method_error_handling(self, scraper_tool):
        """Test HTTP method error handling scenarios"""
        print("DEBUG: Testing HTTP method error handling")
        
        # Test connection errors
        with pytest.raises(HttpError):
            await scraper_tool.get_httpx("https://nonexistent-domain-12345.invalid", async_mode=True)
        
        # Test with urllib
        with pytest.raises(HttpError):
            await scraper_tool.get_urllib("https://nonexistent-domain-12345.invalid")
        
        # Test invalid HTTP method (using string manipulation to avoid enum validation)
        with pytest.raises((HttpError, AttributeError)):
            # This will fail because httpx client doesn't have invalid method
            await scraper_tool.get_httpx("https://httpbin.org/get", HttpMethod.GET, async_mode=True)
            # Manually test invalid method by patching
            original_method = HttpMethod.GET.value
            HttpMethod.GET._value_ = "INVALID"
            try:
                await scraper_tool.get_httpx("https://httpbin.org/get", HttpMethod.GET, async_mode=True)
            finally:
                HttpMethod.GET._value_ = original_method
        
        print("DEBUG: HTTP method error handling test passed")

    def test_parse_html_comprehensive_selectors(self, scraper_tool):
        """Test HTML parsing with comprehensive selector scenarios"""
        print("DEBUG: Testing HTML parsing with comprehensive selectors")
        
        complex_html = """
        <html>
        <body>
            <div class="container">
                <article data-type="blog" data-id="123">
                    <h2>Article Title</h2>
                    <p class="intro">Introduction paragraph</p>
                    <div class="content">
                        <p>Content paragraph 1</p>
                        <p>Content paragraph 2</p>
                    </div>
                    <footer>
                        <span class="author">John Doe</span>
                        <time datetime="2023-01-01">January 1, 2023</time>
                    </footer>
                </article>
            </div>
        </body>
        </html>
        """
        
        test_cases = [
            # CSS selectors
            ("article h2", "css", True, None, 1, "Article Title"),
            (".intro", "css", True, None, 1, "Introduction paragraph"),
            ("p", "css", True, None, 3, None),  # Should find 3 paragraphs
            ("article", "css", False, "data-id", 1, "123"),
            (".author", "css", True, None, 1, "John Doe"),
            ("time", "css", False, "datetime", 1, "2023-01-01"),
            ("nonexistent", "css", True, None, 0, None)  # No matches
        ]
        
        for selector, sel_type, extract_text, extract_attr, expected_count, expected_content in test_cases:
            result = scraper_tool.parse_html(
                html=complex_html,
                selector=selector,
                selector_type=sel_type,
                extract_text=extract_text,
                extract_attr=extract_attr
            )
            
            assert result['count'] == expected_count
            if expected_content and result['results']:
                assert expected_content in result['results']
            
            print(f"DEBUG: Selector '{selector}' found {result['count']} elements")
        
        print("DEBUG: Comprehensive HTML selectors test passed")

    def test_parse_html_xpath_comprehensive(self, scraper_tool):
        """Test HTML parsing with comprehensive XPath scenarios"""
        print("DEBUG: Testing HTML parsing with comprehensive XPath")
        
        html = """
        <html>
        <body>
            <div id="main">
                <ul>
                    <li class="item">Item 1</li>
                    <li class="item special">Item 2</li>
                    <li class="item">Item 3</li>
                </ul>
            </div>
        </body>
        </html>
        """
        
        xpath_cases = [
            ("//li[@class='item']", 2),  # Items with exact class
            ("//li[contains(@class, 'item')]", 3),  # Items containing class
            ("//li[contains(@class, 'special')]", 1),  # Special item
            ("//div[@id='main']//li", 3),  # All li under main div
            ("//nonexistent", 0)  # No matches
        ]
        
        for xpath, expected_count in xpath_cases:
            try:
                result = scraper_tool.parse_html(
                    html=html,
                    selector=xpath,
                    selector_type="xpath",
                    extract_text=True
                )
                assert result['count'] == expected_count
                print(f"DEBUG: XPath '{xpath}' found {result['count']} elements")
            except ParsingError as e:
                # XPath parsing might fail if lxml is not available
                print(f"DEBUG: XPath parsing failed (expected if lxml unavailable): {e}")
        
        print("DEBUG: Comprehensive XPath test passed")

    @pytest.mark.asyncio
    async def test_binary_content_handling(self, scraper_tool, temp_output_dir):
        """Test binary content handling"""
        print("DEBUG: Testing binary content handling")
        
        # Test with image content
        url = "https://httpbin.org/image/png"
        
        result = await scraper_tool.get_httpx(
            url=url,
            content_type=ContentType.BINARY,
            async_mode=True
        )
        
        assert isinstance(result, dict)
        assert 'content' in result
        assert isinstance(result['content'], bytes)
        assert len(result['content']) > 0
        print(f"DEBUG: Binary content size: {len(result['content'])} bytes")
        
        # Test saving binary content
        output_path = os.path.join(temp_output_dir, "test_image.png")
        with open(output_path, 'wb') as f:
            f.write(result['content'])
        
        assert os.path.exists(output_path)
        print(f"DEBUG: Binary content saved to: {output_path}")
        print("DEBUG: Binary content handling test passed")

    @pytest.mark.asyncio
    async def test_timeout_and_rate_limiting_simulation(self, scraper_tool):
        """Test timeout and rate limiting scenarios"""
        print("DEBUG: Testing timeout and rate limiting simulation")
        
        # Test with delay endpoint to simulate slow response
        url = "https://httpbin.org/delay/1"  # 1 second delay
        
        start_time = time.time()
        result = await scraper_tool.get_httpx(url, async_mode=True)
        elapsed_time = time.time() - start_time
        
        assert isinstance(result, (dict, str))
        assert elapsed_time >= 1.0  # Should take at least 1 second
        print(f"DEBUG: Delayed request took {elapsed_time:.2f} seconds")
        
        # Test rapid successive requests (rate limiting simulation)
        rapid_requests = []
        for i in range(3):
            task = scraper_tool.get_httpx(f"https://httpbin.org/get?request={i}", async_mode=True)
            rapid_requests.append(task)
        
        results = await asyncio.gather(*rapid_requests, return_exceptions=True)
        successful_requests = sum(1 for r in results if not isinstance(r, Exception))
        print(f"DEBUG: {successful_requests}/3 rapid requests succeeded")
        
        print("DEBUG: Timeout and rate limiting simulation test passed")

    def test_file_operation_error_comprehensive(self, scraper_tool):
        """Test comprehensive file operation error handling"""
        print("DEBUG: Testing comprehensive file operation errors")
        
        # Test invalid file paths
        invalid_paths = [
            "/root/cannot_write_here.txt",  # Permission denied
            "/dev/null/invalid_path.txt",  # Invalid directory
            "",  # Empty path
            None  # None path
        ]
        
        for invalid_path in invalid_paths:
            if invalid_path is None:
                continue  # Skip None path for now
                
            try:
                asyncio.run(scraper_tool._save_output(
                    "test content", 
                    invalid_path, 
                    OutputFormat.TEXT
                ))
                print(f"WARNING: Expected error for path {invalid_path} but succeeded")
            except (FileOperationError, OSError, TypeError) as e:
                print(f"DEBUG: Expected error for invalid path '{invalid_path}': {e}")
        
        print("DEBUG: Comprehensive file operation errors test passed")

    @pytest.mark.asyncio
    async def test_network_resilience_and_retry_logic(self, scraper_tool):
        """Test network resilience and handling of various HTTP status codes"""
        print("DEBUG: Testing network resilience")
        
        # Test various HTTP status codes
        status_tests = [
            (200, True),   # OK
            (404, False),  # Not Found
            (500, False),  # Server Error  
            (429, False),  # Too Many Requests
        ]
        
        for status_code, should_succeed in status_tests:
            url = f"https://httpbin.org/status/{status_code}"
            try:
                result = await scraper_tool.get_httpx(url, async_mode=True)
                if should_succeed:
                    print(f"DEBUG: Status {status_code} succeeded as expected")
                else:
                    print(f"WARNING: Status {status_code} succeeded but was expected to fail")
            except HttpError as e:
                if not should_succeed:
                    print(f"DEBUG: Status {status_code} failed as expected: {e}")
                else:
                    print(f"WARNING: Status {status_code} failed but was expected to succeed: {e}")
        
        print("DEBUG: Network resilience test passed")

    def test_scrapy_integration_structure(self, scraper_tool, temp_output_dir):
        """Test Scrapy integration structure and error handling"""
        print("DEBUG: Testing Scrapy integration structure")
        
        # Test with non-existent project directory
        fake_project_path = "/tmp/nonexistent_scrapy_project"
        output_path = os.path.join(temp_output_dir, "scrapy_output.json")
        
        with pytest.raises(ExternalToolError):
            scraper_tool.crawl_scrapy(
                project_path=fake_project_path,
                spider_name="test_spider",
                output_path=output_path
            )
        
        # Test command structure
        assert scraper_tool.settings.scrapy_command == "scrapy"
        print("DEBUG: Scrapy integration structure test passed")

    def test_scrapy_crawl_with_real_project(self, scraper_tool, temp_output_dir):
        """Test Scrapy crawl with a real Scrapy project"""
        print("DEBUG: Testing Scrapy crawl with real project")
        
        # Create a temporary Scrapy project for testing
        import tempfile
        import shutil
        
        # Create temporary directory for Scrapy project
        with tempfile.TemporaryDirectory() as temp_project_dir:
            project_name = "test_scrapy_project"
            project_path = os.path.join(temp_project_dir, project_name)
            
            # Create Scrapy project structure
            os.makedirs(project_path, exist_ok=True)
            os.makedirs(os.path.join(project_path, "spiders"), exist_ok=True)
            
            # Create scrapy.cfg
            scrapy_cfg_content = f"""[settings]
default = {project_name}.settings

[deploy]
project = {project_name}
"""
            with open(os.path.join(project_path, "scrapy.cfg"), "w") as f:
                f.write(scrapy_cfg_content)
            
            # Create settings.py
            settings_content = """# Scrapy settings for test_scrapy_project project

BOT_NAME = 'test_scrapy_project'

SPIDER_MODULES = ['test_scrapy_project.spiders']
NEWSPIDER_MODULE = 'test_scrapy_project.spiders'

# Obey robots.txt rules
ROBOTSTXT_OBEY = True

# Configure pipelines
ITEM_PIPELINES = {
    'test_scrapy_project.pipelines.TestScrapyProjectPipeline': 300,
}

# Configure output
FEEDS = {
    'output.json': {'format': 'json'},
}
"""
            with open(os.path.join(project_path, "settings.py"), "w") as f:
                f.write(settings_content)
            
            # Create __init__.py files
            with open(os.path.join(project_path, "__init__.py"), "w") as f:
                f.write("")
            with open(os.path.join(project_path, "spiders", "__init__.py"), "w") as f:
                f.write("")
            
            # Create a simple spider
            spider_content = '''import scrapy

class TestSpider(scrapy.Spider):
    name = 'test_spider'
    allowed_domains = ['httpbin.org']
    start_urls = ['https://httpbin.org/html']
    
    def parse(self, response):
        yield {
            'title': response.css('title::text').get(),
            'url': response.url,
            'status': response.status,
            'headers': dict(response.headers)
        }
'''
            with open(os.path.join(project_path, "spiders", "test_spider.py"), "w") as f:
                f.write(spider_content)
            
            # Create pipelines.py
            pipeline_content = '''class TestScrapyProjectPipeline:
    def process_item(self, item, spider):
        return item
'''
            with open(os.path.join(project_path, "pipelines.py"), "w") as f:
                f.write(pipeline_content)
            
            # Test the crawl_scrapy method
            output_path = os.path.join(temp_output_dir, "scrapy_test_output.json")
            
            try:
                result = scraper_tool.crawl_scrapy(
                    project_path=project_path,
                    spider_name="test_spider",
                    output_path=output_path
                )
                
                # Verify the result structure
                assert isinstance(result, dict)
                assert 'output_path' in result
                assert 'execution_time' in result
                assert 'file_size' in result
                assert 'stdout' in result
                assert 'stderr' in result
                
                # Verify output file was created
                assert os.path.exists(result['output_path'])
                assert result['file_size'] > 0
                
                # Verify execution time is reasonable
                assert result['execution_time'] > 0
                assert result['execution_time'] < 30  # Should complete within 30 seconds
                
                print(f"DEBUG: Scrapy crawl result: {result}")
                print("DEBUG: Scrapy crawl with real project test passed")
                
            except Exception as e:
                print(f"DEBUG: Scrapy crawl failed with error: {e}")
                # This might fail due to network issues, but we should still test the method structure
                assert "Scrapy crawl" in str(e) or "Error running Scrapy" in str(e)
                print("DEBUG: Scrapy crawl error handling test passed")

    def test_scrapy_crawl_with_spider_args(self, scraper_tool, temp_output_dir):
        """Test Scrapy crawl with spider arguments"""
        print("DEBUG: Testing Scrapy crawl with spider arguments")
        
        # Create a temporary Scrapy project with argument support
        import tempfile
        
        with tempfile.TemporaryDirectory() as temp_project_dir:
            project_name = "test_scrapy_args_project"
            project_path = os.path.join(temp_project_dir, project_name)
            
            # Create Scrapy project structure
            os.makedirs(project_path, exist_ok=True)
            os.makedirs(os.path.join(project_path, "spiders"), exist_ok=True)
            
            # Create scrapy.cfg
            scrapy_cfg_content = f"""[settings]
default = {project_name}.settings

[deploy]
project = {project_name}
"""
            with open(os.path.join(project_path, "scrapy.cfg"), "w") as f:
                f.write(scrapy_cfg_content)
            
            # Create settings.py
            settings_content = """# Scrapy settings for test_scrapy_args_project project

BOT_NAME = 'test_scrapy_args_project'

SPIDER_MODULES = ['test_scrapy_args_project.spiders']
NEWSPIDER_MODULE = 'test_scrapy_args_project.spiders'

# Obey robots.txt rules
ROBOTSTXT_OBEY = True
"""
            with open(os.path.join(project_path, "settings.py"), "w") as f:
                f.write(settings_content)
            
            # Create __init__.py files
            with open(os.path.join(project_path, "__init__.py"), "w") as f:
                f.write("")
            with open(os.path.join(project_path, "spiders", "__init__.py"), "w") as f:
                f.write("")
            
            # Create a spider that accepts arguments
            spider_content = '''import scrapy

class ArgsSpider(scrapy.Spider):
    name = 'args_spider'
    allowed_domains = ['httpbin.org']
    
    def __init__(self, url=None, *args, **kwargs):
        super(ArgsSpider, self).__init__(*args, **kwargs)
        self.start_urls = [url] if url else ['https://httpbin.org/html']
    
    def parse(self, response):
        yield {
            'title': response.css('title::text').get(),
            'url': response.url,
            'status': response.status,
            'custom_arg': getattr(self, 'url', 'default')
        }
'''
            with open(os.path.join(project_path, "spiders", "args_spider.py"), "w") as f:
                f.write(spider_content)
            
            # Test the crawl_scrapy method with arguments
            output_path = os.path.join(temp_output_dir, "scrapy_args_test_output.json")
            
            try:
                result = scraper_tool.crawl_scrapy(
                    project_path=project_path,
                    spider_name="args_spider",
                    output_path=output_path,
                    spider_args={"url": "https://httpbin.org/json"}
                )
                
                # Verify the result structure
                assert isinstance(result, dict)
                assert 'output_path' in result
                assert 'execution_time' in result
                assert 'file_size' in result
                
                print(f"DEBUG: Scrapy crawl with args result: {result}")
                print("DEBUG: Scrapy crawl with spider arguments test passed")
                
            except Exception as e:
                print(f"DEBUG: Scrapy crawl with args failed with error: {e}")
                # This might fail due to network issues, but we should still test the method structure
                assert "Scrapy crawl" in str(e) or "Error running Scrapy" in str(e)
                print("DEBUG: Scrapy crawl with args error handling test passed")

    def test_scrapy_crawl_error_handling(self, scraper_tool, temp_output_dir):
        """Test Scrapy crawl error handling scenarios"""
        print("DEBUG: Testing Scrapy crawl error handling")
        
        # Test with invalid spider name
        with tempfile.TemporaryDirectory() as temp_project_dir:
            project_name = "test_error_project"
            project_path = os.path.join(temp_project_dir, project_name)
            
            # Create minimal project structure
            os.makedirs(project_path, exist_ok=True)
            os.makedirs(os.path.join(project_path, "spiders"), exist_ok=True)
            
            # Create scrapy.cfg
            scrapy_cfg_content = f"""[settings]
default = {project_name}.settings

[deploy]
project = {project_name}
"""
            with open(os.path.join(project_path, "scrapy.cfg"), "w") as f:
                f.write(scrapy_cfg_content)
            
            # Create settings.py
            settings_content = f"""# Scrapy settings for {project_name} project

BOT_NAME = '{project_name}'

SPIDER_MODULES = ['{project_name}.spiders']
NEWSPIDER_MODULE = '{project_name}.spiders'

ROBOTSTXT_OBEY = True
"""
            with open(os.path.join(project_path, "settings.py"), "w") as f:
                f.write(settings_content)
            
            # Create __init__.py files
            with open(os.path.join(project_path, "__init__.py"), "w") as f:
                f.write("")
            with open(os.path.join(project_path, "spiders", "__init__.py"), "w") as f:
                f.write("")
            
            output_path = os.path.join(temp_output_dir, "scrapy_error_test_output.json")
            
            # Test with non-existent spider
            with pytest.raises(ExternalToolError):
                scraper_tool.crawl_scrapy(
                    project_path=project_path,
                    spider_name="non_existent_spider",
                    output_path=output_path
                )
            
            print("DEBUG: Scrapy crawl error handling test passed")

    @pytest.mark.asyncio
    async def test_complex_real_world_scenarios(self, scraper_tool, temp_output_dir):
        """Test complex real-world scraping scenarios"""
        print("DEBUG: Testing complex real-world scenarios")
        
        # Scenario 1: Fetch, parse, and save in one workflow
        url = "https://httpbin.org/html" 
        
        # Fetch HTML
        html_result = await scraper_tool.get_httpx(
            url=url,
            content_type=ContentType.HTML,
            async_mode=True
        )
        
        # Parse multiple elements
        title_result = scraper_tool.parse_html(html_result['html'], "title", "css")
        paragraph_result = scraper_tool.parse_html(html_result['html'], "p", "css")
        
        # Combine results
        combined_result = {
            'url': url,
            'title_count': title_result['count'],
            'paragraph_count': paragraph_result['count'],
            'titles': title_result['results'],
            'paragraphs': paragraph_result['results']
        }
        
        # Save as multiple formats
        json_path = os.path.join(temp_output_dir, "combined_result.json")
        md_path = os.path.join(temp_output_dir, "combined_result.md")
        
        await scraper_tool._save_output(combined_result, json_path, OutputFormat.JSON)
        await scraper_tool._save_output(combined_result, md_path, OutputFormat.MARKDOWN)
        
        assert os.path.exists(json_path)
        assert os.path.exists(md_path)
        print("DEBUG: Complex real-world scenario test passed")

    def test_memory_usage_and_resource_management(self, scraper_tool):
        """Test memory usage and resource management"""
        print("DEBUG: Testing memory usage and resource management")
        
        # Test multiple tool instances
        tools = []
        for i in range(5):
            config = {'user_agent': f'TestAgent{i}/1.0'}
            tool = ScraperTool(config)
            tools.append(tool)
            print(f"DEBUG: Created tool instance {i}")
        
        # Verify each tool has its own settings
        for i, tool in enumerate(tools):
            assert tool.settings.user_agent == f'TestAgent{i}/1.0'
        
        # Cleanup (Python GC should handle this)
        del tools
        print("DEBUG: Memory usage and resource management test passed")

    @pytest.mark.asyncio
    async def test_content_encoding_handling(self, scraper_tool):
        """Test various content encodings"""
        print("DEBUG: Testing content encoding handling")
        
        # Test with gzip encoding
        url = "https://httpbin.org/gzip"
        
        result = await scraper_tool.get_httpx(
            url=url,
            content_type=ContentType.JSON,
            async_mode=True
        )
        
        assert isinstance(result, dict)
        assert 'gzipped' in result
        assert result['gzipped'] == True
        print(f"DEBUG: Gzip encoded content decoded successfully")
        
        # Test with deflate encoding
        url = "https://httpbin.org/deflate"
        
        result = await scraper_tool.get_httpx(
            url=url,
            content_type=ContentType.JSON,
            async_mode=True
        )
        
        assert isinstance(result, dict)
        assert 'deflated' in result
        assert result['deflated'] == True
        print(f"DEBUG: Deflate encoded content decoded successfully")
        
        print("DEBUG: Content encoding handling test passed")

    @pytest.mark.asyncio
    async def test_user_agent_verification(self, scraper_tool):
        """Test user agent is properly sent"""
        print("DEBUG: Testing user agent verification")
        
        # Test default user agent
        url = "https://httpbin.org/user-agent"
        
        result = await scraper_tool.get_httpx(
            url=url,
            content_type=ContentType.JSON,
            async_mode=True
        )
        
        assert isinstance(result, dict)
        assert 'user-agent' in result
        assert scraper_tool.settings.user_agent in result['user-agent']
        print(f"DEBUG: User agent verified: {result['user-agent']}")
        
        # Test custom user agent
        custom_headers = {'User-Agent': 'CustomTestAgent/1.0'}
        result_custom = await scraper_tool.get_httpx(
            url=url,
            headers=custom_headers,
            content_type=ContentType.JSON,
            async_mode=True
        )
        
        assert 'CustomTestAgent/1.0' in result_custom['user-agent']
        print(f"DEBUG: Custom user agent verified: {result_custom['user-agent']}")
        print("DEBUG: User agent verification test passed")

    @pytest.mark.asyncio
    async def test_response_status_handling(self, scraper_tool):
        """Test handling of different response statuses"""
        print("DEBUG: Testing response status handling")
        
        # Test successful statuses
        success_urls = [
            "https://httpbin.org/status/200",
            "https://httpbin.org/status/201", 
            "https://httpbin.org/status/202"
        ]
        
        for url in success_urls:
            try:
                result = await scraper_tool.get_httpx(url, async_mode=True)
                print(f"DEBUG: Success status {url} handled correctly")
            except HttpError:
                print(f"WARNING: Unexpected error for success status {url}")
        
        # Test client error statuses 
        error_urls = [
            "https://httpbin.org/status/400",  # Bad Request
            "https://httpbin.org/status/401",  # Unauthorized
            "https://httpbin.org/status/403",  # Forbidden
            "https://httpbin.org/status/404"   # Not Found
        ]
        
        for url in error_urls:
            with pytest.raises(HttpError):
                await scraper_tool.get_httpx(url, async_mode=True)
            print(f"DEBUG: Error status {url} properly raised HttpError")
        
        print("DEBUG: Response status handling test passed")

    def test_settings_environment_variables(self):
        """Test settings can be configured via environment variables"""
        print("DEBUG: Testing settings environment variables")
        
        # Test environment variable prefix
        settings = ScraperSettings()
        assert settings.model_config['env_prefix'] == "SCRAPER_TOOL_"
        
        # Test with environment variables (simulated)
        with patch.dict(os.environ, {
            'SCRAPER_TOOL_USER_AGENT': 'EnvAgent/1.0',
            'SCRAPER_TOOL_MAX_CONTENT_LENGTH': '5000000'
        }):
            env_settings = ScraperSettings()
            assert env_settings.user_agent == 'EnvAgent/1.0'
            assert env_settings.max_content_length == 5000000
            print("DEBUG: Environment variables properly loaded")
        
        print("DEBUG: Settings environment variables test passed")

    @pytest.mark.asyncio
    async def test_url_validation_and_parsing(self, scraper_tool):
        """Test URL validation and parsing"""
        print("DEBUG: Testing URL validation and parsing")
        
        # Test various URL formats
        valid_urls = [
            "https://httpbin.org/get",
            "http://httpbin.org/get",
            "https://httpbin.org/get?param=value",
            "https://httpbin.org/get#fragment"
        ]
        
        for url in valid_urls:
            try:
                result = await scraper_tool.get_httpx(url, async_mode=True)
                print(f"DEBUG: Valid URL {url} processed successfully")
            except HttpError as e:
                print(f"DEBUG: URL {url} failed: {e}")
        
        # Test invalid URLs
        invalid_urls = [
            "not-a-url",
            "ftp://invalid-protocol.com",
            "javascript:alert('xss')"
        ]
        
        for url in invalid_urls:
            with pytest.raises(HttpError):
                await scraper_tool.get_httpx(url, async_mode=True)
            print(f"DEBUG: Invalid URL {url} properly rejected")
        
        print("DEBUG: URL validation and parsing test passed")

    @pytest.mark.asyncio
    async def test_concurrent_parsing_operations(self, scraper_tool, sample_html_content):
        """Test concurrent parsing operations"""
        print("DEBUG: Testing concurrent parsing operations")
        
        # Multiple parsing operations on the same HTML
        selectors = [
            ("h1", "css"),
            ("p", "css"),
            ("a", "css"),
            ("div", "css"),
            ("li", "css")
        ]
        
        # Simulate concurrent parsing
        results = []
        for selector, sel_type in selectors:
            result = scraper_tool.parse_html(
                html=sample_html_content,
                selector=selector,
                selector_type=sel_type,
                extract_text=True
            )
            results.append(result)
            print(f"DEBUG: Parsed {selector} selector: {result['count']} matches")
        
        # Verify all parsing operations succeeded
        for result in results:
            assert isinstance(result, dict)
            assert 'count' in result
            assert 'results' in result
        
        print("DEBUG: Concurrent parsing operations test passed")

    @pytest.mark.asyncio
    async def test_error_propagation_and_context(self, scraper_tool):
        """Test error propagation and context information"""
        print("DEBUG: Testing error propagation and context")
        
        # Test that errors include useful context
        try:
            await scraper_tool.get_httpx("https://httpbin.org/status/500", async_mode=True)
        except HttpError as e:
            error_msg = str(e)
            assert "500" in error_msg or "Server Error" in error_msg.lower()
            print(f"DEBUG: HTTP error includes context: {error_msg}")
        
        try:
            scraper_tool.parse_html("invalid", "//invalid[xpath", "xpath")
        except ParsingError as e:
            error_msg = str(e)
            assert "parsing" in error_msg.lower()
            print(f"DEBUG: Parsing error includes context: {error_msg}")
        
        print("DEBUG: Error propagation and context test passed")
