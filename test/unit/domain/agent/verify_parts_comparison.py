#!/usr/bin/env python3
"""
éªŒè¯è„šæœ¬ï¼šæ¯”è¾ƒ Vertex AI multi-part response ä¸­ä¸¤ä¸ª parts çš„å†…å®¹
åˆ†æ line 724 (Candidate) å’Œ line 767 (Response) çš„å·®å¼‚
"""

import json
import re
from typing import Dict, List, Any, Tuple

def extract_parts_from_log(log_file_path: str) -> Tuple[Dict, Dict]:
    """
    ä»æ—¥å¿—æ–‡ä»¶ä¸­æå– Candidate å’Œ Response çš„ parts å†…å®¹
    """
    with open(log_file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    # æŸ¥æ‰¾ Candidate éƒ¨åˆ† (line 724 é™„è¿‘)
    candidate_start = None
    candidate_end = None
    for i, line in enumerate(lines):
        if "Candidate:" in line and i > 720:  # line 724 é™„è¿‘
            candidate_start = i
            break
    
    if candidate_start is None:
        raise ValueError("æœªæ‰¾åˆ° Candidate éƒ¨åˆ†")
    
    # æŸ¥æ‰¾ Candidate ç»“æŸä½ç½®
    for i in range(candidate_start + 1, len(lines)):
        if "Response:" in lines[i]:
            candidate_end = i
            break
    
    if candidate_end is None:
        raise ValueError("æœªæ‰¾åˆ° Candidate ç»“æŸä½ç½®")
    
    # æå– Candidate JSON
    candidate_lines = lines[candidate_start:candidate_end]
    candidate_text = ''.join(candidate_lines).replace("Candidate:", "").strip()
    candidate_data = json.loads(candidate_text)
    
    # æŸ¥æ‰¾ Response éƒ¨åˆ† (line 767 é™„è¿‘)
    response_start = None
    for i, line in enumerate(lines):
        if "Response:" in line and i > 760:  # line 767 é™„è¿‘
            response_start = i
            break
    
    if response_start is None:
        raise ValueError("æœªæ‰¾åˆ° Response éƒ¨åˆ†")
    
    # æå– Response JSON (ä» Response: å¼€å§‹åˆ°æ–‡ä»¶ç»“æŸæˆ–ä¸‹ä¸€ä¸ªä¸»è¦éƒ¨åˆ†)
    response_lines = lines[response_start:]
    response_text = ''.join(response_lines).replace("Response:", "").strip()
    
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå®Œæ•´çš„ JSON å¯¹è±¡
    brace_count = 0
    response_json_end = 0
    for i, char in enumerate(response_text):
        if char == '{':
            brace_count += 1
        elif char == '}':
            brace_count -= 1
            if brace_count == 0:
                response_json_end = i + 1
                break
    
    if response_json_end == 0:
        raise ValueError("æœªæ‰¾åˆ°å®Œæ•´çš„ Response JSON")
    
    response_json = response_text[:response_json_end]
    response_data = json.loads(response_json)
    
    return candidate_data, response_data

def compare_parts(candidate_data: Dict, response_data: Dict) -> Dict[str, Any]:
    """
    æ¯”è¾ƒä¸¤ä¸ª parts çš„å†…å®¹
    """
    result = {
        "comparison_summary": {},
        "detailed_analysis": {},
        "differences": [],
        "similarities": []
    }
    
    # æå– parts
    candidate_parts = candidate_data.get("content", {}).get("parts", [])
    response_parts = response_data.get("candidates", [{}])[0].get("content", {}).get("parts", [])
    
    result["comparison_summary"] = {
        "candidate_parts_count": len(candidate_parts),
        "response_parts_count": len(response_parts),
        "parts_match": len(candidate_parts) == len(response_parts)
    }
    
    # è¯¦ç»†æ¯”è¾ƒæ¯ä¸ª part
    for i, (candidate_part, response_part) in enumerate(zip(candidate_parts, response_parts)):
        part_analysis = {
            "part_index": i,
            "candidate_text": candidate_part.get("text", ""),
            "response_text": response_part.get("text", ""),
            "text_length_match": len(candidate_part.get("text", "")) == len(response_part.get("text", "")),
            "text_identical": candidate_part.get("text", "") == response_part.get("text", ""),
            "has_thinking_tags": "<thinking>" in candidate_part.get("text", ""),
            "thinking_tag_complete": _check_thinking_tag_completeness(candidate_part.get("text", ""))
        }
        
        result["detailed_analysis"][f"part_{i}"] = part_analysis
        
        # æ£€æŸ¥å·®å¼‚
        if not part_analysis["text_identical"]:
            result["differences"].append({
                "part_index": i,
                "difference_type": "content_mismatch",
                "candidate_length": len(candidate_part.get("text", "")),
                "response_length": len(response_part.get("text", ""))
            })
        else:
            result["similarities"].append({
                "part_index": i,
                "similarity_type": "identical_content"
            })
    
    return result

def _check_thinking_tag_completeness(text: str) -> Dict[str, Any]:
    """
    æ£€æŸ¥ <thinking> æ ‡ç­¾çš„å®Œæ•´æ€§
    """
    analysis = {
        "has_opening_tag": "<thinking>" in text,
        "has_closing_tag": "</thinking>" in text,
        "is_complete": False,
        "opening_count": text.count("<thinking>"),
        "closing_count": text.count("</thinking>"),
        "tag_balanced": False
    }
    
    if analysis["has_opening_tag"] and analysis["has_closing_tag"]:
        analysis["is_complete"] = True
        analysis["tag_balanced"] = analysis["opening_count"] == analysis["closing_count"]
    
    return analysis

def analyze_thinking_tags(text: str) -> Dict[str, Any]:
    """
    åˆ†æ thinking æ ‡ç­¾çš„è¯¦ç»†æƒ…å†µ
    """
    analysis = {
        "thinking_content": "",
        "non_thinking_content": "",
        "tag_structure": {},
        "issues": []
    }
    
    # æå– thinking å†…å®¹
    thinking_match = re.search(r'<thinking>(.*?)</thinking>', text, re.DOTALL)
    if thinking_match:
        analysis["thinking_content"] = thinking_match.group(1).strip()
        analysis["tag_structure"]["has_complete_tags"] = True
    else:
        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸å®Œæ•´çš„æ ‡ç­¾
        if "<thinking>" in text and "</thinking>" not in text:
            analysis["issues"].append("incomplete_thinking_tag")
            analysis["tag_structure"]["has_opening_only"] = True
        elif "</thinking>" in text and "<thinking>" not in text:
            analysis["issues"].append("orphaned_closing_tag")
            analysis["tag_structure"]["has_closing_only"] = True
    
    # æå–é thinking å†…å®¹
    if analysis["thinking_content"]:
        analysis["non_thinking_content"] = text.replace(f"<thinking>{analysis['thinking_content']}</thinking>", "").strip()
    else:
        analysis["non_thinking_content"] = text
    
    return analysis

def generate_verification_report(log_file_path: str) -> str:
    """
    ç”Ÿæˆå®Œæ•´çš„éªŒè¯æŠ¥å‘Š
    """
    try:
        # æå–æ•°æ®
        candidate_data, response_data = extract_parts_from_log(log_file_path)
        
        # æ¯”è¾ƒ parts
        comparison_result = compare_parts(candidate_data, response_data)
        
        # åˆ†æ thinking æ ‡ç­¾
        thinking_analysis = {}
        for i, part in enumerate(candidate_data.get("content", {}).get("parts", [])):
            text = part.get("text", "")
            thinking_analysis[f"part_{i}"] = analyze_thinking_tags(text)
        
        # ç”ŸæˆæŠ¥å‘Š
        report = f"""
# Vertex AI Multi-Part Response éªŒè¯æŠ¥å‘Š

## ğŸ“Š åŸºæœ¬ä¿¡æ¯
- **Candidate Parts æ•°é‡**: {comparison_result['comparison_summary']['candidate_parts_count']}
- **Response Parts æ•°é‡**: {comparison_result['comparison_summary']['response_parts_count']}
- **Parts æ•°é‡åŒ¹é…**: {'âœ…' if comparison_result['comparison_summary']['parts_match'] else 'âŒ'}

## ğŸ” è¯¦ç»†åˆ†æ

### Parts å†…å®¹æ¯”è¾ƒ
"""
        
        for part_key, analysis in comparison_result["detailed_analysis"].items():
            report += f"""
#### {part_key.upper()}
- **å†…å®¹é•¿åº¦åŒ¹é…**: {'âœ…' if analysis['text_length_match'] else 'âŒ'}
- **å†…å®¹å®Œå…¨ç›¸åŒ**: {'âœ…' if analysis['text_identical'] else 'âŒ'}
- **åŒ…å« thinking æ ‡ç­¾**: {'âœ…' if analysis['has_thinking_tags'] else 'âŒ'}
- **thinking æ ‡ç­¾å®Œæ•´**: {'âœ…' if analysis['thinking_tag_complete']['is_complete'] else 'âŒ'}
"""
            
            if analysis['has_thinking_tags']:
                tag_analysis = analysis['thinking_tag_complete']
                report += f"""
**Thinking æ ‡ç­¾åˆ†æ**:
- å¼€å§‹æ ‡ç­¾æ•°é‡: {tag_analysis['opening_count']}
- ç»“æŸæ ‡ç­¾æ•°é‡: {tag_analysis['closing_count']}
- æ ‡ç­¾å¹³è¡¡: {'âœ…' if tag_analysis['tag_balanced'] else 'âŒ'}
"""
        
        # æ·»åŠ å·®å¼‚æ€»ç»“
        if comparison_result["differences"]:
            report += f"""
## âš ï¸ å‘ç°çš„å·®å¼‚
"""
            for diff in comparison_result["differences"]:
                report += f"- Part {diff['part_index']}: {diff['difference_type']}\n"
        else:
            report += """
## âœ… æœªå‘ç°å·®å¼‚
æ‰€æœ‰ parts å†…å®¹å®Œå…¨åŒ¹é…
"""
        
        # æ·»åŠ  thinking æ ‡ç­¾åˆ†æ
        report += """
## ğŸ·ï¸ Thinking æ ‡ç­¾è¯¦ç»†åˆ†æ
"""
        for part_key, analysis in thinking_analysis.items():
            report += f"""
### {part_key.upper()}
- **Thinking å†…å®¹é•¿åº¦**: {len(analysis['thinking_content'])} å­—ç¬¦
- **é Thinking å†…å®¹é•¿åº¦**: {len(analysis['non_thinking_content'])} å­—ç¬¦
- **æ ‡ç­¾ç»“æ„**: {analysis['tag_structure']}
"""
            if analysis['issues']:
                report += f"- **å‘ç°çš„é—®é¢˜**: {', '.join(analysis['issues'])}\n"
        
        return report
        
    except Exception as e:
        return f"éªŒè¯è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}"

def main():
    """
    ä¸»å‡½æ•°
    """
    log_file_path = "/home/coder1/python-middleware-dev/test_debug_output.log"
    
    print("ğŸ” å¼€å§‹éªŒè¯ Vertex AI Multi-Part Response...")
    print("=" * 80)
    
    try:
        report = generate_verification_report(log_file_path)
        print(report)
        
        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        with open("/home/coder1/python-middleware-dev/parts_verification_report.md", "w", encoding="utf-8") as f:
            f.write(report)
        
        print("\nğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜åˆ°: parts_verification_report.md")
        
    except Exception as e:
        print(f"âŒ éªŒè¯å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    main()
