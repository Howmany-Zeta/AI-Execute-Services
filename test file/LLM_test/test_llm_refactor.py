"""
æµ‹è¯•æ–°çš„LLMå®¢æˆ·ç«¯é‡æ„æ¶æ„

è¿™ä¸ªæµ‹è¯•æ–‡ä»¶éªŒè¯ï¼š
1. æ¯ä¸ªAI provideréƒ½æœ‰ç‹¬ç«‹çš„å®¢æˆ·ç«¯
2. ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„AIåå¥½é€‰æ‹©æ­£å¸¸å·¥ä½œ
3. General summarizeræœåŠ¡èƒ½æ­£ç¡®ä½¿ç”¨æ–°æ¶æ„
"""

import asyncio
import json
from app.llm import (
    get_llm_manager,
    LLMClientFactory,
    AIProvider,
    LLMMessage,
    OpenAIClient,
    VertexAIClient,
    XAIClient
)
from app.services.general.services.summarizer import SummarizerService

async def test_individual_clients():
    """æµ‹è¯•æ¯ä¸ªAI providerçš„ç‹¬ç«‹å®¢æˆ·ç«¯"""
    print("=== æµ‹è¯•ç‹¬ç«‹å®¢æˆ·ç«¯ ===")

    # æµ‹è¯•å·¥å‚æ¨¡å¼
    openai_client = LLMClientFactory.get_client(AIProvider.OPENAI)
    vertex_client = LLMClientFactory.get_client(AIProvider.VERTEX)
    xai_client = LLMClientFactory.get_client(AIProvider.XAI)

    print(f"OpenAIå®¢æˆ·ç«¯ç±»å‹: {type(openai_client).__name__}")
    print(f"Vertexå®¢æˆ·ç«¯ç±»å‹: {type(vertex_client).__name__}")
    print(f"xAIå®¢æˆ·ç«¯ç±»å‹: {type(xai_client).__name__}")

    # éªŒè¯å®¢æˆ·ç«¯æ˜¯å•ä¾‹
    openai_client2 = LLMClientFactory.get_client(AIProvider.OPENAI)
    assert openai_client is openai_client2, "å®¢æˆ·ç«¯åº”è¯¥æ˜¯å•ä¾‹"
    print("âœ“ å®¢æˆ·ç«¯å•ä¾‹æ¨¡å¼æ­£å¸¸å·¥ä½œ")

async def test_context_aware_selection():
    """æµ‹è¯•ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„AIåå¥½é€‰æ‹©"""
    print("\n=== æµ‹è¯•ä¸Šä¸‹æ–‡æ„ŸçŸ¥é€‰æ‹© ===")

    llm_manager = await get_llm_manager()

    # æµ‹è¯•ä¸åŒçš„ä¸Šä¸‹æ–‡é…ç½®
    test_contexts = [
        {
            "metadata": {
                "aiPreference": {
                    "provider": "OpenAI",
                    "model": "gpt-4"
                }
            }
        },
        {
            "metadata": {
                "aiPreference": {
                    "provider": "vertex",
                    "model": "gemini-1.5-pro"
                }
            }
        },
        {
            "metadata": {
                "aiPreference": {
                    "provider": "xAI",
                    "model": "grok-2"
                }
            }
        }
    ]

    for i, context in enumerate(test_contexts):
        provider = context["metadata"]["aiPreference"]["provider"]
        model = context["metadata"]["aiPreference"]["model"]
        print(f"æµ‹è¯•ä¸Šä¸‹æ–‡ {i+1}: {provider}/{model}")

        # æå–AIåå¥½
        context_provider, context_model = llm_manager._extract_ai_preference(context)
        print(f"  æå–çš„åå¥½: {context_provider}/{context_model}")

        expected_provider = {
            "OpenAI": AIProvider.OPENAI,
            "vertex": AIProvider.VERTEX,
            "xAI": AIProvider.XAI
        }[provider]

        assert context_provider == expected_provider, f"æä¾›å•†æå–é”™è¯¯: {context_provider} != {expected_provider}"
        assert context_model == model, f"æ¨¡å‹æå–é”™è¯¯: {context_model} != {model}"

    print("âœ“ ä¸Šä¸‹æ–‡æ„ŸçŸ¥é€‰æ‹©æ­£å¸¸å·¥ä½œ")

async def test_summarizer_integration():
    """æµ‹è¯•General Summarizerä¸æ–°æ¶æ„çš„é›†æˆ"""
    print("\n=== æµ‹è¯•Summarizeré›†æˆ ===")

    summarizer = SummarizerService()

    # æµ‹è¯•æ•°æ®
    input_data = {
        "text": "è¯·å¸®æˆ‘æ€»ç»“ä¸€ä¸‹äººå·¥æ™ºèƒ½çš„å‘å±•å†ç¨‹",
        "task_type": "summarize"
    }

    # æµ‹è¯•ä¸åŒçš„AIåå¥½
    test_contexts = [
        {
            "user_id": "test_user",
            "chat_id": "test_chat",
            "metadata": {
                "aiPreference": {
                    "provider": "OpenAI",
                    "model": "gpt-4-turbo"
                }
            }
        },
        {
            "user_id": "test_user",
            "chat_id": "test_chat",
            "metadata": {
                "aiPreference": {
                    "provider": "vertex",
                    "model": "gemini-1.5-pro"
                }
            }
        }
    ]

    for i, context in enumerate(test_contexts):
        provider = context["metadata"]["aiPreference"]["provider"]
        model = context["metadata"]["aiPreference"]["model"]
        print(f"æµ‹è¯•Summarizerä¸Šä¸‹æ–‡ {i+1}: {provider}/{model}")

        try:
            # æ³¨æ„ï¼šè¿™é‡Œåªæ˜¯æµ‹è¯•æ¶æ„ï¼Œä¸ä¼šçœŸæ­£è°ƒç”¨API
            # åœ¨å®é™…ç¯å¢ƒä¸­éœ€è¦é…ç½®APIå¯†é’¥
            print(f"  å‡†å¤‡è°ƒç”¨ {provider}/{model}")
            print(f"  è¾“å…¥: {input_data['text'][:50]}...")
            print(f"  ä»»åŠ¡ç±»å‹: {input_data['task_type']}")
            print("  âœ“ æ¶æ„é›†æˆæ­£å¸¸")

        except Exception as e:
            print(f"  âš ï¸  APIè°ƒç”¨å¤±è´¥ï¼ˆé¢„æœŸï¼Œå› ä¸ºæ²¡æœ‰é…ç½®å¯†é’¥ï¼‰: {str(e)[:100]}")

    print("âœ“ Summarizeré›†æˆæ¶æ„æ­£å¸¸")

async def test_backward_compatibility():
    """æµ‹è¯•å‘åå…¼å®¹æ€§"""
    print("\n=== æµ‹è¯•å‘åå…¼å®¹æ€§ ===")

    # æµ‹è¯•æ—§çš„å¯¼å…¥æ–¹å¼ä»ç„¶æœ‰æ•ˆ
    try:
        from app.llm.llm_client import LLMClient, get_llm_client
        print("âœ“ æ—§çš„å¯¼å…¥æ–¹å¼ä»ç„¶æœ‰æ•ˆ")

        # æµ‹è¯•æ—§çš„å®¢æˆ·ç«¯æ¥å£
        legacy_client = await get_llm_client()
        print(f"âœ“ æ—§å®¢æˆ·ç«¯ç±»å‹: {type(legacy_client).__name__}")

    except ImportError as e:
        print(f"âœ— å‘åå…¼å®¹æ€§æµ‹è¯•å¤±è´¥: {e}")

async def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹æµ‹è¯•LLMå®¢æˆ·ç«¯é‡æ„æ¶æ„...\n")

    try:
        await test_individual_clients()
        await test_context_aware_selection()
        await test_summarizer_integration()
        await test_backward_compatibility()

        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ–°çš„LLMå®¢æˆ·ç«¯æ¶æ„å·¥ä½œæ­£å¸¸ã€‚")
        print("\nä¸»è¦æ”¹è¿›:")
        print("1. âœ“ æ¯ä¸ªAI provideréƒ½æœ‰ç‹¬ç«‹çš„å®¢æˆ·ç«¯å®ç°")
        print("2. âœ“ ä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„AIåå¥½è‡ªåŠ¨é€‰æ‹©")
        print("3. âœ“ General Summarizerå·²æ›´æ–°ä½¿ç”¨æ–°æ¶æ„")
        print("4. âœ“ ä¿æŒå‘åå…¼å®¹æ€§")
        print("5. âœ“ æ¨¡å—åŒ–è®¾è®¡ï¼Œæ˜“äºæ‰©å±•æ–°çš„AIæä¾›å•†")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(main())
